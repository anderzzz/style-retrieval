{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Model Style Evaluation\n",
    "\n",
    "This notebook evaluates reconstructions across different **model × prompting method** combinations.\n",
    "\n",
    "## Methodology\n",
    "\n",
    "Unlike the single-experiment evaluation in `style_evaluation.ipynb`, this notebook:\n",
    "\n",
    "1. **Pulls reconstructions** from multiple existing evaluation databases\n",
    "2. **Combines** them into unified comparison sets (e.g., Mistral+fewshot vs Kimi+fewshot vs Mistral+agent_holistic)\n",
    "3. **Runs random 4-way comparisons**: For each sample, randomly sample 4 combinations and judge them\n",
    "4. **Aggregates** using Bradley-Terry model to estimate strength of each combination\n",
    "\n",
    "## Advantages over Pairwise ELO\n",
    "\n",
    "- **More information per judgment**: 4-way ranking > pairwise comparison\n",
    "- **Statistically principled**: Bradley-Terry gives confidence intervals and p-values\n",
    "- **Efficient**: O(k*n) comparisons instead of O(n²) pairwise round-robin\n",
    "- **Reuses existing infrastructure**: Same judge prompts and 4-way ranking code\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "You must have already run `style_evaluation.ipynb` with different reconstruction models to generate source databases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "from belletrist import (\n",
    "    LLM, LLMConfig, PromptMaker,\n",
    "    CrossModelComparisonStore, Combination\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "### 1. Specify which combinations to compare\n",
    "\n",
    "Each `Combination` specifies:\n",
    "- `db_path`: Path to evaluation database\n",
    "- `method`: Reconstruction method (e.g., 'fewshot', 'agent_holistic')\n",
    "- `label`: Unique identifier for this combination (e.g., 'mistral_fewshot')\n",
    "- `reconstruction_run`: Which run to use (default 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Compare fewshot and agent_holistic across two models\n",
    "combinations = [\n",
    "    # Mistral reconstructions\n",
    "    Combination(\n",
    "        db_path=Path(\"style_eval_s_mistral_r_mistral_j_anthropic.db\"),\n",
    "        method=\"fewshot\",\n",
    "        label=\"mistral_fewshot\"\n",
    "    ),\n",
    "    Combination(\n",
    "        db_path=Path(\"style_eval_s_mistral_r_mistral_j_anthropic.db\"),\n",
    "        method=\"agent_holistic\",\n",
    "        label=\"mistral_agent\"\n",
    "    ),\n",
    "    \n",
    "    # Kimi reconstructions\n",
    "    Combination(\n",
    "        db_path=Path(\"style_eval_s_mistral_r_kimi_j_anthropic.db\"),\n",
    "        method=\"fewshot\",\n",
    "        label=\"kimi_fewshot\"\n",
    "    ),\n",
    "    Combination(\n",
    "        db_path=Path(\"style_eval_s_mistral_r_kimi_j_anthropic.db\"),\n",
    "        method=\"agent_holistic\",\n",
    "        label=\"kimi_agent\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Verify all databases exist\n",
    "for combo in combinations:\n",
    "    if not combo.db_path.exists():\n",
    "        raise FileNotFoundError(f\"Database not found: {combo.db_path}\")\n",
    "\n",
    "print(f\"Configured {len(combinations)} combinations:\")\n",
    "for combo in combinations:\n",
    "    print(f\"  {combo.label:20s} = {combo.db_path.name} / {combo.method}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Configure judge LLM and comparison parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Judge LLM configuration\n",
    "JUDGE_MODEL = 'anthropic/claude-sonnet-4-5-20250929'\n",
    "JUDGE_API_KEY_ENV_VAR = 'ANTHROPIC_API_KEY'\n",
    "\n",
    "# Comparison parameters\n",
    "N_COMPARISONS_PER_SAMPLE = 8  # How many random 4-way comparisons per sample\n",
    "RANDOM_SEED = 42  # For reproducibility\n",
    "\n",
    "# Output database\n",
    "OUTPUT_DB = Path(f\"cross_eval_{datetime.now().strftime('%Y%m%d_%H%M%S')}.db\")\n",
    "\n",
    "print(f\"Judge: {JUDGE_MODEL}\")\n",
    "print(f\"Comparisons per sample: {N_COMPARISONS_PER_SAMPLE}\")\n",
    "print(f\"Output: {OUTPUT_DB}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize judge LLM\n",
    "judge_llm = LLM(LLMConfig(\n",
    "    model=JUDGE_MODEL,\n",
    "    api_key=os.environ.get(JUDGE_API_KEY_ENV_VAR)\n",
    "))\n",
    "\n",
    "# Initialize prompt maker\n",
    "prompt_maker = PromptMaker()\n",
    "\n",
    "# Initialize comparison store\n",
    "comparer = CrossModelComparisonStore(\n",
    "    output_db=OUTPUT_DB,\n",
    "    combinations=combinations\n",
    ")\n",
    "\n",
    "print(\"✓ Components initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Reconstructions from Source Databases\n",
    "\n",
    "Pull reconstructions from all configured databases. This verifies:\n",
    "- All samples exist in all databases\n",
    "- Original texts are consistent across databases\n",
    "- Requested methods exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all samples (or specify sample_ids=['sample_000', 'sample_001', ...])\n",
    "comparer.load_reconstructions()\n",
    "\n",
    "# Check what was loaded\n",
    "stats = comparer.get_stats()\n",
    "print(f\"\\n=== Loaded Reconstructions ===\")\n",
    "print(f\"Samples: {stats['n_samples']}\")\n",
    "print(f\"Combinations: {stats['n_combinations']}\")\n",
    "print(f\"Total reconstructions: {stats['n_samples'] * stats['n_combinations']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Run Random 4-Way Comparisons\n",
    "\n",
    "For each sample, randomly sample `N_COMPARISONS_PER_SAMPLE` sets of 4 combinations and judge them.\n",
    "\n",
    "This generates `n_samples × N_COMPARISONS_PER_SAMPLE` judgments total.\n",
    "\n",
    "**Note:** This step calls the LLM judge and may take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparer.run_comparisons(\n",
    "    judge_llm=judge_llm,\n",
    "    prompt_maker=prompt_maker,\n",
    "    n_comparisons_per_sample=N_COMPARISONS_PER_SAMPLE,\n",
    "    seed=RANDOM_SEED\n",
    ")\n",
    "\n",
    "stats = comparer.get_stats()\n",
    "print(f\"\\n=== Comparison Complete ===\")\n",
    "print(f\"Total judgments: {stats['n_judgments']}\")\n",
    "print(f\"Expected: {stats['n_samples'] * N_COMPARISONS_PER_SAMPLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export full judgment data\n",
    "df_judgments = comparer.to_dataframe()\n",
    "\n",
    "print(f\"Total judgments: {len(df_judgments)}\")\n",
    "print(f\"\\n=== Sample Judgments ===\")\n",
    "display_cols = [\n",
    "    'sample_id', 'comparison_run',\n",
    "    'label_text_a', 'label_text_b', 'label_text_c', 'label_text_d',\n",
    "    'ranking_text_a', 'ranking_text_b', 'ranking_text_c', 'ranking_text_d',\n",
    "    'confidence'\n",
    "]\n",
    "print(df_judgments[display_cols].head(10))\n",
    "\n",
    "# Save to CSV\n",
    "output_csv = OUTPUT_DB.with_suffix('.csv')\n",
    "df_judgments.to_csv(output_csv, index=False)\n",
    "print(f\"\\n✓ Saved to {output_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export Bradley-Terry format (pairwise preferences)\n",
    "df_bt = comparer.to_bradley_terry_format()\n",
    "\n",
    "print(f\"\\n=== Bradley-Terry Format ===\")\n",
    "print(f\"Total pairwise preferences: {len(df_bt)}\")\n",
    "print(f\"\\nSample rows:\")\n",
    "print(df_bt.head(10))\n",
    "\n",
    "# Save Bradley-Terry data\n",
    "bt_csv = OUTPUT_DB.with_suffix('.bt.csv')\n",
    "df_bt.to_csv(bt_csv, index=False)\n",
    "print(f\"\\n✓ Saved to {bt_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Bradley-Terry Analysis\n",
    "\n",
    "Fit a Bradley-Terry model to estimate the \"strength\" of each combination.\n",
    "\n",
    "Bradley-Terry models pairwise preferences as:\n",
    "$$P(i > j) = \\frac{\\pi_i}{\\pi_i + \\pi_j}$$\n",
    "\n",
    "where $\\pi_i$ is the strength parameter for combination $i$.\n",
    "\n",
    "We'll use iterative maximum likelihood estimation (MM algorithm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_bradley_terry(df_pairs, max_iter=100, tol=1e-6):\n",
    "    \"\"\"\n",
    "    Fit Bradley-Terry model using MM algorithm.\n",
    "    \n",
    "    Args:\n",
    "        df_pairs: DataFrame with 'winner' and 'loser' columns\n",
    "        max_iter: Maximum iterations\n",
    "        tol: Convergence tolerance\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with columns: label, strength, log_strength\n",
    "    \"\"\"\n",
    "    # Count wins for each pair\n",
    "    wins = df_pairs.groupby(['winner', 'loser']).size().reset_index(name='count')\n",
    "    \n",
    "    # Get all unique labels\n",
    "    labels = sorted(set(df_pairs['winner']) | set(df_pairs['loser']))\n",
    "    n = len(labels)\n",
    "    label_to_idx = {label: i for i, label in enumerate(labels)}\n",
    "    \n",
    "    # Initialize strengths uniformly\n",
    "    pi = np.ones(n)\n",
    "    \n",
    "    # Build win matrix W[i,j] = number of times i beat j\n",
    "    W = np.zeros((n, n))\n",
    "    for _, row in wins.iterrows():\n",
    "        i = label_to_idx[row['winner']]\n",
    "        j = label_to_idx[row['loser']]\n",
    "        W[i, j] = row['count']\n",
    "    \n",
    "    # Total comparisons involving each item\n",
    "    n_comparisons = W.sum(axis=1) + W.sum(axis=0)\n",
    "    \n",
    "    # MM algorithm\n",
    "    for iteration in range(max_iter):\n",
    "        pi_old = pi.copy()\n",
    "        \n",
    "        # Update each strength parameter\n",
    "        for i in range(n):\n",
    "            if n_comparisons[i] == 0:\n",
    "                continue\n",
    "            \n",
    "            # Sum of 1/(pi_i + pi_j) over all j that i played against\n",
    "            denom = 0\n",
    "            for j in range(n):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                n_ij = W[i, j] + W[j, i]  # Total matches between i and j\n",
    "                if n_ij > 0:\n",
    "                    denom += n_ij / (pi_old[i] + pi_old[j])\n",
    "            \n",
    "            if denom > 0:\n",
    "                pi[i] = n_comparisons[i] / denom\n",
    "        \n",
    "        # Normalize to prevent drift\n",
    "        pi = pi / pi.sum() * n\n",
    "        \n",
    "        # Check convergence\n",
    "        if np.max(np.abs(pi - pi_old)) < tol:\n",
    "            print(f\"Converged in {iteration + 1} iterations\")\n",
    "            break\n",
    "    \n",
    "    # Build results DataFrame\n",
    "    results = pd.DataFrame({\n",
    "        'label': labels,\n",
    "        'strength': pi,\n",
    "        'log_strength': np.log(pi),\n",
    "        'n_comparisons': n_comparisons\n",
    "    })\n",
    "    \n",
    "    return results.sort_values('strength', ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Fit model\n",
    "bt_results = fit_bradley_terry(df_bt)\n",
    "\n",
    "print(\"\\n=== Bradley-Terry Rankings ===\")\n",
    "print(bt_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Visualization\n",
    "\n",
    "Visualize the Bradley-Terry strength estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Bar chart of strengths\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "ax.barh(bt_results['label'], bt_results['strength'])\n",
    "ax.set_xlabel('Bradley-Terry Strength')\n",
    "ax.set_ylabel('Combination')\n",
    "ax.set_title('Model × Method Strength Estimates')\n",
    "ax.invert_yaxis()  # Best at top\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Win Rate Matrix\n",
    "\n",
    "Show pairwise win rates for interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate win rate matrix\n",
    "labels = bt_results['label'].tolist()\n",
    "n = len(labels)\n",
    "\n",
    "win_matrix = np.zeros((n, n))\n",
    "\n",
    "for i, label_i in enumerate(labels):\n",
    "    for j, label_j in enumerate(labels):\n",
    "        if i == j:\n",
    "            win_matrix[i, j] = 0.5  # Diagonal\n",
    "            continue\n",
    "        \n",
    "        # Count wins\n",
    "        wins_i = len(df_bt[(df_bt['winner'] == label_i) & (df_bt['loser'] == label_j)])\n",
    "        wins_j = len(df_bt[(df_bt['winner'] == label_j) & (df_bt['loser'] == label_i)])\n",
    "        total = wins_i + wins_j\n",
    "        \n",
    "        if total > 0:\n",
    "            win_matrix[i, j] = wins_i / total\n",
    "        else:\n",
    "            win_matrix[i, j] = 0.5  # No data\n",
    "\n",
    "# Visualize as heatmap\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "im = ax.imshow(win_matrix, cmap='RdYlGn', vmin=0, vmax=1)\n",
    "\n",
    "# Labels\n",
    "ax.set_xticks(np.arange(n))\n",
    "ax.set_yticks(np.arange(n))\n",
    "ax.set_xticklabels(labels)\n",
    "ax.set_yticklabels(labels)\n",
    "\n",
    "# Rotate x labels\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "\n",
    "# Annotate cells\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        text = ax.text(j, i, f\"{win_matrix[i, j]:.2f}\",\n",
    "                      ha=\"center\", va=\"center\", color=\"black\", fontsize=9)\n",
    "\n",
    "ax.set_title(\"Win Rate Matrix (row vs column)\")\n",
    "fig.colorbar(im, ax=ax, label='Win Rate')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation: Cell (i, j) shows how often combination i beats combination j.\")\n",
    "print(\"Green = high win rate, Red = low win rate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Statistical Summary\n",
    "\n",
    "Compute additional statistics for each combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate statistics for each combination\n",
    "stats_records = []\n",
    "\n",
    "for label in labels:\n",
    "    # Total wins and losses\n",
    "    wins = len(df_bt[df_bt['winner'] == label])\n",
    "    losses = len(df_bt[df_bt['loser'] == label])\n",
    "    total = wins + losses\n",
    "    \n",
    "    # Win rate\n",
    "    win_rate = wins / total if total > 0 else 0\n",
    "    \n",
    "    # Average rank (from original judgments)\n",
    "    # Find all judgments involving this label\n",
    "    ranks = []\n",
    "    for _, row in df_judgments.iterrows():\n",
    "        for letter in ['a', 'b', 'c', 'd']:\n",
    "            if row[f'label_text_{letter}'] == label:\n",
    "                ranks.append(row[f'ranking_text_{letter}'])\n",
    "    \n",
    "    avg_rank = np.mean(ranks) if ranks else np.nan\n",
    "    \n",
    "    # Get BT strength\n",
    "    bt_strength = bt_results[bt_results['label'] == label]['strength'].values[0]\n",
    "    \n",
    "    stats_records.append({\n",
    "        'label': label,\n",
    "        'bt_strength': bt_strength,\n",
    "        'win_rate': win_rate,\n",
    "        'avg_rank': avg_rank,\n",
    "        'total_wins': wins,\n",
    "        'total_losses': losses,\n",
    "        'n_judgments': len(ranks)\n",
    "    })\n",
    "\n",
    "df_stats = pd.DataFrame(stats_records).sort_values('bt_strength', ascending=False)\n",
    "\n",
    "print(\"\\n=== Comprehensive Statistics ===\")\n",
    "print(df_stats.to_string(index=False))\n",
    "\n",
    "# Save stats\n",
    "stats_csv = OUTPUT_DB.with_suffix('.stats.csv')\n",
    "df_stats.to_csv(stats_csv, index=False)\n",
    "print(f\"\\n✓ Saved to {stats_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Interpretation Guide:**\n",
    "\n",
    "- **Bradley-Terry Strength**: Higher = better. Represents estimated \"ability\" to beat other combinations\n",
    "- **Win Rate**: Proportion of pairwise comparisons won (should correlate with BT strength)\n",
    "- **Average Rank**: Lower = better (1 = best, 4 = worst in each 4-way comparison)\n",
    "\n",
    "**Next Steps:**\n",
    "\n",
    "1. Compare BT rankings to simple average rank to validate model fit\n",
    "2. Examine judge reasoning for close competitors\n",
    "3. Bootstrap confidence intervals for BT strengths\n",
    "4. Run additional comparisons if rankings are unstable"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
