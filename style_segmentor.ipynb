{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Style Segmentor: Building a Catalog of Exemplary Passages\n",
    "\n",
    "This notebook builds a segment catalog by analyzing chapters and extracting exemplary passages that demonstrate specific craft moves.\n",
    "\n",
    "## Workflow\n",
    "\n",
    "For each chapter defined in `chapters_config.yaml`:\n",
    "1. **Analyze**: LLM identifies exemplary passages demonstrating teachable craft moves\n",
    "2. **Store**: Save passages to SQLite catalog with craft annotations (craft_move, teaching_note, tags)\n",
    "3. **Browse**: Demonstrate catalog browsing pattern (list tags → browse summaries → retrieve full text)\n",
    "\n",
    "## Output\n",
    "\n",
    "- **segments.db**: SQLite database containing annotated passages ready for agent retrieval\n",
    "- Agents can browse by tags, search by craft move, and retrieve full text with provenance\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- **Skip existing chapters**: Resume after interruption without re-analyzing\n",
    "- **Tag consistency**: Encourages reuse of existing tags across chapters\n",
    "- **Provenance tracking**: Every segment includes file index and paragraph range\n",
    "- **Skills pattern**: Catalog designed for agent browsing, not pre-loaded prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Libraries and Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: litellm>=1.80.0 in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from -r requirements.txt (line 2)) (1.80.7)\n",
      "Requirement already satisfied: pydantic==2.7.4 in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from -r requirements.txt (line 5)) (2.7.4)\n",
      "Requirement already satisfied: jinja2>=3.1.0 in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from -r requirements.txt (line 8)) (3.1.6)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from pydantic==2.7.4->-r requirements.txt (line 5)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from pydantic==2.7.4->-r requirements.txt (line 5)) (2.18.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from pydantic==2.7.4->-r requirements.txt (line 5)) (4.15.0)\n",
      "Requirement already satisfied: aiohttp>=3.10 in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from litellm>=1.80.0->-r requirements.txt (line 2)) (3.13.2)\n",
      "Requirement already satisfied: click in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from litellm>=1.80.0->-r requirements.txt (line 2)) (8.3.1)\n",
      "Requirement already satisfied: fastuuid>=0.13.0 in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from litellm>=1.80.0->-r requirements.txt (line 2)) (0.14.0)\n",
      "Requirement already satisfied: grpcio<1.68.0,>=1.62.3 in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from litellm>=1.80.0->-r requirements.txt (line 2)) (1.67.1)\n",
      "Requirement already satisfied: httpx>=0.23.0 in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from litellm>=1.80.0->-r requirements.txt (line 2)) (0.28.1)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from litellm>=1.80.0->-r requirements.txt (line 2)) (8.7.0)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from litellm>=1.80.0->-r requirements.txt (line 2)) (4.25.1)\n",
      "Requirement already satisfied: openai>=2.8.0 in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from litellm>=1.80.0->-r requirements.txt (line 2)) (2.8.1)\n",
      "Requirement already satisfied: python-dotenv>=0.2.0 in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from litellm>=1.80.0->-r requirements.txt (line 2)) (1.2.1)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from litellm>=1.80.0->-r requirements.txt (line 2)) (0.12.0)\n",
      "Requirement already satisfied: tokenizers in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from litellm>=1.80.0->-r requirements.txt (line 2)) (0.22.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from jinja2>=3.1.0->-r requirements.txt (line 8)) (3.0.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from aiohttp>=3.10->litellm>=1.80.0->-r requirements.txt (line 2)) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from aiohttp>=3.10->litellm>=1.80.0->-r requirements.txt (line 2)) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from aiohttp>=3.10->litellm>=1.80.0->-r requirements.txt (line 2)) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from aiohttp>=3.10->litellm>=1.80.0->-r requirements.txt (line 2)) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from aiohttp>=3.10->litellm>=1.80.0->-r requirements.txt (line 2)) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from aiohttp>=3.10->litellm>=1.80.0->-r requirements.txt (line 2)) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from aiohttp>=3.10->litellm>=1.80.0->-r requirements.txt (line 2)) (1.22.0)\n",
      "Requirement already satisfied: anyio in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from httpx>=0.23.0->litellm>=1.80.0->-r requirements.txt (line 2)) (4.11.0)\n",
      "Requirement already satisfied: certifi in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from httpx>=0.23.0->litellm>=1.80.0->-r requirements.txt (line 2)) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from httpx>=0.23.0->litellm>=1.80.0->-r requirements.txt (line 2)) (1.0.9)\n",
      "Requirement already satisfied: idna in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from httpx>=0.23.0->litellm>=1.80.0->-r requirements.txt (line 2)) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.23.0->litellm>=1.80.0->-r requirements.txt (line 2)) (0.16.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from importlib-metadata>=6.8.0->litellm>=1.80.0->-r requirements.txt (line 2)) (3.23.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.80.0->-r requirements.txt (line 2)) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.80.0->-r requirements.txt (line 2)) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.80.0->-r requirements.txt (line 2)) (0.30.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from openai>=2.8.0->litellm>=1.80.0->-r requirements.txt (line 2)) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from openai>=2.8.0->litellm>=1.80.0->-r requirements.txt (line 2)) (0.12.0)\n",
      "Requirement already satisfied: sniffio in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from openai>=2.8.0->litellm>=1.80.0->-r requirements.txt (line 2)) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from openai>=2.8.0->litellm>=1.80.0->-r requirements.txt (line 2)) (4.67.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from tiktoken>=0.7.0->litellm>=1.80.0->-r requirements.txt (line 2)) (2025.11.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from tiktoken>=0.7.0->litellm>=1.80.0->-r requirements.txt (line 2)) (2.32.5)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from tokenizers->litellm>=1.80.0->-r requirements.txt (line 2)) (1.1.7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: filelock in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm>=1.80.0->-r requirements.txt (line 2)) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm>=1.80.0->-r requirements.txt (line 2)) (2025.10.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm>=1.80.0->-r requirements.txt (line 2)) (1.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm>=1.80.0->-r requirements.txt (line 2)) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm>=1.80.0->-r requirements.txt (line 2)) (6.0.3)\n",
      "Requirement already satisfied: shellingham in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm>=1.80.0->-r requirements.txt (line 2)) (1.5.4)\n",
      "Requirement already satisfied: typer-slim in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm>=1.80.0->-r requirements.txt (line 2)) (0.20.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken>=0.7.0->litellm>=1.80.0->-r requirements.txt (line 2)) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken>=0.7.0->litellm>=1.80.0->-r requirements.txt (line 2)) (2.5.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Providers\n",
      "=========\n",
      "* openai\n",
      "* openai_like\n",
      "* bytez\n",
      "* xai\n",
      "* custom_openai\n",
      "* text-completion-openai\n",
      "* cohere\n",
      "* cohere_chat\n",
      "* clarifai\n",
      "* anthropic\n",
      "* anthropic_text\n",
      "* replicate\n",
      "* huggingface\n",
      "* together_ai\n",
      "* datarobot\n",
      "* openrouter\n",
      "* cometapi\n",
      "* vertex_ai\n",
      "* vertex_ai_beta\n",
      "* gemini\n",
      "* ai21\n",
      "* baseten\n",
      "* azure\n",
      "* azure_text\n",
      "* azure_ai\n",
      "* sagemaker\n",
      "* sagemaker_chat\n",
      "* bedrock\n",
      "* vllm\n",
      "* nlp_cloud\n",
      "* petals\n",
      "* oobabooga\n",
      "* ollama\n",
      "* ollama_chat\n",
      "* deepinfra\n",
      "* perplexity\n",
      "* mistral\n",
      "* groq\n",
      "* nvidia_nim\n",
      "* cerebras\n",
      "* baseten\n",
      "* ai21_chat\n",
      "* volcengine\n",
      "* codestral\n",
      "* text-completion-codestral\n",
      "* deepseek\n",
      "* sambanova\n",
      "* maritalk\n",
      "* cloudflare\n",
      "* fireworks_ai\n",
      "* friendliai\n",
      "* watsonx\n",
      "* watsonx_text\n",
      "* triton\n",
      "* predibase\n",
      "* databricks\n",
      "* empower\n",
      "* github\n",
      "* custom\n",
      "* litellm_proxy\n",
      "* hosted_vllm\n",
      "* llamafile\n",
      "* lm_studio\n",
      "* galadriel\n",
      "* gradient_ai\n",
      "* github_copilot\n",
      "* novita\n",
      "* meta_llama\n",
      "* featherless_ai\n",
      "* nscale\n",
      "* nebius\n",
      "* dashscope\n",
      "* moonshot\n",
      "* v0\n",
      "* heroku\n",
      "* oci\n",
      "* morph\n",
      "* lambda_ai\n",
      "* vercel_ai_gateway\n",
      "* wandb\n",
      "* ovhcloud\n",
      "* lemonade\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import litellm\n",
    "    print('Providers\\n=========')\n",
    "    print('* ' + '\\n* '.join(litellm.LITELLM_CHAT_PROVIDERS))\n",
    "    litellm.drop_params = True\n",
    "except ImportError as e:\n",
    "    print(f\"✗ Cannot import litellm: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from typing import List, Optional\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter Configuration Models\n",
    "\n",
    "Define Pydantic models for validating chapter configurations loaded from YAML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Chapter configuration models loaded\n"
     ]
    }
   ],
   "source": [
    "class ChapterConfig(BaseModel):\n",
    "    \"\"\"Configuration for a single chapter to analyze.\"\"\"\n",
    "    file_index: int = Field(..., ge=0, description=\"Index of file in data directory\")\n",
    "    paragraph_start: int = Field(..., ge=0, description=\"Starting paragraph (inclusive)\")\n",
    "    paragraph_end: int = Field(..., gt=0, description=\"Ending paragraph (exclusive)\")\n",
    "    description: str = Field(..., min_length=1, description=\"Human-readable chapter description\")\n",
    "    enabled: bool = Field(default=True, description=\"Whether to process this chapter\")\n",
    "\n",
    "    @property\n",
    "    def paragraph_range(self) -> slice:\n",
    "        \"\"\"Convert to slice for DataSampler.\"\"\"\n",
    "        return slice(self.paragraph_start, self.paragraph_end)\n",
    "\n",
    "\n",
    "class ChaptersConfig(BaseModel):\n",
    "    \"\"\"Root configuration containing all chapters.\"\"\"\n",
    "    chapters: List[ChapterConfig] = Field(..., min_items=1)\n",
    "\n",
    "print(\"✓ Chapter configuration models loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Configuration\n",
    "\n",
    "Configure which LLM to use for segment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model: openai/gpt-4.1-2025-04-14\n",
      "✓ API key from env var: OPENAI_API_KEY\n"
     ]
    }
   ],
   "source": [
    "# Model configuration\n",
    "#model_string = 'together_ai/Qwen/Qwen3-235B-A22B-Thinking-2507'\n",
    "#model_api_key_env_var = 'TOGETHER_AI_API_KEY'\n",
    "\n",
    "# Alternative examples (uncomment to use):\n",
    "#model_string = 'anthropic/claude-sonnet-4-5-20250929'\n",
    "#model_api_key_env_var = 'ANTHROPIC_API_KEY'\n",
    "model_string = 'openai/gpt-4.1-2025-04-14'\n",
    "model_api_key_env_var = 'OPENAI_API_KEY'\n",
    "#model_string = 'mistral/mistral-large-2512'\n",
    "#model_api_key_env_var = 'MISTRAL_API_KEY'\n",
    "\n",
    "print(f\"✓ Model: {model_string}\")\n",
    "print(f\"✓ API key from env var: {model_api_key_env_var}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Base Objects\n",
    "\n",
    "Initialize core components:\n",
    "- `LLM`: LLM interface for analysis\n",
    "- `PromptMaker`: Template rendering engine\n",
    "- `DataSampler`: Text loading with provenance\n",
    "- `SegmentStore`: SQLite catalog for passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Data path: /Users/andersohrn/PycharmProjects/ClaudeCodeCourse/style-retrieval/data/russell\n",
      "✓ Segment database: /Users/andersohrn/PycharmProjects/ClaudeCodeCourse/style-retrieval/segments_openai.db\n",
      "✓ Chapters config: /Users/andersohrn/PycharmProjects/ClaudeCodeCourse/style-retrieval/chapters_config.yaml\n",
      "✓ DataSampler loaded 7 files\n",
      "✓ LLM configured: openai/gpt-4.1-2025-04-14 (temp=0.7)\n",
      "✓ Skip existing chapters: True\n"
     ]
    }
   ],
   "source": [
    "from belletrist import LLM, LLMConfig, PromptMaker, DataSampler, SegmentStore\n",
    "from belletrist.prompts import ExemplarySegmentAnalysisConfig, ExemplarySegmentAnalysis\n",
    "from belletrist.prompts.canonical_tags import get_all_canonical_tags, format_for_jinja\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION - Modify these parameters before running\n",
    "# ============================================================================\n",
    "\n",
    "# Data paths\n",
    "DATA_PATH = Path(os.getcwd()) / \"data\" / \"russell\"\n",
    "SEGMENT_DB_PATH = Path(os.getcwd()) / \"segments_openai.db\"\n",
    "CHAPTERS_CONFIG_PATH = Path(os.getcwd()) / \"chapters_config.yaml\"\n",
    "\n",
    "# Analysis parameters\n",
    "TEMPERATURE = 0.7\n",
    "NUM_SEGMENTS_PER_CHAPTER = 7  # How many passages to extract per chapter\n",
    "\n",
    "# Processing control\n",
    "SKIP_EXISTING_CHAPTERS = True  # Skip chapters already processed\n",
    "\n",
    "# Catalog browsing (for demo at end)\n",
    "CATALOG_PREVIEW_LIMIT = 5  # Number of segments to show in browse demo\n",
    "TAG_PREVIEW_LIMIT = 10  # Number of tags to show in tag list\n",
    "\n",
    "# ============================================================================\n",
    "\n",
    "# Validate configuration\n",
    "if not DATA_PATH.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Data directory not found: {DATA_PATH}\\n\"\n",
    "        f\"Please ensure the data directory exists.\"\n",
    "    )\n",
    "\n",
    "if not CHAPTERS_CONFIG_PATH.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Chapters configuration not found: {CHAPTERS_CONFIG_PATH}\\n\"\n",
    "        f\"Please create chapters_config.yaml with chapter definitions.\"\n",
    "    )\n",
    "\n",
    "# Initialize components\n",
    "prompt_maker = PromptMaker()\n",
    "sampler = DataSampler(data_path=DATA_PATH.resolve())\n",
    "\n",
    "llm = LLM(LLMConfig(\n",
    "    model=model_string,\n",
    "    api_key=os.environ.get(model_api_key_env_var),\n",
    "    temperature=TEMPERATURE,\n",
    "    max_tokens=16384  # Ensure enough tokens for full JSON response\n",
    "))\n",
    "\n",
    "print(f\"✓ Data path: {DATA_PATH}\")\n",
    "print(f\"✓ Segment database: {SEGMENT_DB_PATH}\")\n",
    "print(f\"✓ Chapters config: {CHAPTERS_CONFIG_PATH}\")\n",
    "print(f\"✓ DataSampler loaded {len(sampler.fps)} files\")\n",
    "print(f\"✓ LLM configured: {model_string} (temp={TEMPERATURE})\")\n",
    "print(f\"✓ Skip existing chapters: {SKIP_EXISTING_CHAPTERS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Chapters Configuration\n",
    "\n",
    "Load chapter definitions from YAML file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded 17 enabled chapters\n",
      "  (Skipping 8 disabled chapters)\n",
      "\n",
      "Chapters to process:\n",
      "  1. Chapter 1: Postulates of Modern Educational Theory\n",
      "     File 0, paragraphs 9-41\n",
      "  2. Chapter 2: The Aims of Education\n",
      "     File 0, paragraphs 43-83\n",
      "  3. Chapter 0: Mysticism and Logic\n",
      "     File 1, paragraphs 3-50\n",
      "  4. Chapter 1: Reason and Intuition\n",
      "     File 1, paragraphs 51-60\n",
      "  5. Chapter 3: Time\n",
      "     File 1, paragraphs 67-79\n",
      "  ... and 12 more\n"
     ]
    }
   ],
   "source": [
    "def load_chapters_config(config_path: Path) -> ChaptersConfig:\n",
    "    \"\"\"Load and validate chapters configuration from YAML file.\"\"\"\n",
    "    with open(config_path, 'r') as f:\n",
    "        data = yaml.safe_load(f)\n",
    "    \n",
    "    try:\n",
    "        config = ChaptersConfig(**data)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Invalid chapters configuration: {e}\")\n",
    "    \n",
    "    return config\n",
    "\n",
    "# Load configuration\n",
    "chapters_config = load_chapters_config(CHAPTERS_CONFIG_PATH)\n",
    "enabled_chapters = [ch for ch in chapters_config.chapters if ch.enabled]\n",
    "total_chapters = len(enabled_chapters)\n",
    "\n",
    "print(f\"✓ Loaded {total_chapters} enabled chapters\")\n",
    "print(f\"  (Skipping {len(chapters_config.chapters) - total_chapters} disabled chapters)\")\n",
    "\n",
    "# Preview chapters\n",
    "print(\"\\nChapters to process:\")\n",
    "for i, chapter in enumerate(enabled_chapters[:5], 1):\n",
    "    print(f\"  {i}. {chapter.description}\")\n",
    "    print(f\"     File {chapter.file_index}, paragraphs {chapter.paragraph_start}-{chapter.paragraph_end}\")\n",
    "if len(enabled_chapters) > 5:\n",
    "    print(f\"  ... and {len(enabled_chapters) - 5} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "Define functions for the analysis workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Helper functions loaded\n"
     ]
    }
   ],
   "source": [
    "def chapter_already_processed(\n",
    "    store: SegmentStore,\n",
    "    chapter: ChapterConfig\n",
    ") -> bool:\n",
    "    \"\"\"Check if a chapter has already been processed.\"\"\"\n",
    "    cursor = store.conn.cursor()\n",
    "    cursor.execute(\n",
    "        \"\"\"\n",
    "        SELECT COUNT(*) FROM segments\n",
    "        WHERE file_index = ?\n",
    "        AND paragraph_start >= ?\n",
    "        AND paragraph_end <= ?\n",
    "        \"\"\",\n",
    "        (chapter.file_index, chapter.paragraph_start, chapter.paragraph_end)\n",
    "    )\n",
    "    count = cursor.fetchone()[0]\n",
    "    return count > 0\n",
    "\n",
    "\n",
    "def find_passage_in_chapter(\n",
    "    passage_text: str,\n",
    "    chapter_text: str,\n",
    "    sampler: DataSampler,\n",
    "    file_index: int,\n",
    "    chapter_start_paragraph: int\n",
    ") -> tuple[int, int] | None:\n",
    "    \"\"\"Find paragraph range for a passage within a chapter.\"\"\"\n",
    "    # Normalize text for comparison\n",
    "    normalized_passage = ' '.join(passage_text.split())\n",
    "    \n",
    "    # Check if passage exists in chapter\n",
    "    if normalized_passage not in ' '.join(chapter_text.split()):\n",
    "        return None\n",
    "    \n",
    "    # Iterate through paragraphs to find the match\n",
    "    file_path = sampler.fps[file_index]\n",
    "    max_paragraphs = sampler.n_paragraphs[file_path.name]\n",
    "    \n",
    "    for length in range(1, 10):  # Try up to 10 paragraphs\n",
    "        for start_offset in range(0, 50):  # Search within first 50 paragraphs\n",
    "            abs_start = chapter_start_paragraph + start_offset\n",
    "            abs_end = abs_start + length\n",
    "            \n",
    "            if abs_end > max_paragraphs:\n",
    "                break\n",
    "            \n",
    "            chunk = sampler.get_paragraph_chunk(file_index, slice(abs_start, abs_end))\n",
    "            normalized_chunk = ' '.join(chunk.text.split())\n",
    "            \n",
    "            if normalized_passage in normalized_chunk:\n",
    "                return (abs_start, abs_end)\n",
    "    \n",
    "    return None\n",
    "\n",
    "print(\"✓ Helper functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1 & 2: Analyze and Store Chapters\n",
    "\n",
    "Loop through chapters, analyzing each to identify exemplary passages and storing them in the catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PROCESSING CHAPTERS\n",
      "============================================================\n",
      "Mode: Skip already-processed chapters\n",
      "\n",
      "\n",
      "============================================================\n",
      "CHAPTER 1/17: Chapter 1: Postulates of Modern Educational Theory\n",
      "============================================================\n",
      "File: 0, Paragraphs: 9-41\n",
      "\n",
      "No Tier 2 tags yet - LLM will create author-specific vocabulary\n",
      "\n",
      "[1/3] Loading chapter text...\n",
      "      File: education_and_the_good_life.txt\n",
      "      Length: 41,073 characters\n",
      "\n",
      "[2/3] Analyzing with LLM...\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "      ✓ Identified 7 exemplary passages\n",
      "\n",
      "[3/3] Storing passages in catalog...\n",
      "  [1/7] definition_via_chain_of_utility\n",
      "      ✓ Saved: seg_001 (para 14-15)\n",
      "      Tags: builds_via_refinement, pairs_abstract_and_concrete, technical_term_then_unpacks\n",
      "  [2/7] argument_structured_by_numbered_issues\n",
      "      ✓ Saved: seg_002 (para 19-20)\n",
      "      Tags: builds_via_refinement, layers_parallel_structure, embeds_qualification\n",
      "  [3/7] paired_anecdotes_for_contrast\n",
      "      ✓ Saved: seg_003 (para 30-31)\n",
      "      Tags: builds_via_examples, builds_via_contrast, pairs_abstract_and_concrete\n",
      "  [4/7] shift_from_personal_to_universal\n",
      "      ✓ Saved: seg_004 (para 37-38)\n",
      "      Tags: pivots_from_specific_to_general, builds_via_escalation, varies_sentence_length\n",
      "  [5/7] sarcastic_pivot_and_rhetorical_climax\n",
      "      ✓ Saved: seg_005 (para 26-29)\n",
      "      Tags: pivots_with_but, closes_with_short_sentence, mock_serious_tone\n",
      "  [6/7] personal_anecdote_to_broader_principle\n",
      "      ✓ Saved: seg_006 (para 31-32)\n",
      "      Tags: pivots_from_specific_to_general, uses_cumulative_structure, invites_reader_in\n",
      "  [7/7] balanced_argument_with_caveat_closure\n",
      "      ✓ Saved: seg_007 (para 23-24)\n",
      "      Tags: embeds_qualification, closes_with_short_sentence, balances_clauses\n",
      "\n",
      "✓ Chapter 1 complete: 7 passages stored\n",
      "  Progress: 1/17 chapters processed\n",
      "\n",
      "============================================================\n",
      "CHAPTER 2/17: Chapter 2: The Aims of Education\n",
      "============================================================\n",
      "File: 0, Paragraphs: 43-83\n",
      "\n",
      "Catalog now contains 2 author-specific tags (Tier 2)\n",
      "\n",
      "[1/3] Loading chapter text...\n",
      "      File: education_and_the_good_life.txt\n",
      "      Length: 48,881 characters\n",
      "\n",
      "[2/3] Analyzing with LLM...\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "      ✓ Identified 7 exemplary passages\n",
      "\n",
      "[3/3] Storing passages in catalog...\n",
      "  [1/7] concessive_opening_with_accumulating_examples\n",
      "      ✓ Saved: seg_008 (para 43-44)\n",
      "      Tags: opens_with_concession, builds_via_examples, acknowledges_complexity, invites_reader_in\n",
      "  [2/7] example_driven_argument_with_pivot\n",
      "      ✓ Saved: seg_009 (para 44-45)\n",
      "      Tags: builds_via_examples, pivots_with_but, pairs_abstract_and_concrete\n",
      "  [3/7] contrastive_paragraph_with_cumulative_structure\n",
      "      ✓ Saved: seg_010 (para 46-47)\n",
      "      Tags: builds_via_contrast, uses_cumulative_structure, embeds_qualification\n",
      "  [4/7] definition_through_nuance_and_parallelism\n",
      "      ✓ Saved: seg_011 (para 49-50)\n",
      "      Tags: builds_via_refinement, balances_clauses, embeds_qualification\n",
      "  [5/7] anticipating_counterargument_with_careful_qualification\n",
      "      ✓ Saved: seg_012 (para 54-55)\n",
      "      Tags: addresses_skeptic, embeds_qualification, builds_via_refinement\n",
      "  [6/7] signposting_with_list_and_caveat\n",
      "      ✓ Saved: seg_013 (para 60-61)\n",
      "      Tags: deploys_list_with_variation, embeds_qualification, uses_cumulative_structure\n",
      "  [7/7] escalating_climax_with_varied_sentence_length\n",
      "      ✓ Saved: seg_014 (para 81-82)\n",
      "      Tags: builds_via_escalation, varies_sentence_length, closes_with_short_sentence, parenthetical_asides_for_intimacy\n",
      "\n",
      "✓ Chapter 2 complete: 7 passages stored\n",
      "  Progress: 2/17 chapters processed\n",
      "\n",
      "============================================================\n",
      "CHAPTER 3/17: Chapter 0: Mysticism and Logic\n",
      "============================================================\n",
      "File: 1, Paragraphs: 3-50\n",
      "\n",
      "Catalog now contains 3 author-specific tags (Tier 2)\n",
      "\n",
      "[1/3] Loading chapter text...\n",
      "      File: mysticism_and_logic_and_other_essays.txt\n",
      "      Length: 18,919 characters\n",
      "\n",
      "[2/3] Analyzing with LLM...\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "      ✓ Identified 7 exemplary passages\n",
      "\n",
      "[3/3] Storing passages in catalog...\n",
      "  [1/7] concessive_opening_with_pivot\n",
      "      ✓ Saved: seg_015 (para 3-4)\n",
      "      Tags: opens_with_declaration, builds_via_contrast, pivots_with_but, acknowledges_complexity\n",
      "  [2/7] builds_via_examples_with_explanatory_commentary\n",
      "      ✓ Saved: seg_016 (para 6-7)\n",
      "      Tags: builds_via_examples, technical_term_then_unpacks, pairs_abstract_and_concrete\n",
      "  [3/7] pairs_abstract_and_concrete_with_embedded_qualification\n",
      "      ✓ Saved: seg_017 (para 10-13)\n",
      "      Tags: pairs_abstract_and_concrete, embeds_qualification, builds_via_refinement\n",
      "  [4/7] closes_with_implication_and_metaphoric_flourish\n",
      "      ✓ Saved: seg_018 (para 17-18)\n",
      "      Tags: closes_with_implication, closes_by_widening_scope, parenthetical_asides_for_intimacy\n",
      "  [5/7] pivots_with_but_and_authorial_stance\n",
      "      ✓ Saved: seg_019 (para 33-34)\n",
      "      Tags: pivots_with_but, asserts_with_confidence, balances_clauses, parenthetical_asides_for_intimacy\n",
      "  [6/7] unpacks_definition_via_iteration_and_example\n",
      "      ✓ Saved: seg_020 (para 37-39)\n",
      "      Tags: technical_term_then_unpacks, pairs_abstract_and_concrete, builds_via_refinement\n",
      "  [7/7] builds_argument_via_cumulative_examples_and_contrast\n",
      "      ✓ Saved: seg_021 (para 41-42)\n",
      "      Tags: builds_via_examples, builds_via_contrast, uses_cumulative_structure\n",
      "\n",
      "✓ Chapter 3 complete: 7 passages stored\n",
      "  Progress: 3/17 chapters processed\n",
      "\n",
      "============================================================\n",
      "CHAPTER 4/17: Chapter 1: Reason and Intuition\n",
      "============================================================\n",
      "File: 1, Paragraphs: 51-60\n",
      "\n",
      "Catalog now contains 3 author-specific tags (Tier 2)\n",
      "\n",
      "[1/3] Loading chapter text...\n",
      "      File: mysticism_and_logic_and_other_essays.txt\n",
      "      Length: 11,552 characters\n",
      "\n",
      "[2/3] Analyzing with LLM...\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "✗ ERROR processing chapter 4: LLM response failed schema validation for ExemplarySegmentAnalysis.\n",
      "Validation errors:\n",
      "1 validation error for ExemplarySegmentAnalysis\n",
      "passages.5.tags\n",
      "  List should have at least 3 items after validation, not 2 [type=too_short, input_value=['closes_with_short_sente...e', 'mock_serious_tone'], input_type=list]\n",
      "    For further information visit https://errors.pydantic.dev/2.7/v/too_short\n",
      "Response preview: {\n",
      "  \"passages\": [\n",
      "    {\n",
      "      \"text\": \"Of the reality or unreality of the mystic's world I know nothing. I have no wish to deny it, nor even to declare that the insight which reveals it is not a genuine insight. What I do wish to maintain--and it is here that the scientific attitude becomes imperative--is that insight, untested and unsupported, is an insufficient guarantee of truth, in spite of the fact that much of the most important truth is first suggested by its means. It is common to speak \n",
      "  Continuing with next chapter...\n",
      "\n",
      "============================================================\n",
      "CHAPTER 5/17: Chapter 3: Time\n",
      "============================================================\n",
      "File: 1, Paragraphs: 67-79\n",
      "\n",
      "Catalog now contains 3 author-specific tags (Tier 2)\n",
      "\n",
      "[1/3] Loading chapter text...\n",
      "      File: mysticism_and_logic_and_other_essays.txt\n",
      "      Length: 9,858 characters\n",
      "\n",
      "[2/3] Analyzing with LLM...\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "✗ ERROR processing chapter 5: LLM response failed schema validation for ExemplarySegmentAnalysis.\n",
      "Validation errors:\n",
      "1 validation error for ExemplarySegmentAnalysis\n",
      "passages.0.tags\n",
      "  List should have at least 3 items after validation, not 2 [type=too_short, input_value=['opens_with_declaration'..._abstract_and_concrete'], input_type=list]\n",
      "    For further information visit https://errors.pydantic.dev/2.7/v/too_short\n",
      "Response preview: {\n",
      "  \"passages\": [\n",
      "    {\n",
      "      \"text\": \"The unreality of time is a cardinal doctrine of many metaphysical systems, often nominally based, as already by Parmenides, upon logical arguments, but originally derived, at any rate in the founders of new systems, from the certainty which is born in the moment of mystic insight. As a Persian Sufi poet says:\\n\\n\\\"Past and future are what veil God from our sight.\\n    Burn up both of them with fire! How long\\n    Wilt thou be partitioned by these segments a\n",
      "  Continuing with next chapter...\n",
      "\n",
      "============================================================\n",
      "CHAPTER 6/17: Chapter 4: Good and Evil\n",
      "============================================================\n",
      "File: 1, Paragraphs: 80-93\n",
      "\n",
      "Catalog now contains 3 author-specific tags (Tier 2)\n",
      "\n",
      "[1/3] Loading chapter text...\n",
      "      File: mysticism_and_logic_and_other_essays.txt\n",
      "      Length: 11,343 characters\n",
      "\n",
      "[2/3] Analyzing with LLM...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "✗ ERROR processing chapter 6: LLM response failed schema validation for ExemplarySegmentAnalysis.\n",
      "Validation errors:\n",
      "1 validation error for ExemplarySegmentAnalysis\n",
      "passages.3.tags\n",
      "  List should have at least 3 items after validation, not 2 [type=too_short, input_value=['builds_via_analogy', 'p..._abstract_and_concrete'], input_type=list]\n",
      "    For further information visit https://errors.pydantic.dev/2.7/v/too_short\n",
      "Response preview: {\n",
      "  \"passages\": [\n",
      "    {\n",
      "      \"text\": \"Mysticism maintains that all evil is illusory, and sometimes maintains the same view as regards good, but more often holds that all Reality is good. Both views are to be found in Heraclitus: \\\"Good and ill are one,\\\" he says, but again, \\\"To God all things are fair and good and right, but men hold some things wrong and some right.\\\" A similar twofold position is to be found in Spinoza, but he uses the word \\\"perfection\\\" when he means to speak of the good t\n",
      "  Continuing with next chapter...\n",
      "\n",
      "============================================================\n",
      "CHAPTER 7/17: Chapter 1: Touch and Sight: The Earth and Heavens\n",
      "============================================================\n",
      "File: 2, Paragraphs: 2-13\n",
      "\n",
      "Catalog now contains 3 author-specific tags (Tier 2)\n",
      "\n",
      "[1/3] Loading chapter text...\n",
      "      File: the_abc_of_relativity.txt\n",
      "      Length: 14,636 characters\n",
      "\n",
      "[2/3] Analyzing with LLM...\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "✗ ERROR processing chapter 7: LLM response failed schema validation for ExemplarySegmentAnalysis.\n",
      "Validation errors:\n",
      "1 validation error for ExemplarySegmentAnalysis\n",
      "passages.1.tags\n",
      "  List should have at least 3 items after validation, not 2 [type=too_short, input_value=['builds_via_examples', '..._abstract_and_concrete'], input_type=list]\n",
      "    For further information visit https://errors.pydantic.dev/2.7/v/too_short\n",
      "Response preview: {\n",
      "  \"passages\": [\n",
      "    {\n",
      "      \"text\": \"Everybody knows that Einstein has done something astonishing, but\\nvery few people know exactly what it is that he has done. It is\\ngenerally recognized that he has revolutionized our conception of the\\nphysical world, but his new conceptions are wrapped up in mathematical\\ntechnicalities. It is true that there are innumerable popular\\naccounts of the theory of relativity, but they generally cease to\\nbe intelligible just at the point where they begin to sa\n",
      "  Continuing with next chapter...\n",
      "\n",
      "============================================================\n",
      "CHAPTER 8/17: Chapter 2: What Happens and What is Observed\n",
      "============================================================\n",
      "File: 2, Paragraphs: 14-27\n",
      "\n",
      "Catalog now contains 3 author-specific tags (Tier 2)\n",
      "\n",
      "[1/3] Loading chapter text...\n",
      "      File: the_abc_of_relativity.txt\n",
      "      Length: 16,047 characters\n",
      "\n",
      "[2/3] Analyzing with LLM...\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/litellm/llms/openai/openai.py:745\u001b[39m, in \u001b[36mOpenAIChatCompletion.completion\u001b[39m\u001b[34m(self, model_response, timeout, optional_params, litellm_params, logging_obj, model, messages, print_verbose, api_key, api_base, api_version, dynamic_params, azure_ad_token, acompletion, logger_fn, headers, custom_prompt_dict, client, organization, custom_llm_provider, drop_params, shared_session)\u001b[39m\n\u001b[32m    744\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m745\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    746\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m OpenAIError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/litellm/llms/openai/openai.py:673\u001b[39m, in \u001b[36mOpenAIChatCompletion.completion\u001b[39m\u001b[34m(self, model_response, timeout, optional_params, litellm_params, logging_obj, model, messages, print_verbose, api_key, api_base, api_version, dynamic_params, azure_ad_token, acompletion, logger_fn, headers, custom_prompt_dict, client, organization, custom_llm_provider, drop_params, shared_session)\u001b[39m\n\u001b[32m    659\u001b[39m logging_obj.pre_call(\n\u001b[32m    660\u001b[39m     \u001b[38;5;28minput\u001b[39m=messages,\n\u001b[32m    661\u001b[39m     api_key=openai_client.api_key,\n\u001b[32m   (...)\u001b[39m\u001b[32m    667\u001b[39m     },\n\u001b[32m    668\u001b[39m )\n\u001b[32m    670\u001b[39m (\n\u001b[32m    671\u001b[39m     headers,\n\u001b[32m    672\u001b[39m     response,\n\u001b[32m--> \u001b[39m\u001b[32m673\u001b[39m ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmake_sync_openai_chat_completion_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m    \u001b[49m\u001b[43mopenai_client\u001b[49m\u001b[43m=\u001b[49m\u001b[43mopenai_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    680\u001b[39m logging_obj.model_call_details[\u001b[33m\"\u001b[39m\u001b[33mresponse_headers\u001b[39m\u001b[33m\"\u001b[39m] = headers\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_utils.py:237\u001b[39m, in \u001b[36mtrack_llm_api_timing.<locals>.decorator.<locals>.sync_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    236\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m237\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    238\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/litellm/llms/openai/openai.py:489\u001b[39m, in \u001b[36mOpenAIChatCompletion.make_sync_openai_chat_completion_request\u001b[39m\u001b[34m(self, openai_client, data, timeout, logging_obj)\u001b[39m\n\u001b[32m    488\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m489\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/litellm/llms/openai/openai.py:471\u001b[39m, in \u001b[36mOpenAIChatCompletion.make_sync_openai_chat_completion_request\u001b[39m\u001b[34m(self, openai_client, data, timeout, logging_obj)\u001b[39m\n\u001b[32m    470\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m471\u001b[39m     raw_response = \u001b[43mopenai_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwith_raw_response\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    472\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    473\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    475\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/openai/_legacy_response.py:364\u001b[39m, in \u001b[36mto_raw_response_wrapper.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    362\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33mextra_headers\u001b[39m\u001b[33m\"\u001b[39m] = extra_headers\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(LegacyAPIResponse[R], \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/openai/_utils/_utils.py:286\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py:1189\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1188\u001b[39m validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1189\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1190\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1191\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1192\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1197\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1198\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1199\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1200\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1201\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1202\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1203\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1204\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1205\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1206\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1207\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1208\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1209\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1210\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_retention\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_retention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1211\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1212\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1213\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1214\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1215\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1216\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1217\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1218\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1219\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1220\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1221\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1222\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1225\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1226\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbosity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1227\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1228\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1229\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m   1230\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m   1231\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1232\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1233\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1234\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1235\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1236\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1237\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1238\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1239\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/openai/_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1256\u001b[39m opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m     method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/openai/_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1046\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mBadRequestError\u001b[39m: Error code: 400 - {'error': {'message': \"Invalid schema for response_format 'ExemplarySegmentAnalysis': In context=(), 'additionalProperties' is required to be supplied and to be false.\", 'type': 'invalid_request_error', 'param': 'response_format', 'code': None}}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mOpenAIError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/litellm/main.py:2156\u001b[39m, in \u001b[36mcompletion\u001b[39m\u001b[34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, web_search_options, deployment_id, extra_headers, safety_identifier, service_tier, functions, function_call, base_url, api_version, api_key, model_list, thinking, shared_session, **kwargs)\u001b[39m\n\u001b[32m   2150\u001b[39m     logging.post_call(\n\u001b[32m   2151\u001b[39m         \u001b[38;5;28minput\u001b[39m=messages,\n\u001b[32m   2152\u001b[39m         api_key=api_key,\n\u001b[32m   2153\u001b[39m         original_response=\u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m   2154\u001b[39m         additional_args={\u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m: headers},\n\u001b[32m   2155\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m2156\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   2158\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m optional_params.get(\u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m   2159\u001b[39m     \u001b[38;5;66;03m## LOGGING\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/litellm/main.py:2128\u001b[39m, in \u001b[36mcompletion\u001b[39m\u001b[34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, web_search_options, deployment_id, extra_headers, safety_identifier, service_tier, functions, function_call, base_url, api_version, api_key, model_list, thinking, shared_session, **kwargs)\u001b[39m\n\u001b[32m   2127\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2128\u001b[39m         response = \u001b[43mopenai_chat_completions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2129\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2130\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2131\u001b[39m \u001b[43m            \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2132\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2133\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprint_verbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_verbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2134\u001b[39m \u001b[43m            \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2135\u001b[39m \u001b[43m            \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2136\u001b[39m \u001b[43m            \u001b[49m\u001b[43macompletion\u001b[49m\u001b[43m=\u001b[49m\u001b[43macompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2137\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogging\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2138\u001b[39m \u001b[43m            \u001b[49m\u001b[43moptional_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptional_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2139\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlitellm_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlitellm_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2140\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlogger_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogger_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2141\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[32m   2142\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcustom_prompt_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_prompt_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2143\u001b[39m \u001b[43m            \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pass AsyncOpenAI, OpenAI client\u001b[39;49;00m\n\u001b[32m   2144\u001b[39m \u001b[43m            \u001b[49m\u001b[43morganization\u001b[49m\u001b[43m=\u001b[49m\u001b[43morganization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2145\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2146\u001b[39m \u001b[43m            \u001b[49m\u001b[43mshared_session\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshared_session\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2147\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2148\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   2149\u001b[39m     \u001b[38;5;66;03m## LOGGING - log the original exception returned\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/litellm/llms/openai/openai.py:756\u001b[39m, in \u001b[36mOpenAIChatCompletion.completion\u001b[39m\u001b[34m(self, model_response, timeout, optional_params, litellm_params, logging_obj, model, messages, print_verbose, api_key, api_base, api_version, dynamic_params, azure_ad_token, acompletion, logger_fn, headers, custom_prompt_dict, client, organization, custom_llm_provider, drop_params, shared_session)\u001b[39m\n\u001b[32m    755\u001b[39m     error_headers = \u001b[38;5;28mgetattr\u001b[39m(error_response, \u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m756\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[32m    757\u001b[39m     status_code=status_code,\n\u001b[32m    758\u001b[39m     message=error_text,\n\u001b[32m    759\u001b[39m     headers=error_headers,\n\u001b[32m    760\u001b[39m     body=error_body,\n\u001b[32m    761\u001b[39m )\n",
      "\u001b[31mOpenAIError\u001b[39m: Error code: 400 - {'error': {'message': \"Invalid schema for response_format 'ExemplarySegmentAnalysis': In context=(), 'additionalProperties' is required to be supplied and to be false.\", 'type': 'invalid_request_error', 'param': 'response_format', 'code': None}}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/ClaudeCodeCourse/style-retrieval/belletrist/llm.py:248\u001b[39m, in \u001b[36mLLM.complete_with_schema\u001b[39m\u001b[34m(self, prompt, schema_model, system, strict, **kwargs)\u001b[39m\n\u001b[32m    247\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m248\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcomplete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    250\u001b[39m     \u001b[38;5;66;03m# If strict schema fails (provider doesn't support it), retry with json_object mode\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/ClaudeCodeCourse/style-retrieval/belletrist/llm.py:153\u001b[39m, in \u001b[36mLLM.complete\u001b[39m\u001b[34m(self, prompt, system, **kwargs)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;66;03m# Execute the completion\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m raw_response = \u001b[43mlitellm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrequest_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[38;5;66;03m# Parse and return the response\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/litellm/utils.py:1377\u001b[39m, in \u001b[36mclient.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1374\u001b[39m     logging_obj.failure_handler(\n\u001b[32m   1375\u001b[39m         e, traceback_exception, start_time, end_time\n\u001b[32m   1376\u001b[39m     )  \u001b[38;5;66;03m# DO NOT MAKE THREADED - router retry fallback relies on this!\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1377\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/litellm/utils.py:1246\u001b[39m, in \u001b[36mclient.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1246\u001b[39m result = \u001b[43moriginal_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1247\u001b[39m end_time = datetime.datetime.now()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/litellm/main.py:3770\u001b[39m, in \u001b[36mcompletion\u001b[39m\u001b[34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, web_search_options, deployment_id, extra_headers, safety_identifier, service_tier, functions, function_call, base_url, api_version, api_key, model_list, thinking, shared_session, **kwargs)\u001b[39m\n\u001b[32m   3768\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   3769\u001b[39m     \u001b[38;5;66;03m## Map to OpenAI Exception\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3770\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mexception_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3771\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3772\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3773\u001b[39m \u001b[43m        \u001b[49m\u001b[43moriginal_exception\u001b[49m\u001b[43m=\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3774\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompletion_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3775\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3776\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:2323\u001b[39m, in \u001b[36mexception_type\u001b[39m\u001b[34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[39m\n\u001b[32m   2322\u001b[39m     \u001b[38;5;28msetattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mlitellm_response_headers\u001b[39m\u001b[33m\"\u001b[39m, litellm_response_headers)\n\u001b[32m-> \u001b[39m\u001b[32m2323\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:412\u001b[39m, in \u001b[36mexception_type\u001b[39m\u001b[34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[39m\n\u001b[32m    411\u001b[39m     exception_mapping_worked = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m412\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m BadRequestError(\n\u001b[32m    413\u001b[39m         message=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexception_provider\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    414\u001b[39m         llm_provider=custom_llm_provider,\n\u001b[32m    415\u001b[39m         model=model,\n\u001b[32m    416\u001b[39m         response=\u001b[38;5;28mgetattr\u001b[39m(original_exception, \u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    417\u001b[39m         litellm_debug_info=extra_information,\n\u001b[32m    418\u001b[39m         body=\u001b[38;5;28mgetattr\u001b[39m(original_exception, \u001b[33m\"\u001b[39m\u001b[33mbody\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    419\u001b[39m     )\n\u001b[32m    420\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[32m    421\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mWeb server is returning an unknown error\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error_str\n\u001b[32m    422\u001b[39m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mThe server had an error processing your request.\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error_str\n\u001b[32m    423\u001b[39m ):\n",
      "\u001b[31mBadRequestError\u001b[39m: litellm.BadRequestError: OpenAIException - Invalid schema for response_format 'ExemplarySegmentAnalysis': In context=(), 'additionalProperties' is required to be supplied and to be false.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 70\u001b[39m\n\u001b[32m     61\u001b[39m config = ExemplarySegmentAnalysisConfig(\n\u001b[32m     62\u001b[39m     chapter_text=chapter_segment.text,\n\u001b[32m     63\u001b[39m     file_name=chapter_segment.file_path.name,\n\u001b[32m   (...)\u001b[39m\u001b[32m     66\u001b[39m     canonical_tags_formatted=canonical_tags_formatted\n\u001b[32m     67\u001b[39m )\n\u001b[32m     68\u001b[39m prompt = prompt_maker.render(config)\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m response = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcomplete_with_schema\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschema_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mExemplarySegmentAnalysis\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m analysis = response.content\n\u001b[32m     76\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m      ✓ Identified \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(analysis.passages)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m exemplary passages\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/ClaudeCodeCourse/style-retrieval/belletrist/llm.py:254\u001b[39m, in \u001b[36mLLM.complete_with_schema\u001b[39m\u001b[34m(self, prompt, schema_model, system, strict, **kwargs)\u001b[39m\n\u001b[32m    251\u001b[39m error_msg = \u001b[38;5;28mstr\u001b[39m(e).lower()\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m strict \u001b[38;5;129;01mand\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33mschema\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error_msg \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mstrict\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error_msg \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mjson_schema\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m error_msg):\n\u001b[32m    253\u001b[39m     \u001b[38;5;66;03m# Fallback to json_object mode\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m254\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcomplete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtype\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mjson_object\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    255\u001b[39m     validation_mode = \u001b[33m\"\u001b[39m\u001b[33mfallback\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    256\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     \u001b[38;5;66;03m# Different error, re-raise\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/PycharmProjects/ClaudeCodeCourse/style-retrieval/belletrist/llm.py:153\u001b[39m, in \u001b[36mLLM.complete\u001b[39m\u001b[34m(self, prompt, system, **kwargs)\u001b[39m\n\u001b[32m    150\u001b[39m request_params = \u001b[38;5;28mself\u001b[39m._build_request_params(messages, **kwargs)\n\u001b[32m    152\u001b[39m \u001b[38;5;66;03m# Execute the completion\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m raw_response = \u001b[43mlitellm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrequest_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[38;5;66;03m# Parse and return the response\u001b[39;00m\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._parse_response(raw_response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/litellm/utils.py:1246\u001b[39m, in \u001b[36mclient.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1244\u001b[39m         print_verbose(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError while checking max token limit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1245\u001b[39m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1246\u001b[39m result = \u001b[43moriginal_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1247\u001b[39m end_time = datetime.datetime.now()\n\u001b[32m   1248\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_streaming_request(\n\u001b[32m   1249\u001b[39m     kwargs=kwargs,\n\u001b[32m   1250\u001b[39m     call_type=call_type,\n\u001b[32m   1251\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/litellm/main.py:2128\u001b[39m, in \u001b[36mcompletion\u001b[39m\u001b[34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, web_search_options, deployment_id, extra_headers, safety_identifier, service_tier, functions, function_call, base_url, api_version, api_key, model_list, thinking, shared_session, **kwargs)\u001b[39m\n\u001b[32m   2108\u001b[39m         response = base_llm_http_handler.completion(\n\u001b[32m   2109\u001b[39m             model=model,\n\u001b[32m   2110\u001b[39m             messages=messages,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2125\u001b[39m             provider_config=provider_config,\n\u001b[32m   2126\u001b[39m         )\n\u001b[32m   2127\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2128\u001b[39m         response = \u001b[43mopenai_chat_completions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2129\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2130\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2131\u001b[39m \u001b[43m            \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2132\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2133\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprint_verbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_verbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2134\u001b[39m \u001b[43m            \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2135\u001b[39m \u001b[43m            \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2136\u001b[39m \u001b[43m            \u001b[49m\u001b[43macompletion\u001b[49m\u001b[43m=\u001b[49m\u001b[43macompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2137\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogging\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2138\u001b[39m \u001b[43m            \u001b[49m\u001b[43moptional_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptional_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2139\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlitellm_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlitellm_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2140\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlogger_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogger_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2141\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[32m   2142\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcustom_prompt_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_prompt_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2143\u001b[39m \u001b[43m            \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pass AsyncOpenAI, OpenAI client\u001b[39;49;00m\n\u001b[32m   2144\u001b[39m \u001b[43m            \u001b[49m\u001b[43morganization\u001b[49m\u001b[43m=\u001b[49m\u001b[43morganization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2145\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2146\u001b[39m \u001b[43m            \u001b[49m\u001b[43mshared_session\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshared_session\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2147\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2148\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   2149\u001b[39m     \u001b[38;5;66;03m## LOGGING - log the original exception returned\u001b[39;00m\n\u001b[32m   2150\u001b[39m     logging.post_call(\n\u001b[32m   2151\u001b[39m         \u001b[38;5;28minput\u001b[39m=messages,\n\u001b[32m   2152\u001b[39m         api_key=api_key,\n\u001b[32m   2153\u001b[39m         original_response=\u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m   2154\u001b[39m         additional_args={\u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m: headers},\n\u001b[32m   2155\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/litellm/llms/openai/openai.py:673\u001b[39m, in \u001b[36mOpenAIChatCompletion.completion\u001b[39m\u001b[34m(self, model_response, timeout, optional_params, litellm_params, logging_obj, model, messages, print_verbose, api_key, api_base, api_version, dynamic_params, azure_ad_token, acompletion, logger_fn, headers, custom_prompt_dict, client, organization, custom_llm_provider, drop_params, shared_session)\u001b[39m\n\u001b[32m    658\u001b[39m \u001b[38;5;66;03m## LOGGING\u001b[39;00m\n\u001b[32m    659\u001b[39m logging_obj.pre_call(\n\u001b[32m    660\u001b[39m     \u001b[38;5;28minput\u001b[39m=messages,\n\u001b[32m    661\u001b[39m     api_key=openai_client.api_key,\n\u001b[32m   (...)\u001b[39m\u001b[32m    667\u001b[39m     },\n\u001b[32m    668\u001b[39m )\n\u001b[32m    670\u001b[39m (\n\u001b[32m    671\u001b[39m     headers,\n\u001b[32m    672\u001b[39m     response,\n\u001b[32m--> \u001b[39m\u001b[32m673\u001b[39m ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmake_sync_openai_chat_completion_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m    \u001b[49m\u001b[43mopenai_client\u001b[49m\u001b[43m=\u001b[49m\u001b[43mopenai_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    680\u001b[39m logging_obj.model_call_details[\u001b[33m\"\u001b[39m\u001b[33mresponse_headers\u001b[39m\u001b[33m\"\u001b[39m] = headers\n\u001b[32m    681\u001b[39m stringified_response = response.model_dump()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/litellm/litellm_core_utils/logging_utils.py:237\u001b[39m, in \u001b[36mtrack_llm_api_timing.<locals>.decorator.<locals>.sync_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    234\u001b[39m parent_otel_span = _get_parent_otel_span_from_logging_obj(logging_obj)\n\u001b[32m    236\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m237\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    238\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/litellm/llms/openai/openai.py:471\u001b[39m, in \u001b[36mOpenAIChatCompletion.make_sync_openai_chat_completion_request\u001b[39m\u001b[34m(self, openai_client, data, timeout, logging_obj)\u001b[39m\n\u001b[32m    469\u001b[39m raw_response = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    470\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m471\u001b[39m     raw_response = \u001b[43mopenai_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwith_raw_response\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    472\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    473\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    475\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    476\u001b[39m         headers = \u001b[38;5;28mdict\u001b[39m(raw_response.headers)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/openai/_legacy_response.py:364\u001b[39m, in \u001b[36mto_raw_response_wrapper.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    360\u001b[39m extra_headers[RAW_RESPONSE_HEADER] = \u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    362\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33mextra_headers\u001b[39m\u001b[33m\"\u001b[39m] = extra_headers\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(LegacyAPIResponse[R], \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/openai/_utils/_utils.py:286\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    284\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py:1189\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1142\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1143\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mcreate\u001b[39m(\n\u001b[32m   1144\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1186\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m   1187\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m   1188\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1189\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1190\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1191\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1192\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1197\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1198\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1199\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1200\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1201\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1202\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1203\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1204\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1205\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1206\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1207\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1208\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1209\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1210\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_retention\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_retention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1211\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1212\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1213\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1214\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1215\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1216\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1217\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1218\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1219\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1220\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1221\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1222\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1225\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1226\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbosity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1227\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1228\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1229\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m   1230\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m   1231\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1232\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1233\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1234\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1235\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1236\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1238\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1239\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/openai/_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/openai/_base_client.py:982\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m    980\u001b[39m response = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    981\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m982\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    987\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    988\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mEncountered httpx.TimeoutException\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/httpx/_client.py:901\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    893\u001b[39m follow_redirects = (\n\u001b[32m    894\u001b[39m     \u001b[38;5;28mself\u001b[39m.follow_redirects\n\u001b[32m    895\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(follow_redirects, UseClientDefault)\n\u001b[32m    896\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m follow_redirects\n\u001b[32m    897\u001b[39m )\n\u001b[32m    899\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m901\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    902\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    903\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    904\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    906\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    907\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    908\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/httpx/_client.py:929\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    926\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    928\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m929\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    934\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    935\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/httpx/_client.py:966\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    963\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    964\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m966\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    968\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/httpx/_client.py:1002\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    997\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    998\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    999\u001b[39m     )\n\u001b[32m   1001\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1002\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1006\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/httpx/_transports/default.py:228\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    215\u001b[39m req = httpcore.Request(\n\u001b[32m    216\u001b[39m     method=request.method,\n\u001b[32m    217\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    225\u001b[39m     extensions=request.extensions,\n\u001b[32m    226\u001b[39m )\n\u001b[32m    227\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m228\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    230\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    233\u001b[39m     status_code=resp.status,\n\u001b[32m    234\u001b[39m     headers=resp.headers,\n\u001b[32m    235\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    236\u001b[39m     extensions=resp.extensions,\n\u001b[32m    237\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:268\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    266\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ShieldCancellation():\n\u001b[32m    267\u001b[39m         \u001b[38;5;28mself\u001b[39m.response_closed(status)\n\u001b[32m--> \u001b[39m\u001b[32m268\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m    269\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    270\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:251\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    248\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m    250\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    253\u001b[39m     \u001b[38;5;66;03m# The ConnectionNotAvailable exception is a special case, that\u001b[39;00m\n\u001b[32m    254\u001b[39m     \u001b[38;5;66;03m# indicates we need to retry the request on a new connection.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    258\u001b[39m     \u001b[38;5;66;03m# might end up as an HTTP/2 connection, but which actually ends\u001b[39;00m\n\u001b[32m    259\u001b[39m     \u001b[38;5;66;03m# up as HTTP/1.1.\u001b[39;00m\n\u001b[32m    260\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pool_lock:\n\u001b[32m    261\u001b[39m         \u001b[38;5;66;03m# Maintain our position in the request queue, but reset the\u001b[39;00m\n\u001b[32m    262\u001b[39m         \u001b[38;5;66;03m# status so that the request becomes queued again.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/httpcore/_sync/connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection.is_available():\n\u001b[32m    101\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m ConnectionNotAvailable()\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/httpcore/_sync/http11.py:133\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    131\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    132\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/httpcore/_sync/http11.py:111\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m    104\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m    105\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    106\u001b[39m     (\n\u001b[32m    107\u001b[39m         http_version,\n\u001b[32m    108\u001b[39m         status,\n\u001b[32m    109\u001b[39m         reason_phrase,\n\u001b[32m    110\u001b[39m         headers,\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    112\u001b[39m     trace.return_value = (\n\u001b[32m    113\u001b[39m         http_version,\n\u001b[32m    114\u001b[39m         status,\n\u001b[32m    115\u001b[39m         reason_phrase,\n\u001b[32m    116\u001b[39m         headers,\n\u001b[32m    117\u001b[39m     )\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    120\u001b[39m     status=status,\n\u001b[32m    121\u001b[39m     headers=headers,\n\u001b[32m   (...)\u001b[39m\u001b[32m    127\u001b[39m     },\n\u001b[32m    128\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/httpcore/_sync/http11.py:176\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    173\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    178\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/httpcore/_sync/http11.py:212\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    209\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    216\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    217\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    218\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    224\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/httpcore/_backends/sync.py:126\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    125\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/Cellar/python@3.11/3.11.9_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/ssl.py:1295\u001b[39m, in \u001b[36mSSLSocket.recv\u001b[39m\u001b[34m(self, buflen, flags)\u001b[39m\n\u001b[32m   1291\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1292\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1293\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1294\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1295\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1296\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1297\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv(buflen, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/Cellar/python@3.11/3.11.9_1/Frameworks/Python.framework/Versions/3.11/lib/python3.11/ssl.py:1168\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1166\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[32m   1167\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1168\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[32m   1170\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m x.args[\u001b[32m0\u001b[39m] == SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.suppress_ragged_eofs:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Open segment store\n",
    "store = SegmentStore(SEGMENT_DB_PATH)\n",
    "\n",
    "# Track progress\n",
    "all_segment_ids = []\n",
    "processed_count = 0\n",
    "skipped_count = 0\n",
    "failed_chapters = []\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PROCESSING CHAPTERS\")\n",
    "print(\"=\"*60)\n",
    "if SKIP_EXISTING_CHAPTERS:\n",
    "    print(\"Mode: Skip already-processed chapters\\n\")\n",
    "else:\n",
    "    print(\"Mode: Reprocess all chapters\\n\")\n",
    "\n",
    "for chapter_idx, chapter in enumerate(enabled_chapters, 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"CHAPTER {chapter_idx}/{total_chapters}: {chapter.description}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"File: {chapter.file_index}, Paragraphs: {chapter.paragraph_start}-{chapter.paragraph_end}\")\n",
    "    \n",
    "    # Check if already processed\n",
    "    if SKIP_EXISTING_CHAPTERS and chapter_already_processed(store, chapter):\n",
    "        print(f\"\\n⏭ Skipping - chapter already processed\")\n",
    "        skipped_count += 1\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        # Get existing tags and filter to Tier 2 only\n",
    "        existing_tags_dict = store.list_all_tags()\n",
    "        all_tags = list(existing_tags_dict.keys()) if existing_tags_dict else []\n",
    "        \n",
    "        # Filter out canonical tags to get only Tier 2\n",
    "        canonical_tag_set = get_all_canonical_tags()\n",
    "        existing_tier2_tags = [tag for tag in all_tags if tag not in canonical_tag_set]\n",
    "        \n",
    "        # Get formatted canonical tags for template injection\n",
    "        canonical_tags_formatted = format_for_jinja()\n",
    "        \n",
    "        if chapter_idx == 1:\n",
    "            if existing_tier2_tags:\n",
    "                print(f\"\\nCatalog contains {len(existing_tier2_tags)} author-specific tags (Tier 2)\")\n",
    "                print(f\"Will encourage reuse for consistency\")\n",
    "            else:\n",
    "                print(\"\\nNo Tier 2 tags yet - LLM will create author-specific vocabulary\")\n",
    "        else:\n",
    "            print(f\"\\nCatalog now contains {len(existing_tier2_tags)} author-specific tags (Tier 2)\")\n",
    "        \n",
    "        # PHASE 1: ANALYSIS\n",
    "        print(\"\\n[1/3] Loading chapter text...\")\n",
    "        chapter_segment = sampler.get_paragraph_chunk(\n",
    "            chapter.file_index,\n",
    "            chapter.paragraph_range\n",
    "        )\n",
    "        print(f\"      File: {chapter_segment.file_path.name}\")\n",
    "        print(f\"      Length: {len(chapter_segment.text):,} characters\")\n",
    "        \n",
    "        print(\"\\n[2/3] Analyzing with LLM...\")\n",
    "        config = ExemplarySegmentAnalysisConfig(\n",
    "            chapter_text=chapter_segment.text,\n",
    "            file_name=chapter_segment.file_path.name,\n",
    "            num_segments=NUM_SEGMENTS_PER_CHAPTER,\n",
    "            existing_tier2_tags=existing_tier2_tags,\n",
    "            canonical_tags_formatted=canonical_tags_formatted\n",
    "        )\n",
    "        prompt = prompt_maker.render(config)\n",
    "        \n",
    "        response = llm.complete_with_schema(\n",
    "            prompt=prompt,\n",
    "            schema_model=ExemplarySegmentAnalysis\n",
    "        )\n",
    "        analysis = response.content\n",
    "        \n",
    "        print(f\"      ✓ Identified {len(analysis.passages)} exemplary passages\")\n",
    "        if analysis.overall_observations:\n",
    "            print(f\"      Observations: {analysis.overall_observations[:100]}...\")\n",
    "        \n",
    "        # PHASE 2: STORAGE\n",
    "        print(\"\\n[3/3] Storing passages in catalog...\")\n",
    "        chapter_segment_ids = []\n",
    "        \n",
    "        for i, passage in enumerate(analysis.passages, 1):\n",
    "            print(f\"  [{i}/{len(analysis.passages)}] {passage.craft_move}\")\n",
    "            \n",
    "            # Find passage location\n",
    "            para_range = find_passage_in_chapter(\n",
    "                passage.text,\n",
    "                chapter_segment.text,\n",
    "                sampler,\n",
    "                chapter.file_index,\n",
    "                chapter.paragraph_start\n",
    "            )\n",
    "            \n",
    "            if para_range is None:\n",
    "                print(f\"      ⚠ Warning: Could not locate passage, using approximate range\")\n",
    "                para_start = chapter.paragraph_start\n",
    "                para_end = chapter.paragraph_start + 1\n",
    "            else:\n",
    "                para_start, para_end = para_range\n",
    "            \n",
    "            # Get TextSegment with provenance\n",
    "            text_segment = sampler.get_paragraph_chunk(\n",
    "                chapter.file_index,\n",
    "                slice(para_start, para_end)\n",
    "            )\n",
    "            \n",
    "            # Save to catalog\n",
    "            segment_id = store.save_segment(\n",
    "                text_segment=text_segment,\n",
    "                craft_move=passage.craft_move,\n",
    "                teaching_note=passage.teaching_note,\n",
    "                tags=passage.tags\n",
    "            )\n",
    "            \n",
    "            chapter_segment_ids.append(segment_id)\n",
    "            print(f\"      ✓ Saved: {segment_id} (para {para_start}-{para_end})\")\n",
    "            print(f\"      Tags: {', '.join(passage.tags)}\")\n",
    "        \n",
    "        all_segment_ids.extend(chapter_segment_ids)\n",
    "        processed_count += 1\n",
    "        \n",
    "        print(f\"\\n✓ Chapter {chapter_idx} complete: {len(chapter_segment_ids)} passages stored\")\n",
    "        print(f\"  Progress: {processed_count}/{total_chapters} chapters processed\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n✗ ERROR processing chapter {chapter_idx}: {e}\")\n",
    "        failed_chapters.append((chapter_idx, chapter.description, str(e)))\n",
    "        print(f\"  Continuing with next chapter...\")\n",
    "        continue\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"CHAPTER PROCESSING COMPLETE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Successfully processed: {processed_count}/{total_chapters} chapters\")\n",
    "if skipped_count > 0:\n",
    "    print(f\"Skipped (already processed): {skipped_count} chapters\")\n",
    "print(f\"Total segments stored (this run): {len(all_segment_ids)}\")\n",
    "\n",
    "if failed_chapters:\n",
    "    print(f\"\\nFailed chapters ({len(failed_chapters)}):\")\n",
    "    for idx, desc, error in failed_chapters:\n",
    "        print(f\"  - Chapter {idx} ({desc}): {error}\")\n",
    "elif processed_count > 0:\n",
    "    print(\"\\n✓ All processed chapters completed successfully!\")\n",
    "\n",
    "if skipped_count == total_chapters:\n",
    "    print(\"\\n⏭ All chapters were already processed - no new segments added\")\n",
    "    print(\"  Set SKIP_EXISTING_CHAPTERS=False to reprocess\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 3: Catalog Browsing Demo\n",
    "\n",
    "Demonstrate the skills pattern: how agents browse the catalog to find and retrieve passages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"CATALOG BROWSING DEMONSTRATION (Skills Pattern)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Step 1: List available tags\n",
    "print(\"\\n[1/3] Listing available tags...\")\n",
    "tags = store.list_all_tags()\n",
    "print(f\"      Found {len(tags)} unique tags in catalog\")\n",
    "print(f\"\\n      Top {TAG_PREVIEW_LIMIT} tags:\")\n",
    "for tag, count in list(tags.items())[:TAG_PREVIEW_LIMIT]:\n",
    "    print(f\"      - {tag}: {count} segments\")\n",
    "\n",
    "# Step 2: Browse catalog summaries\n",
    "print(f\"\\n[2/3] Browsing catalog (showing {CATALOG_PREVIEW_LIMIT} segments)...\")\n",
    "catalog = store.browse_catalog(limit=CATALOG_PREVIEW_LIMIT)\n",
    "print(f\"      Retrieved {len(catalog)} segment summaries\")\n",
    "for i, entry in enumerate(catalog, 1):\n",
    "    print(f\"\\n      Segment {i}/{len(catalog)}: {entry['segment_id']}\")\n",
    "    print(f\"      File: {entry['file_name']}\")\n",
    "    print(f\"      Range: paragraphs {entry['paragraph_range']}\")\n",
    "    print(f\"      Craft Move: {entry['craft_move']}\")\n",
    "    print(f\"      Teaching Note: {entry['teaching_note'][:80]}...\")\n",
    "    print(f\"      Tags: {', '.join(entry['tags'])}\")\n",
    "\n",
    "# Step 3: Retrieve a specific segment\n",
    "if catalog:\n",
    "    segment_id = catalog[0]['segment_id']\n",
    "    print(f\"\\n[3/3] Retrieving full text for segment: {segment_id}\")\n",
    "    record = store.get_segment(segment_id)\n",
    "    \n",
    "    if record:\n",
    "        print(f\"      ✓ Retrieved {len(record.text)} characters\")\n",
    "        print(f\"\\n      Preview (first 300 chars):\")\n",
    "        print(f\"      {record.text[:300]}...\")\n",
    "        \n",
    "        # Demonstrate conversion back to TextSegment\n",
    "        text_segment = record.to_text_segment(sampler)\n",
    "        print(f\"\\n      ✓ Re-retrieved via DataSampler:\")\n",
    "        print(f\"        File: {text_segment.file_path.name}\")\n",
    "        print(f\"        Range: {text_segment.paragraph_start}-{text_segment.paragraph_end}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "Show catalog statistics and suggest next steps for agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"WORKFLOW COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nSegment catalog saved to: {SEGMENT_DB_PATH}\")\n",
    "print(f\"\\nThis run:\")\n",
    "print(f\"  Chapters processed: {processed_count}/{total_chapters}\")\n",
    "if skipped_count > 0:\n",
    "    print(f\"  Chapters skipped: {skipped_count}\")\n",
    "print(f\"  New segments stored: {len(all_segment_ids)}\")\n",
    "\n",
    "if all_segment_ids:\n",
    "    print(f\"    First: {all_segment_ids[0]}\")\n",
    "    print(f\"    Last:  {all_segment_ids[-1]}\")\n",
    "\n",
    "# Show total catalog size\n",
    "total_in_catalog = store.get_count()\n",
    "total_tags = len(store.list_all_tags())\n",
    "\n",
    "print(f\"\\nCatalog totals:\")\n",
    "print(f\"  Total segments: {total_in_catalog}\")\n",
    "print(f\"  Unique tags: {total_tags}\")\n",
    "\n",
    "print(\"\\nNext steps - agents can now:\")\n",
    "print(\"  • store.list_all_tags() → discover available categories\")\n",
    "print(\"  • store.browse_catalog() → read segment descriptions\")\n",
    "print(\"  • store.get_segment(id) → retrieve full text\")\n",
    "print(\"  • store.search_by_tag(tag) → filter by form/function\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Close Store\n",
    "\n",
    "Clean up database connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store.close()\n",
    "print(\"✓ Database connection closed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (russell_writes)",
   "language": "python",
   "name": "russell_writes"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
