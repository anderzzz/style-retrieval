{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Style Segmentor: Building a Catalog of Exemplary Passages\n",
    "\n",
    "This notebook builds a segment catalog by analyzing chapters and extracting exemplary passages that demonstrate specific craft moves.\n",
    "\n",
    "## Workflow\n",
    "\n",
    "For each chapter defined in `chapters_config.yaml`:\n",
    "1. **Analyze**: LLM identifies exemplary passages demonstrating teachable craft moves\n",
    "2. **Store**: Save passages to SQLite catalog with craft annotations (craft_move, teaching_note, tags)\n",
    "3. **Browse**: Demonstrate catalog browsing pattern (list tags → browse summaries → retrieve full text)\n",
    "\n",
    "## Output\n",
    "\n",
    "- **segments.db**: SQLite database containing annotated passages ready for agent retrieval\n",
    "- Agents can browse by tags, search by craft move, and retrieve full text with provenance\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- **Skip existing chapters**: Resume after interruption without re-analyzing\n",
    "- **Tag consistency**: Encourages reuse of existing tags across chapters\n",
    "- **Provenance tracking**: Every segment includes file index and paragraph range\n",
    "- **Skills pattern**: Catalog designed for agent browsing, not pre-loaded prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Libraries and Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import litellm\n",
    "    print('Providers\\n=========')\n",
    "    print('* ' + '\\n* '.join(litellm.LITELLM_CHAT_PROVIDERS))\n",
    "    litellm.drop_params = True\n",
    "except ImportError as e:\n",
    "    print(f\"✗ Cannot import litellm: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from typing import List, Optional\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter Configuration Models\n",
    "\n",
    "Define Pydantic models for validating chapter configurations loaded from YAML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChapterConfig(BaseModel):\n",
    "    \"\"\"Configuration for a single chapter to analyze.\"\"\"\n",
    "    file_index: int = Field(..., ge=0, description=\"Index of file in data directory\")\n",
    "    paragraph_start: int = Field(..., ge=0, description=\"Starting paragraph (inclusive)\")\n",
    "    paragraph_end: int = Field(..., gt=0, description=\"Ending paragraph (exclusive)\")\n",
    "    description: str = Field(..., min_length=1, description=\"Human-readable chapter description\")\n",
    "    enabled: bool = Field(default=True, description=\"Whether to process this chapter\")\n",
    "\n",
    "    @property\n",
    "    def paragraph_range(self) -> slice:\n",
    "        \"\"\"Convert to slice for DataSampler.\"\"\"\n",
    "        return slice(self.paragraph_start, self.paragraph_end)\n",
    "\n",
    "\n",
    "class ChaptersConfig(BaseModel):\n",
    "    \"\"\"Root configuration containing all chapters.\"\"\"\n",
    "    chapters: List[ChapterConfig] = Field(..., min_items=1)\n",
    "\n",
    "print(\"✓ Chapter configuration models loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Configuration\n",
    "\n",
    "Configure which LLM to use for segment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "model_string = 'together_ai/Qwen/Qwen3-235B-A22B-Thinking-2507'\n",
    "model_api_key_env_var = 'TOGETHER_AI_API_KEY'\n",
    "\n",
    "# Alternative examples (uncomment to use):\n",
    "#model_string = 'anthropic/claude-sonnet-4-5-20250929'\n",
    "#model_api_key_env_var = 'ANTHROPIC_API_KEY'\n",
    "#model_string = 'openai/gpt-4o'\n",
    "#model_api_key_env_var = 'OPENAI_API_KEY'\n",
    "#model_string = 'mistral/mistral-large-2512'\n",
    "#model_api_key_env_var = 'MISTRAL_API_KEY'\n",
    "\n",
    "print(f\"✓ Model: {model_string}\")\n",
    "print(f\"✓ API key from env var: {model_api_key_env_var}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Base Objects\n",
    "\n",
    "Initialize core components:\n",
    "- `LLM`: LLM interface for analysis\n",
    "- `PromptMaker`: Template rendering engine\n",
    "- `DataSampler`: Text loading with provenance\n",
    "- `SegmentStore`: SQLite catalog for passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from belletrist import LLM, LLMConfig, PromptMaker, DataSampler, SegmentStore\n",
    "from belletrist.prompts import ExemplarySegmentAnalysisConfig, ExemplarySegmentAnalysis\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION - Modify these parameters before running\n",
    "# ============================================================================\n",
    "\n",
    "# Data paths\n",
    "DATA_PATH = Path(os.getcwd()) / \"data\" / \"russell\"\n",
    "SEGMENT_DB_PATH = Path(os.getcwd()) / \"segments.db\"\n",
    "CHAPTERS_CONFIG_PATH = Path(os.getcwd()) / \"chapters_config.yaml\"\n",
    "\n",
    "# Analysis parameters\n",
    "TEMPERATURE = 0.7\n",
    "NUM_SEGMENTS_PER_CHAPTER = 5  # How many passages to extract per chapter\n",
    "\n",
    "# Processing control\n",
    "SKIP_EXISTING_CHAPTERS = True  # Skip chapters already processed\n",
    "\n",
    "# Catalog browsing (for demo at end)\n",
    "CATALOG_PREVIEW_LIMIT = 5  # Number of segments to show in browse demo\n",
    "TAG_PREVIEW_LIMIT = 10  # Number of tags to show in tag list\n",
    "\n",
    "# ============================================================================\n",
    "\n",
    "# Validate configuration\n",
    "if not DATA_PATH.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Data directory not found: {DATA_PATH}\\n\"\n",
    "        f\"Please ensure the data directory exists.\"\n",
    "    )\n",
    "\n",
    "if not CHAPTERS_CONFIG_PATH.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Chapters configuration not found: {CHAPTERS_CONFIG_PATH}\\n\"\n",
    "        f\"Please create chapters_config.yaml with chapter definitions.\"\n",
    "    )\n",
    "\n",
    "# Initialize components\n",
    "prompt_maker = PromptMaker()\n",
    "sampler = DataSampler(data_path=DATA_PATH.resolve())\n",
    "\n",
    "llm = LLM(LLMConfig(\n",
    "    model=model_string,\n",
    "    api_key=os.environ.get(model_api_key_env_var),\n",
    "    temperature=TEMPERATURE,\n",
    "    max_tokens=16384  # Ensure enough tokens for full JSON response\n",
    "))\n",
    "\n",
    "print(f\"✓ Data path: {DATA_PATH}\")\n",
    "print(f\"✓ Segment database: {SEGMENT_DB_PATH}\")\n",
    "print(f\"✓ Chapters config: {CHAPTERS_CONFIG_PATH}\")\n",
    "print(f\"✓ DataSampler loaded {len(sampler.fps)} files\")\n",
    "print(f\"✓ LLM configured: {model_string} (temp={TEMPERATURE})\")\n",
    "print(f\"✓ Skip existing chapters: {SKIP_EXISTING_CHAPTERS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Chapters Configuration\n",
    "\n",
    "Load chapter definitions from YAML file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_chapters_config(config_path: Path) -> ChaptersConfig:\n",
    "    \"\"\"Load and validate chapters configuration from YAML file.\"\"\"\n",
    "    with open(config_path, 'r') as f:\n",
    "        data = yaml.safe_load(f)\n",
    "    \n",
    "    try:\n",
    "        config = ChaptersConfig(**data)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Invalid chapters configuration: {e}\")\n",
    "    \n",
    "    return config\n",
    "\n",
    "# Load configuration\n",
    "chapters_config = load_chapters_config(CHAPTERS_CONFIG_PATH)\n",
    "enabled_chapters = [ch for ch in chapters_config.chapters if ch.enabled]\n",
    "total_chapters = len(enabled_chapters)\n",
    "\n",
    "print(f\"✓ Loaded {total_chapters} enabled chapters\")\n",
    "print(f\"  (Skipping {len(chapters_config.chapters) - total_chapters} disabled chapters)\")\n",
    "\n",
    "# Preview chapters\n",
    "print(\"\\nChapters to process:\")\n",
    "for i, chapter in enumerate(enabled_chapters[:5], 1):\n",
    "    print(f\"  {i}. {chapter.description}\")\n",
    "    print(f\"     File {chapter.file_index}, paragraphs {chapter.paragraph_start}-{chapter.paragraph_end}\")\n",
    "if len(enabled_chapters) > 5:\n",
    "    print(f\"  ... and {len(enabled_chapters) - 5} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "Define functions for the analysis workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chapter_already_processed(\n",
    "    store: SegmentStore,\n",
    "    chapter: ChapterConfig\n",
    ") -> bool:\n",
    "    \"\"\"Check if a chapter has already been processed.\"\"\"\n",
    "    cursor = store.conn.cursor()\n",
    "    cursor.execute(\n",
    "        \"\"\"\n",
    "        SELECT COUNT(*) FROM segments\n",
    "        WHERE file_index = ?\n",
    "        AND paragraph_start >= ?\n",
    "        AND paragraph_end <= ?\n",
    "        \"\"\",\n",
    "        (chapter.file_index, chapter.paragraph_start, chapter.paragraph_end)\n",
    "    )\n",
    "    count = cursor.fetchone()[0]\n",
    "    return count > 0\n",
    "\n",
    "\n",
    "def find_passage_in_chapter(\n",
    "    passage_text: str,\n",
    "    chapter_text: str,\n",
    "    sampler: DataSampler,\n",
    "    file_index: int,\n",
    "    chapter_start_paragraph: int\n",
    ") -> tuple[int, int] | None:\n",
    "    \"\"\"Find paragraph range for a passage within a chapter.\"\"\"\n",
    "    # Normalize text for comparison\n",
    "    normalized_passage = ' '.join(passage_text.split())\n",
    "    \n",
    "    # Check if passage exists in chapter\n",
    "    if normalized_passage not in ' '.join(chapter_text.split()):\n",
    "        return None\n",
    "    \n",
    "    # Iterate through paragraphs to find the match\n",
    "    file_path = sampler.fps[file_index]\n",
    "    max_paragraphs = sampler.n_paragraphs[file_path.name]\n",
    "    \n",
    "    for length in range(1, 10):  # Try up to 10 paragraphs\n",
    "        for start_offset in range(0, 50):  # Search within first 50 paragraphs\n",
    "            abs_start = chapter_start_paragraph + start_offset\n",
    "            abs_end = abs_start + length\n",
    "            \n",
    "            if abs_end > max_paragraphs:\n",
    "                break\n",
    "            \n",
    "            chunk = sampler.get_paragraph_chunk(file_index, slice(abs_start, abs_end))\n",
    "            normalized_chunk = ' '.join(chunk.text.split())\n",
    "            \n",
    "            if normalized_passage in normalized_chunk:\n",
    "                return (abs_start, abs_end)\n",
    "    \n",
    "    return None\n",
    "\n",
    "print(\"✓ Helper functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1 & 2: Analyze and Store Chapters\n",
    "\n",
    "Loop through chapters, analyzing each to identify exemplary passages and storing them in the catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open segment store\n",
    "store = SegmentStore(SEGMENT_DB_PATH)\n",
    "\n",
    "# Track progress\n",
    "all_segment_ids = []\n",
    "processed_count = 0\n",
    "skipped_count = 0\n",
    "failed_chapters = []\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PROCESSING CHAPTERS\")\n",
    "print(\"=\"*60)\n",
    "if SKIP_EXISTING_CHAPTERS:\n",
    "    print(\"Mode: Skip already-processed chapters\\n\")\n",
    "else:\n",
    "    print(\"Mode: Reprocess all chapters\\n\")\n",
    "\n",
    "for chapter_idx, chapter in enumerate(enabled_chapters, 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"CHAPTER {chapter_idx}/{total_chapters}: {chapter.description}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"File: {chapter.file_index}, Paragraphs: {chapter.paragraph_start}-{chapter.paragraph_end}\")\n",
    "    \n",
    "    # Check if already processed\n",
    "    if SKIP_EXISTING_CHAPTERS and chapter_already_processed(store, chapter):\n",
    "        print(f\"\\n⏭ Skipping - chapter already processed\")\n",
    "        skipped_count += 1\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        # Get existing tags for consistency\n",
    "        existing_tags_dict = store.list_all_tags()\n",
    "        existing_tags = list(existing_tags_dict.keys()) if existing_tags_dict else []\n",
    "        \n",
    "        if chapter_idx == 1:\n",
    "            if existing_tags:\n",
    "                print(f\"\\nCatalog currently contains {len(existing_tags)} unique tags\")\n",
    "                print(f\"Will encourage reuse for consistency\")\n",
    "            else:\n",
    "                print(\"\\nCatalog is empty - this will establish initial tag vocabulary\")\n",
    "        else:\n",
    "            print(f\"\\nCatalog now contains {len(existing_tags)} unique tags\")\n",
    "        \n",
    "        # PHASE 1: ANALYSIS\n",
    "        print(\"\\n[1/3] Loading chapter text...\")\n",
    "        chapter_segment = sampler.get_paragraph_chunk(\n",
    "            chapter.file_index,\n",
    "            chapter.paragraph_range\n",
    "        )\n",
    "        print(f\"      File: {chapter_segment.file_path.name}\")\n",
    "        print(f\"      Length: {len(chapter_segment.text):,} characters\")\n",
    "        \n",
    "        print(\"\\n[2/3] Analyzing with LLM...\")\n",
    "        config = ExemplarySegmentAnalysisConfig(\n",
    "            chapter_text=chapter_segment.text,\n",
    "            file_name=chapter_segment.file_path.name,\n",
    "            num_segments=NUM_SEGMENTS_PER_CHAPTER,\n",
    "            existing_tags=existing_tags\n",
    "        )\n",
    "        prompt = prompt_maker.render(config)\n",
    "        \n",
    "        response = llm.complete_with_schema(\n",
    "            prompt=prompt,\n",
    "            schema_model=ExemplarySegmentAnalysis\n",
    "        )\n",
    "        analysis = response.content\n",
    "        \n",
    "        print(f\"      ✓ Identified {len(analysis.passages)} exemplary passages\")\n",
    "        if analysis.overall_observations:\n",
    "            print(f\"      Observations: {analysis.overall_observations[:100]}...\")\n",
    "        \n",
    "        # PHASE 2: STORAGE\n",
    "        print(\"\\n[3/3] Storing passages in catalog...\")\n",
    "        chapter_segment_ids = []\n",
    "        \n",
    "        for i, passage in enumerate(analysis.passages, 1):\n",
    "            print(f\"  [{i}/{len(analysis.passages)}] {passage.craft_move}\")\n",
    "            \n",
    "            # Find passage location\n",
    "            para_range = find_passage_in_chapter(\n",
    "                passage.text,\n",
    "                chapter_segment.text,\n",
    "                sampler,\n",
    "                chapter.file_index,\n",
    "                chapter.paragraph_start\n",
    "            )\n",
    "            \n",
    "            if para_range is None:\n",
    "                print(f\"      ⚠ Warning: Could not locate passage, using approximate range\")\n",
    "                para_start = chapter.paragraph_start\n",
    "                para_end = chapter.paragraph_start + 1\n",
    "            else:\n",
    "                para_start, para_end = para_range\n",
    "            \n",
    "            # Get TextSegment with provenance\n",
    "            text_segment = sampler.get_paragraph_chunk(\n",
    "                chapter.file_index,\n",
    "                slice(para_start, para_end)\n",
    "            )\n",
    "            \n",
    "            # Save to catalog\n",
    "            segment_id = store.save_segment(\n",
    "                text_segment=text_segment,\n",
    "                craft_move=passage.craft_move,\n",
    "                teaching_note=passage.teaching_note,\n",
    "                tags=passage.tags\n",
    "            )\n",
    "            \n",
    "            chapter_segment_ids.append(segment_id)\n",
    "            print(f\"      ✓ Saved: {segment_id} (para {para_start}-{para_end})\")\n",
    "            print(f\"      Tags: {', '.join(passage.tags)}\")\n",
    "        \n",
    "        all_segment_ids.extend(chapter_segment_ids)\n",
    "        processed_count += 1\n",
    "        \n",
    "        print(f\"\\n✓ Chapter {chapter_idx} complete: {len(chapter_segment_ids)} passages stored\")\n",
    "        print(f\"  Progress: {processed_count}/{total_chapters} chapters processed\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n✗ ERROR processing chapter {chapter_idx}: {e}\")\n",
    "        failed_chapters.append((chapter_idx, chapter.description, str(e)))\n",
    "        print(f\"  Continuing with next chapter...\")\n",
    "        continue\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"CHAPTER PROCESSING COMPLETE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Successfully processed: {processed_count}/{total_chapters} chapters\")\n",
    "if skipped_count > 0:\n",
    "    print(f\"Skipped (already processed): {skipped_count} chapters\")\n",
    "print(f\"Total segments stored (this run): {len(all_segment_ids)}\")\n",
    "\n",
    "if failed_chapters:\n",
    "    print(f\"\\nFailed chapters ({len(failed_chapters)}):\")\n",
    "    for idx, desc, error in failed_chapters:\n",
    "        print(f\"  - Chapter {idx} ({desc}): {error}\")\n",
    "elif processed_count > 0:\n",
    "    print(\"\\n✓ All processed chapters completed successfully!\")\n",
    "\n",
    "if skipped_count == total_chapters:\n",
    "    print(\"\\n⏭ All chapters were already processed - no new segments added\")\n",
    "    print(\"  Set SKIP_EXISTING_CHAPTERS=False to reprocess\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 3: Catalog Browsing Demo\n",
    "\n",
    "Demonstrate the skills pattern: how agents browse the catalog to find and retrieve passages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"CATALOG BROWSING DEMONSTRATION (Skills Pattern)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Step 1: List available tags\n",
    "print(\"\\n[1/3] Listing available tags...\")\n",
    "tags = store.list_all_tags()\n",
    "print(f\"      Found {len(tags)} unique tags in catalog\")\n",
    "print(f\"\\n      Top {TAG_PREVIEW_LIMIT} tags:\")\n",
    "for tag, count in list(tags.items())[:TAG_PREVIEW_LIMIT]:\n",
    "    print(f\"      - {tag}: {count} segments\")\n",
    "\n",
    "# Step 2: Browse catalog summaries\n",
    "print(f\"\\n[2/3] Browsing catalog (showing {CATALOG_PREVIEW_LIMIT} segments)...\")\n",
    "catalog = store.browse_catalog(limit=CATALOG_PREVIEW_LIMIT)\n",
    "print(f\"      Retrieved {len(catalog)} segment summaries\")\n",
    "for i, entry in enumerate(catalog, 1):\n",
    "    print(f\"\\n      Segment {i}/{len(catalog)}: {entry['segment_id']}\")\n",
    "    print(f\"      File: {entry['file_name']}\")\n",
    "    print(f\"      Range: paragraphs {entry['paragraph_range']}\")\n",
    "    print(f\"      Craft Move: {entry['craft_move']}\")\n",
    "    print(f\"      Teaching Note: {entry['teaching_note'][:80]}...\")\n",
    "    print(f\"      Tags: {', '.join(entry['tags'])}\")\n",
    "\n",
    "# Step 3: Retrieve a specific segment\n",
    "if catalog:\n",
    "    segment_id = catalog[0]['segment_id']\n",
    "    print(f\"\\n[3/3] Retrieving full text for segment: {segment_id}\")\n",
    "    record = store.get_segment(segment_id)\n",
    "    \n",
    "    if record:\n",
    "        print(f\"      ✓ Retrieved {len(record.text)} characters\")\n",
    "        print(f\"\\n      Preview (first 300 chars):\")\n",
    "        print(f\"      {record.text[:300]}...\")\n",
    "        \n",
    "        # Demonstrate conversion back to TextSegment\n",
    "        text_segment = record.to_text_segment(sampler)\n",
    "        print(f\"\\n      ✓ Re-retrieved via DataSampler:\")\n",
    "        print(f\"        File: {text_segment.file_path.name}\")\n",
    "        print(f\"        Range: {text_segment.paragraph_start}-{text_segment.paragraph_end}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "Show catalog statistics and suggest next steps for agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"WORKFLOW COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nSegment catalog saved to: {SEGMENT_DB_PATH}\")\n",
    "print(f\"\\nThis run:\")\n",
    "print(f\"  Chapters processed: {processed_count}/{total_chapters}\")\n",
    "if skipped_count > 0:\n",
    "    print(f\"  Chapters skipped: {skipped_count}\")\n",
    "print(f\"  New segments stored: {len(all_segment_ids)}\")\n",
    "\n",
    "if all_segment_ids:\n",
    "    print(f\"    First: {all_segment_ids[0]}\")\n",
    "    print(f\"    Last:  {all_segment_ids[-1]}\")\n",
    "\n",
    "# Show total catalog size\n",
    "total_in_catalog = store.get_count()\n",
    "total_tags = len(store.list_all_tags())\n",
    "\n",
    "print(f\"\\nCatalog totals:\")\n",
    "print(f\"  Total segments: {total_in_catalog}\")\n",
    "print(f\"  Unique tags: {total_tags}\")\n",
    "\n",
    "print(\"\\nNext steps - agents can now:\")\n",
    "print(\"  • store.list_all_tags() → discover available categories\")\n",
    "print(\"  • store.browse_catalog() → read segment descriptions\")\n",
    "print(\"  • store.get_segment(id) → retrieve full text\")\n",
    "print(\"  • store.search_by_tag(tag) → filter by form/function\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Close Store\n",
    "\n",
    "Clean up database connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store.close()\n",
    "print(\"✓ Database connection closed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
