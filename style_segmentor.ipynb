{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Style Segmentor: Building a Catalog of Exemplary Passages\n",
    "\n",
    "This notebook builds a segment catalog by analyzing chapters and extracting exemplary passages that demonstrate specific craft moves.\n",
    "\n",
    "## Workflow\n",
    "\n",
    "For each chapter defined in `chapters_config.yaml`:\n",
    "1. **Analyze**: LLM identifies exemplary passages demonstrating teachable craft moves\n",
    "2. **Store**: Save passages to SQLite catalog with craft annotations (craft_move, teaching_note, tags)\n",
    "3. **Browse**: Demonstrate catalog browsing pattern (list tags → browse summaries → retrieve full text)\n",
    "\n",
    "## Output\n",
    "\n",
    "- **segments.db**: SQLite database containing annotated passages ready for agent retrieval\n",
    "- Agents can browse by tags, search by craft move, and retrieve full text with provenance\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- **Skip existing chapters**: Resume after interruption without re-analyzing\n",
    "- **Tag consistency**: Encourages reuse of existing tags across chapters\n",
    "- **Provenance tracking**: Every segment includes file index and paragraph range\n",
    "- **Skills pattern**: Catalog designed for agent browsing, not pre-loaded prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Libraries and Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: litellm>=1.80.0 in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from -r requirements.txt (line 2)) (1.80.7)\n",
      "Requirement already satisfied: pydantic==2.7.4 in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from -r requirements.txt (line 5)) (2.7.4)\n",
      "Requirement already satisfied: jinja2>=3.1.0 in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from -r requirements.txt (line 8)) (3.1.6)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from pydantic==2.7.4->-r requirements.txt (line 5)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from pydantic==2.7.4->-r requirements.txt (line 5)) (2.18.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from pydantic==2.7.4->-r requirements.txt (line 5)) (4.15.0)\n",
      "Requirement already satisfied: aiohttp>=3.10 in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from litellm>=1.80.0->-r requirements.txt (line 2)) (3.13.2)\n",
      "Requirement already satisfied: click in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from litellm>=1.80.0->-r requirements.txt (line 2)) (8.3.1)\n",
      "Requirement already satisfied: fastuuid>=0.13.0 in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from litellm>=1.80.0->-r requirements.txt (line 2)) (0.14.0)\n",
      "Requirement already satisfied: grpcio<1.68.0,>=1.62.3 in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from litellm>=1.80.0->-r requirements.txt (line 2)) (1.67.1)\n",
      "Requirement already satisfied: httpx>=0.23.0 in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from litellm>=1.80.0->-r requirements.txt (line 2)) (0.28.1)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from litellm>=1.80.0->-r requirements.txt (line 2)) (8.7.0)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from litellm>=1.80.0->-r requirements.txt (line 2)) (4.25.1)\n",
      "Requirement already satisfied: openai>=2.8.0 in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from litellm>=1.80.0->-r requirements.txt (line 2)) (2.8.1)\n",
      "Requirement already satisfied: python-dotenv>=0.2.0 in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from litellm>=1.80.0->-r requirements.txt (line 2)) (1.2.1)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from litellm>=1.80.0->-r requirements.txt (line 2)) (0.12.0)\n",
      "Requirement already satisfied: tokenizers in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from litellm>=1.80.0->-r requirements.txt (line 2)) (0.22.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from jinja2>=3.1.0->-r requirements.txt (line 8)) (3.0.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from aiohttp>=3.10->litellm>=1.80.0->-r requirements.txt (line 2)) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from aiohttp>=3.10->litellm>=1.80.0->-r requirements.txt (line 2)) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from aiohttp>=3.10->litellm>=1.80.0->-r requirements.txt (line 2)) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from aiohttp>=3.10->litellm>=1.80.0->-r requirements.txt (line 2)) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from aiohttp>=3.10->litellm>=1.80.0->-r requirements.txt (line 2)) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from aiohttp>=3.10->litellm>=1.80.0->-r requirements.txt (line 2)) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from aiohttp>=3.10->litellm>=1.80.0->-r requirements.txt (line 2)) (1.22.0)\n",
      "Requirement already satisfied: anyio in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from httpx>=0.23.0->litellm>=1.80.0->-r requirements.txt (line 2)) (4.11.0)\n",
      "Requirement already satisfied: certifi in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from httpx>=0.23.0->litellm>=1.80.0->-r requirements.txt (line 2)) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from httpx>=0.23.0->litellm>=1.80.0->-r requirements.txt (line 2)) (1.0.9)\n",
      "Requirement already satisfied: idna in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from httpx>=0.23.0->litellm>=1.80.0->-r requirements.txt (line 2)) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.23.0->litellm>=1.80.0->-r requirements.txt (line 2)) (0.16.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from importlib-metadata>=6.8.0->litellm>=1.80.0->-r requirements.txt (line 2)) (3.23.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.80.0->-r requirements.txt (line 2)) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.80.0->-r requirements.txt (line 2)) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.80.0->-r requirements.txt (line 2)) (0.30.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from openai>=2.8.0->litellm>=1.80.0->-r requirements.txt (line 2)) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from openai>=2.8.0->litellm>=1.80.0->-r requirements.txt (line 2)) (0.12.0)\n",
      "Requirement already satisfied: sniffio in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from openai>=2.8.0->litellm>=1.80.0->-r requirements.txt (line 2)) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from openai>=2.8.0->litellm>=1.80.0->-r requirements.txt (line 2)) (4.67.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from tiktoken>=0.7.0->litellm>=1.80.0->-r requirements.txt (line 2)) (2025.11.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from tiktoken>=0.7.0->litellm>=1.80.0->-r requirements.txt (line 2)) (2.32.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from tokenizers->litellm>=1.80.0->-r requirements.txt (line 2)) (1.1.7)\n",
      "Requirement already satisfied: filelock in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm>=1.80.0->-r requirements.txt (line 2)) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm>=1.80.0->-r requirements.txt (line 2)) (2025.10.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm>=1.80.0->-r requirements.txt (line 2)) (1.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm>=1.80.0->-r requirements.txt (line 2)) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm>=1.80.0->-r requirements.txt (line 2)) (6.0.3)\n",
      "Requirement already satisfied: shellingham in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm>=1.80.0->-r requirements.txt (line 2)) (1.5.4)\n",
      "Requirement already satisfied: typer-slim in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm>=1.80.0->-r requirements.txt (line 2)) (0.20.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken>=0.7.0->litellm>=1.80.0->-r requirements.txt (line 2)) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/andersohrn/PycharmProjects/personal_github_website/venv/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken>=0.7.0->litellm>=1.80.0->-r requirements.txt (line 2)) (2.5.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Providers\n",
      "=========\n",
      "* openai\n",
      "* openai_like\n",
      "* bytez\n",
      "* xai\n",
      "* custom_openai\n",
      "* text-completion-openai\n",
      "* cohere\n",
      "* cohere_chat\n",
      "* clarifai\n",
      "* anthropic\n",
      "* anthropic_text\n",
      "* replicate\n",
      "* huggingface\n",
      "* together_ai\n",
      "* datarobot\n",
      "* openrouter\n",
      "* cometapi\n",
      "* vertex_ai\n",
      "* vertex_ai_beta\n",
      "* gemini\n",
      "* ai21\n",
      "* baseten\n",
      "* azure\n",
      "* azure_text\n",
      "* azure_ai\n",
      "* sagemaker\n",
      "* sagemaker_chat\n",
      "* bedrock\n",
      "* vllm\n",
      "* nlp_cloud\n",
      "* petals\n",
      "* oobabooga\n",
      "* ollama\n",
      "* ollama_chat\n",
      "* deepinfra\n",
      "* perplexity\n",
      "* mistral\n",
      "* groq\n",
      "* nvidia_nim\n",
      "* cerebras\n",
      "* baseten\n",
      "* ai21_chat\n",
      "* volcengine\n",
      "* codestral\n",
      "* text-completion-codestral\n",
      "* deepseek\n",
      "* sambanova\n",
      "* maritalk\n",
      "* cloudflare\n",
      "* fireworks_ai\n",
      "* friendliai\n",
      "* watsonx\n",
      "* watsonx_text\n",
      "* triton\n",
      "* predibase\n",
      "* databricks\n",
      "* empower\n",
      "* github\n",
      "* custom\n",
      "* litellm_proxy\n",
      "* hosted_vllm\n",
      "* llamafile\n",
      "* lm_studio\n",
      "* galadriel\n",
      "* gradient_ai\n",
      "* github_copilot\n",
      "* novita\n",
      "* meta_llama\n",
      "* featherless_ai\n",
      "* nscale\n",
      "* nebius\n",
      "* dashscope\n",
      "* moonshot\n",
      "* v0\n",
      "* heroku\n",
      "* oci\n",
      "* morph\n",
      "* lambda_ai\n",
      "* vercel_ai_gateway\n",
      "* wandb\n",
      "* ovhcloud\n",
      "* lemonade\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import litellm\n",
    "    print('Providers\\n=========')\n",
    "    print('* ' + '\\n* '.join(litellm.LITELLM_CHAT_PROVIDERS))\n",
    "    litellm.drop_params = True\n",
    "except ImportError as e:\n",
    "    print(f\"✗ Cannot import litellm: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "from typing import List, Optional\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter Configuration Models\n",
    "\n",
    "Define Pydantic models for validating chapter configurations loaded from YAML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Chapter configuration models loaded\n"
     ]
    }
   ],
   "source": [
    "class ChapterConfig(BaseModel):\n",
    "    \"\"\"Configuration for a single chapter to analyze.\"\"\"\n",
    "    file_index: int = Field(..., ge=0, description=\"Index of file in data directory\")\n",
    "    paragraph_start: int = Field(..., ge=0, description=\"Starting paragraph (inclusive)\")\n",
    "    paragraph_end: int = Field(..., gt=0, description=\"Ending paragraph (exclusive)\")\n",
    "    description: str = Field(..., min_length=1, description=\"Human-readable chapter description\")\n",
    "    enabled: bool = Field(default=True, description=\"Whether to process this chapter\")\n",
    "\n",
    "    @property\n",
    "    def paragraph_range(self) -> slice:\n",
    "        \"\"\"Convert to slice for DataSampler.\"\"\"\n",
    "        return slice(self.paragraph_start, self.paragraph_end)\n",
    "\n",
    "\n",
    "class ChaptersConfig(BaseModel):\n",
    "    \"\"\"Root configuration containing all chapters.\"\"\"\n",
    "    chapters: List[ChapterConfig] = Field(..., min_items=1)\n",
    "\n",
    "print(\"✓ Chapter configuration models loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Configuration\n",
    "\n",
    "Configure which LLM to use for segment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model: together_ai/Qwen/Qwen3-235B-A22B-Thinking-2507\n",
      "✓ API key from env var: TOGETHER_AI_API_KEY\n"
     ]
    }
   ],
   "source": [
    "# Model configuration\n",
    "model_string = 'together_ai/Qwen/Qwen3-235B-A22B-Thinking-2507'\n",
    "model_api_key_env_var = 'TOGETHER_AI_API_KEY'\n",
    "\n",
    "# Alternative examples (uncomment to use):\n",
    "#model_string = 'anthropic/claude-sonnet-4-5-20250929'\n",
    "#model_api_key_env_var = 'ANTHROPIC_API_KEY'\n",
    "#model_string = 'openai/gpt-4o'\n",
    "#model_api_key_env_var = 'OPENAI_API_KEY'\n",
    "#model_string = 'mistral/mistral-large-2512'\n",
    "#model_api_key_env_var = 'MISTRAL_API_KEY'\n",
    "\n",
    "print(f\"✓ Model: {model_string}\")\n",
    "print(f\"✓ API key from env var: {model_api_key_env_var}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Base Objects\n",
    "\n",
    "Initialize core components:\n",
    "- `LLM`: LLM interface for analysis\n",
    "- `PromptMaker`: Template rendering engine\n",
    "- `DataSampler`: Text loading with provenance\n",
    "- `SegmentStore`: SQLite catalog for passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from belletrist import LLM, LLMConfig, PromptMaker, DataSampler, SegmentStore\nfrom belletrist.prompts import ExemplarySegmentAnalysisConfig, ExemplarySegmentAnalysis\nfrom belletrist.prompts.canonical_tags import get_all_canonical_tags, format_for_jinja\n\n# ============================================================================\n# CONFIGURATION - Modify these parameters before running\n# ============================================================================\n\n# Data paths\nDATA_PATH = Path(os.getcwd()) / \"data\" / \"russell\"\nSEGMENT_DB_PATH = Path(os.getcwd()) / \"segments.db\"\nCHAPTERS_CONFIG_PATH = Path(os.getcwd()) / \"chapters_config.yaml\"\n\n# Analysis parameters\nTEMPERATURE = 0.7\nNUM_SEGMENTS_PER_CHAPTER = 5  # How many passages to extract per chapter\n\n# Processing control\nSKIP_EXISTING_CHAPTERS = True  # Skip chapters already processed\n\n# Catalog browsing (for demo at end)\nCATALOG_PREVIEW_LIMIT = 5  # Number of segments to show in browse demo\nTAG_PREVIEW_LIMIT = 10  # Number of tags to show in tag list\n\n# ============================================================================\n\n# Validate configuration\nif not DATA_PATH.exists():\n    raise FileNotFoundError(\n        f\"Data directory not found: {DATA_PATH}\\n\"\n        f\"Please ensure the data directory exists.\"\n    )\n\nif not CHAPTERS_CONFIG_PATH.exists():\n    raise FileNotFoundError(\n        f\"Chapters configuration not found: {CHAPTERS_CONFIG_PATH}\\n\"\n        f\"Please create chapters_config.yaml with chapter definitions.\"\n    )\n\n# Initialize components\nprompt_maker = PromptMaker()\nsampler = DataSampler(data_path=DATA_PATH.resolve())\n\nllm = LLM(LLMConfig(\n    model=model_string,\n    api_key=os.environ.get(model_api_key_env_var),\n    temperature=TEMPERATURE,\n    max_tokens=16384  # Ensure enough tokens for full JSON response\n))\n\nprint(f\"✓ Data path: {DATA_PATH}\")\nprint(f\"✓ Segment database: {SEGMENT_DB_PATH}\")\nprint(f\"✓ Chapters config: {CHAPTERS_CONFIG_PATH}\")\nprint(f\"✓ DataSampler loaded {len(sampler.fps)} files\")\nprint(f\"✓ LLM configured: {model_string} (temp={TEMPERATURE})\")\nprint(f\"✓ Skip existing chapters: {SKIP_EXISTING_CHAPTERS}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Chapters Configuration\n",
    "\n",
    "Load chapter definitions from YAML file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded 2 enabled chapters\n",
      "  (Skipping 23 disabled chapters)\n",
      "\n",
      "Chapters to process:\n",
      "  1. Chapter 1: Postulates of Modern Educational Theory\n",
      "     File 0, paragraphs 9-41\n",
      "  2. Chapter 2: The Aims of Education\n",
      "     File 0, paragraphs 43-83\n"
     ]
    }
   ],
   "source": [
    "def load_chapters_config(config_path: Path) -> ChaptersConfig:\n",
    "    \"\"\"Load and validate chapters configuration from YAML file.\"\"\"\n",
    "    with open(config_path, 'r') as f:\n",
    "        data = yaml.safe_load(f)\n",
    "    \n",
    "    try:\n",
    "        config = ChaptersConfig(**data)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Invalid chapters configuration: {e}\")\n",
    "    \n",
    "    return config\n",
    "\n",
    "# Load configuration\n",
    "chapters_config = load_chapters_config(CHAPTERS_CONFIG_PATH)\n",
    "enabled_chapters = [ch for ch in chapters_config.chapters if ch.enabled]\n",
    "total_chapters = len(enabled_chapters)\n",
    "\n",
    "print(f\"✓ Loaded {total_chapters} enabled chapters\")\n",
    "print(f\"  (Skipping {len(chapters_config.chapters) - total_chapters} disabled chapters)\")\n",
    "\n",
    "# Preview chapters\n",
    "print(\"\\nChapters to process:\")\n",
    "for i, chapter in enumerate(enabled_chapters[:5], 1):\n",
    "    print(f\"  {i}. {chapter.description}\")\n",
    "    print(f\"     File {chapter.file_index}, paragraphs {chapter.paragraph_start}-{chapter.paragraph_end}\")\n",
    "if len(enabled_chapters) > 5:\n",
    "    print(f\"  ... and {len(enabled_chapters) - 5} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "Define functions for the analysis workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Helper functions loaded\n"
     ]
    }
   ],
   "source": [
    "def chapter_already_processed(\n",
    "    store: SegmentStore,\n",
    "    chapter: ChapterConfig\n",
    ") -> bool:\n",
    "    \"\"\"Check if a chapter has already been processed.\"\"\"\n",
    "    cursor = store.conn.cursor()\n",
    "    cursor.execute(\n",
    "        \"\"\"\n",
    "        SELECT COUNT(*) FROM segments\n",
    "        WHERE file_index = ?\n",
    "        AND paragraph_start >= ?\n",
    "        AND paragraph_end <= ?\n",
    "        \"\"\",\n",
    "        (chapter.file_index, chapter.paragraph_start, chapter.paragraph_end)\n",
    "    )\n",
    "    count = cursor.fetchone()[0]\n",
    "    return count > 0\n",
    "\n",
    "\n",
    "def find_passage_in_chapter(\n",
    "    passage_text: str,\n",
    "    chapter_text: str,\n",
    "    sampler: DataSampler,\n",
    "    file_index: int,\n",
    "    chapter_start_paragraph: int\n",
    ") -> tuple[int, int] | None:\n",
    "    \"\"\"Find paragraph range for a passage within a chapter.\"\"\"\n",
    "    # Normalize text for comparison\n",
    "    normalized_passage = ' '.join(passage_text.split())\n",
    "    \n",
    "    # Check if passage exists in chapter\n",
    "    if normalized_passage not in ' '.join(chapter_text.split()):\n",
    "        return None\n",
    "    \n",
    "    # Iterate through paragraphs to find the match\n",
    "    file_path = sampler.fps[file_index]\n",
    "    max_paragraphs = sampler.n_paragraphs[file_path.name]\n",
    "    \n",
    "    for length in range(1, 10):  # Try up to 10 paragraphs\n",
    "        for start_offset in range(0, 50):  # Search within first 50 paragraphs\n",
    "            abs_start = chapter_start_paragraph + start_offset\n",
    "            abs_end = abs_start + length\n",
    "            \n",
    "            if abs_end > max_paragraphs:\n",
    "                break\n",
    "            \n",
    "            chunk = sampler.get_paragraph_chunk(file_index, slice(abs_start, abs_end))\n",
    "            normalized_chunk = ' '.join(chunk.text.split())\n",
    "            \n",
    "            if normalized_passage in normalized_chunk:\n",
    "                return (abs_start, abs_end)\n",
    "    \n",
    "    return None\n",
    "\n",
    "print(\"✓ Helper functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1 & 2: Analyze and Store Chapters\n",
    "\n",
    "Loop through chapters, analyzing each to identify exemplary passages and storing them in the catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Open segment store\nstore = SegmentStore(SEGMENT_DB_PATH)\n\n# Track progress\nall_segment_ids = []\nprocessed_count = 0\nskipped_count = 0\nfailed_chapters = []\n\nprint(\"=\"*60)\nprint(\"PROCESSING CHAPTERS\")\nprint(\"=\"*60)\nif SKIP_EXISTING_CHAPTERS:\n    print(\"Mode: Skip already-processed chapters\\n\")\nelse:\n    print(\"Mode: Reprocess all chapters\\n\")\n\nfor chapter_idx, chapter in enumerate(enabled_chapters, 1):\n    print(f\"\\n{'='*60}\")\n    print(f\"CHAPTER {chapter_idx}/{total_chapters}: {chapter.description}\")\n    print(f\"{'='*60}\")\n    print(f\"File: {chapter.file_index}, Paragraphs: {chapter.paragraph_start}-{chapter.paragraph_end}\")\n    \n    # Check if already processed\n    if SKIP_EXISTING_CHAPTERS and chapter_already_processed(store, chapter):\n        print(f\"\\n⏭ Skipping - chapter already processed\")\n        skipped_count += 1\n        continue\n    \n    try:\n        # Get existing tags and filter to Tier 2 only\n        existing_tags_dict = store.list_all_tags()\n        all_tags = list(existing_tags_dict.keys()) if existing_tags_dict else []\n        \n        # Filter out canonical tags to get only Tier 2\n        canonical_tag_set = get_all_canonical_tags()\n        existing_tier2_tags = [tag for tag in all_tags if tag not in canonical_tag_set]\n        \n        # Get formatted canonical tags for template injection\n        canonical_tags_formatted = format_for_jinja()\n        \n        if chapter_idx == 1:\n            if existing_tier2_tags:\n                print(f\"\\nCatalog contains {len(existing_tier2_tags)} author-specific tags (Tier 2)\")\n                print(f\"Will encourage reuse for consistency\")\n            else:\n                print(\"\\nNo Tier 2 tags yet - LLM will create author-specific vocabulary\")\n        else:\n            print(f\"\\nCatalog now contains {len(existing_tier2_tags)} author-specific tags (Tier 2)\")\n        \n        # PHASE 1: ANALYSIS\n        print(\"\\n[1/3] Loading chapter text...\")\n        chapter_segment = sampler.get_paragraph_chunk(\n            chapter.file_index,\n            chapter.paragraph_range\n        )\n        print(f\"      File: {chapter_segment.file_path.name}\")\n        print(f\"      Length: {len(chapter_segment.text):,} characters\")\n        \n        print(\"\\n[2/3] Analyzing with LLM...\")\n        config = ExemplarySegmentAnalysisConfig(\n            chapter_text=chapter_segment.text,\n            file_name=chapter_segment.file_path.name,\n            num_segments=NUM_SEGMENTS_PER_CHAPTER,\n            existing_tier2_tags=existing_tier2_tags,\n            canonical_tags_formatted=canonical_tags_formatted\n        )\n        prompt = prompt_maker.render(config)\n        \n        response = llm.complete_with_schema(\n            prompt=prompt,\n            schema_model=ExemplarySegmentAnalysis\n        )\n        analysis = response.content\n        \n        print(f\"      ✓ Identified {len(analysis.passages)} exemplary passages\")\n        if analysis.overall_observations:\n            print(f\"      Observations: {analysis.overall_observations[:100]}...\")\n        \n        # PHASE 2: STORAGE\n        print(\"\\n[3/3] Storing passages in catalog...\")\n        chapter_segment_ids = []\n        \n        for i, passage in enumerate(analysis.passages, 1):\n            print(f\"  [{i}/{len(analysis.passages)}] {passage.craft_move}\")\n            \n            # Find passage location\n            para_range = find_passage_in_chapter(\n                passage.text,\n                chapter_segment.text,\n                sampler,\n                chapter.file_index,\n                chapter.paragraph_start\n            )\n            \n            if para_range is None:\n                print(f\"      ⚠ Warning: Could not locate passage, using approximate range\")\n                para_start = chapter.paragraph_start\n                para_end = chapter.paragraph_start + 1\n            else:\n                para_start, para_end = para_range\n            \n            # Get TextSegment with provenance\n            text_segment = sampler.get_paragraph_chunk(\n                chapter.file_index,\n                slice(para_start, para_end)\n            )\n            \n            # Save to catalog\n            segment_id = store.save_segment(\n                text_segment=text_segment,\n                craft_move=passage.craft_move,\n                teaching_note=passage.teaching_note,\n                tags=passage.tags\n            )\n            \n            chapter_segment_ids.append(segment_id)\n            print(f\"      ✓ Saved: {segment_id} (para {para_start}-{para_end})\")\n            print(f\"      Tags: {', '.join(passage.tags)}\")\n        \n        all_segment_ids.extend(chapter_segment_ids)\n        processed_count += 1\n        \n        print(f\"\\n✓ Chapter {chapter_idx} complete: {len(chapter_segment_ids)} passages stored\")\n        print(f\"  Progress: {processed_count}/{total_chapters} chapters processed\")\n        \n    except Exception as e:\n        print(f\"\\n✗ ERROR processing chapter {chapter_idx}: {e}\")\n        failed_chapters.append((chapter_idx, chapter.description, str(e)))\n        print(f\"  Continuing with next chapter...\")\n        continue\n\n# Summary\nprint(f\"\\n{'='*60}\")\nprint(f\"CHAPTER PROCESSING COMPLETE\")\nprint(f\"{'='*60}\")\nprint(f\"Successfully processed: {processed_count}/{total_chapters} chapters\")\nif skipped_count > 0:\n    print(f\"Skipped (already processed): {skipped_count} chapters\")\nprint(f\"Total segments stored (this run): {len(all_segment_ids)}\")\n\nif failed_chapters:\n    print(f\"\\nFailed chapters ({len(failed_chapters)}):\")\n    for idx, desc, error in failed_chapters:\n        print(f\"  - Chapter {idx} ({desc}): {error}\")\nelif processed_count > 0:\n    print(\"\\n✓ All processed chapters completed successfully!\")\n\nif skipped_count == total_chapters:\n    print(\"\\n⏭ All chapters were already processed - no new segments added\")\n    print(\"  Set SKIP_EXISTING_CHAPTERS=False to reprocess\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 3: Catalog Browsing Demo\n",
    "\n",
    "Demonstrate the skills pattern: how agents browse the catalog to find and retrieve passages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CATALOG BROWSING DEMONSTRATION (Skills Pattern)\n",
      "============================================================\n",
      "\n",
      "[1/3] Listing available tags...\n",
      "      Found 30 unique tags in catalog\n",
      "\n",
      "      Top 10 tags:\n",
      "      - logical_pivot: 3 segments\n",
      "      - precision_layering: 3 segments\n",
      "      - qualified_assertion: 3 segments\n",
      "      - conceptual_rhythm: 3 segments\n",
      "      - conceptual_clarity: 3 segments\n",
      "      - builds_definition: 2 segments\n",
      "      - adds_qualification: 2 segments\n",
      "      - philosophical_clarity: 2 segments\n",
      "      - historical_evidence: 2 segments\n",
      "      - parallel_structure: 2 segments\n",
      "\n",
      "[2/3] Browsing catalog (showing 5 segments)...\n",
      "      Retrieved 5 segment summaries\n",
      "\n",
      "      Segment 1/5: seg_001\n",
      "      File: education_and_the_good_life.txt\n",
      "      Range: paragraphs 9-10\n",
      "      Craft Move: concessive_pivot\n",
      "      Teaching Note: Notice how the author establishes credibility by first acknowledging predecessor...\n",
      "      Tags: opens_argument, handles_predecessors, concessive_structure, mathematical_reasoning, logical_pivot\n",
      "\n",
      "      Segment 2/5: seg_002\n",
      "      File: education_and_the_good_life.txt\n",
      "      Range: paragraphs 9-10\n",
      "      Craft Move: precision_nesting\n",
      "      Teaching Note: Notice how the author qualifies statements through successive layers of precisio...\n",
      "      Tags: builds_definition, adds_qualification, nested_conditionals, precision_layering, qualified_assertion\n",
      "\n",
      "      Segment 3/5: seg_003\n",
      "      File: education_and_the_good_life.txt\n",
      "      Range: paragraphs 9-10\n",
      "      Craft Move: definition_through_dissolution\n",
      "      Teaching Note: Notice how the author doesn't just define terms but dissolves the opposition bet...\n",
      "      Tags: redefines_terms, dissolves_opposition, recursive_analysis, conceptual_rhythm, philosophical_clarity\n",
      "\n",
      "      Segment 4/5: seg_004\n",
      "      File: education_and_the_good_life.txt\n",
      "      Range: paragraphs 26-27\n",
      "      Craft Move: rhetorical_inversion\n",
      "      Teaching Note: Notice how the author identifies and dramatically reverses a cultural assumption...\n",
      "      Tags: handles_objection, reverses_assumption, historical_evidence, strategic_italics, conceptual_clarity\n",
      "\n",
      "      Segment 5/5: seg_005\n",
      "      File: education_and_the_good_life.txt\n",
      "      Range: paragraphs 9-10\n",
      "      Craft Move: contextual_anchoring\n",
      "      Teaching Note: Notice how the author transforms an abstract historical figure into a living pre...\n",
      "      Tags: establishes_authority, personal_connection, ironic_deflation, historical_context, immediate_relevance\n",
      "\n",
      "[3/3] Retrieving full text for segment: seg_001\n",
      "      ✓ Retrieved 2307 characters\n",
      "\n",
      "      Preview (first 300 chars):\n",
      "      In reading even the best treatises on education written in former\n",
      "times, one becomes aware of certain changes that have come over\n",
      "educational theory. The two great reformers of educational theory\n",
      "before the nineteenth century were Locke and Rousseau. Both deserved\n",
      "their reputation, for both repudiat...\n",
      "\n",
      "      ✓ Re-retrieved via DataSampler:\n",
      "        File: education_and_the_good_life.txt\n",
      "        Range: 9-10\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"CATALOG BROWSING DEMONSTRATION (Skills Pattern)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Step 1: List available tags\n",
    "print(\"\\n[1/3] Listing available tags...\")\n",
    "tags = store.list_all_tags()\n",
    "print(f\"      Found {len(tags)} unique tags in catalog\")\n",
    "print(f\"\\n      Top {TAG_PREVIEW_LIMIT} tags:\")\n",
    "for tag, count in list(tags.items())[:TAG_PREVIEW_LIMIT]:\n",
    "    print(f\"      - {tag}: {count} segments\")\n",
    "\n",
    "# Step 2: Browse catalog summaries\n",
    "print(f\"\\n[2/3] Browsing catalog (showing {CATALOG_PREVIEW_LIMIT} segments)...\")\n",
    "catalog = store.browse_catalog(limit=CATALOG_PREVIEW_LIMIT)\n",
    "print(f\"      Retrieved {len(catalog)} segment summaries\")\n",
    "for i, entry in enumerate(catalog, 1):\n",
    "    print(f\"\\n      Segment {i}/{len(catalog)}: {entry['segment_id']}\")\n",
    "    print(f\"      File: {entry['file_name']}\")\n",
    "    print(f\"      Range: paragraphs {entry['paragraph_range']}\")\n",
    "    print(f\"      Craft Move: {entry['craft_move']}\")\n",
    "    print(f\"      Teaching Note: {entry['teaching_note'][:80]}...\")\n",
    "    print(f\"      Tags: {', '.join(entry['tags'])}\")\n",
    "\n",
    "# Step 3: Retrieve a specific segment\n",
    "if catalog:\n",
    "    segment_id = catalog[0]['segment_id']\n",
    "    print(f\"\\n[3/3] Retrieving full text for segment: {segment_id}\")\n",
    "    record = store.get_segment(segment_id)\n",
    "    \n",
    "    if record:\n",
    "        print(f\"      ✓ Retrieved {len(record.text)} characters\")\n",
    "        print(f\"\\n      Preview (first 300 chars):\")\n",
    "        print(f\"      {record.text[:300]}...\")\n",
    "        \n",
    "        # Demonstrate conversion back to TextSegment\n",
    "        text_segment = record.to_text_segment(sampler)\n",
    "        print(f\"\\n      ✓ Re-retrieved via DataSampler:\")\n",
    "        print(f\"        File: {text_segment.file_path.name}\")\n",
    "        print(f\"        Range: {text_segment.paragraph_start}-{text_segment.paragraph_end}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "Show catalog statistics and suggest next steps for agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "WORKFLOW COMPLETE\n",
      "============================================================\n",
      "\n",
      "Segment catalog saved to: /Users/andersohrn/PycharmProjects/ClaudeCodeCourse/style-retrieval/segments.db\n",
      "\n",
      "This run:\n",
      "  Chapters processed: 2/2\n",
      "  New segments stored: 10\n",
      "    First: seg_001\n",
      "    Last:  seg_010\n",
      "\n",
      "Catalog totals:\n",
      "  Total segments: 10\n",
      "  Unique tags: 30\n",
      "\n",
      "Next steps - agents can now:\n",
      "  • store.list_all_tags() → discover available categories\n",
      "  • store.browse_catalog() → read segment descriptions\n",
      "  • store.get_segment(id) → retrieve full text\n",
      "  • store.search_by_tag(tag) → filter by form/function\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"WORKFLOW COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nSegment catalog saved to: {SEGMENT_DB_PATH}\")\n",
    "print(f\"\\nThis run:\")\n",
    "print(f\"  Chapters processed: {processed_count}/{total_chapters}\")\n",
    "if skipped_count > 0:\n",
    "    print(f\"  Chapters skipped: {skipped_count}\")\n",
    "print(f\"  New segments stored: {len(all_segment_ids)}\")\n",
    "\n",
    "if all_segment_ids:\n",
    "    print(f\"    First: {all_segment_ids[0]}\")\n",
    "    print(f\"    Last:  {all_segment_ids[-1]}\")\n",
    "\n",
    "# Show total catalog size\n",
    "total_in_catalog = store.get_count()\n",
    "total_tags = len(store.list_all_tags())\n",
    "\n",
    "print(f\"\\nCatalog totals:\")\n",
    "print(f\"  Total segments: {total_in_catalog}\")\n",
    "print(f\"  Unique tags: {total_tags}\")\n",
    "\n",
    "print(\"\\nNext steps - agents can now:\")\n",
    "print(\"  • store.list_all_tags() → discover available categories\")\n",
    "print(\"  • store.browse_catalog() → read segment descriptions\")\n",
    "print(\"  • store.get_segment(id) → retrieve full text\")\n",
    "print(\"  • store.search_by_tag(tag) → filter by form/function\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Close Store\n",
    "\n",
    "Clean up database connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Database connection closed\n"
     ]
    }
   ],
   "source": [
    "store.close()\n",
    "print(\"✓ Database connection closed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (russell_writes)",
   "language": "python",
   "name": "russell_writes"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}