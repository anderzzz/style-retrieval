{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Style Evaluation: Few-Shot Source Comparison\n",
    "\n",
    "This notebook evaluates how the **source of few-shot examples** affects style reconstruction quality.\n",
    "\n",
    "## Research Question\n",
    "\n",
    "When using few-shot prompting to reconstruct style, does it matter whose writing we use as examples?\n",
    "\n",
    "## Methodology\n",
    "\n",
    "For each gold standard text (Bertrand Russell):\n",
    "1. **Flatten**: Extract content while removing style\n",
    "2. **Reconstruct**: Generate text using 5 different few-shot sources (M stochastic runs each):\n",
    "   - Russell (same author - baseline)\n",
    "   - Chesterton (different author)\n",
    "   - Clausewitz (different author, different domain)\n",
    "   - Freud (different author, different domain)\n",
    "   - Hume (different author, same domain)\n",
    "3. **Judge (Blind Comparative)**: Judge ranks all 5 reconstructions based on similarity to original\n",
    "   - **Blind evaluation**: Judge sees only anonymous labels (Text A, B, C, D, E)\n",
    "   - **Ranking**: 1 = most similar, 5 = least similar\n",
    "   - **Order randomized**: Position of methods varies across samples\n",
    "4. **Aggregate**: Analyze rankings to determine which few-shot source works best\n",
    "\n",
    "## Hypothesis\n",
    "\n",
    "We expect Russell examples (same author) to perform best, but want to quantify:\n",
    "- How much worse are other authors?\n",
    "- Does domain similarity (Hume - philosophy) matter more than authorship?\n",
    "- Can cross-author examples still capture some style elements?\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- **Crash resilient**: All LLM responses saved to SQLite immediately\n",
    "- **Resume support**: Can restart after failures, skips completed work\n",
    "- **Blind evaluation**: Eliminates judge bias by hiding method names\n",
    "- **Comparative ranking**: More informative than binary comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Libraries and Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import litellm\n",
    "    print('Providers\\n=========')\n",
    "    print('* ' + '\\n* '.join(litellm.LITELLM_CHAT_PROVIDERS))\n",
    "    litellm.drop_params = True\n",
    "except ImportError as e:\n",
    "    print(f\"✗ Cannot import litellm: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Base Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Configuration\n",
    "model_reconstruction_string = 'anthropic/claude-sonnet-4-5-20250929'\n",
    "model_reconstruction_api_key_env_var = 'ANTHROPIC_API_KEY'\n",
    "model_judge_string = 'anthropic/claude-sonnet-4-5-20250929'\n",
    "model_judge_api_key_env_var = 'ANTHROPIC_API_KEY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from belletrist import PromptMaker, DataSampler, StyleEvaluationStore\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION - Modify these parameters before running\n",
    "# ============================================================================\n",
    "\n",
    "# Data paths\n",
    "DATA_PATH_RUSSELL = Path(os.getcwd()) / \"data\" / \"russell\"\n",
    "DATA_PATH_OTHER = Path(os.getcwd()) / \"data\" / \"other_author\"\n",
    "EVALUATION_DB_PATH = Path(os.getcwd()) / \"style_eval_fewshot_sources.db\"\n",
    "\n",
    "# Methods for this experiment (must be exactly 5 for 5-way comparison)\n",
    "METHODS = [\n",
    "    'fewshot_russell',\n",
    "    'fewshot_chesterton', \n",
    "    'fewshot_clausewitz',\n",
    "    'fewshot_freud',\n",
    "    'fewshot_hume'\n",
    "]\n",
    "\n",
    "# Mapping of methods to source files\n",
    "FEWSHOT_SOURCE_FILES = {\n",
    "    'fewshot_chesterton': 'excerpt_chesterton_orthodoxy.txt',\n",
    "    'fewshot_clausewitz': 'excerpt_clausewitz_on_war.txt',\n",
    "    'fewshot_freud': 'excerpt_freud_a_general_introduction_to_psychoanalysis.txt',\n",
    "    'fewshot_hume': 'excerpt_hume_a_treatise_on_human_nature.txt'\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "\n",
    "# Validate configuration\n",
    "if not DATA_PATH_RUSSELL.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Russell data directory not found: {DATA_PATH_RUSSELL}\\n\"\n",
    "        f\"Please ensure the data directory exists.\"\n",
    "    )\n",
    "\n",
    "if not DATA_PATH_OTHER.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Other authors data directory not found: {DATA_PATH_OTHER}\\n\"\n",
    "        f\"Please ensure the data directory exists.\"\n",
    "    )\n",
    "\n",
    "# Validate that all source files exist\n",
    "for method, filename in FEWSHOT_SOURCE_FILES.items():\n",
    "    filepath = DATA_PATH_OTHER / filename\n",
    "    if not filepath.exists():\n",
    "        raise FileNotFoundError(f\"Source file not found: {filepath}\")\n",
    "\n",
    "# Initialize components\n",
    "prompt_maker = PromptMaker()\n",
    "\n",
    "sampler_russell = DataSampler(data_path=DATA_PATH_RUSSELL.resolve())\n",
    "sampler_other = DataSampler(data_path=DATA_PATH_OTHER.resolve())\n",
    "\n",
    "store = StyleEvaluationStore(\n",
    "    EVALUATION_DB_PATH,\n",
    "    methods=METHODS\n",
    ")\n",
    "\n",
    "print(f\"✓ Russell data path: {DATA_PATH_RUSSELL}\")\n",
    "print(f\"✓ Other authors data path: {DATA_PATH_OTHER}\")\n",
    "print(f\"✓ Evaluation database: {EVALUATION_DB_PATH}\")\n",
    "print(f\"✓ Configured methods: {METHODS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from belletrist import LLM, LLMConfig\n",
    "\n",
    "reconstruction_llm = LLM(LLMConfig(\n",
    "    model=model_reconstruction_string,\n",
    "    api_key=os.environ.get(model_reconstruction_api_key_env_var)\n",
    "))\n",
    "judge_llm = LLM(LLMConfig(\n",
    "    model=model_judge_string,\n",
    "    api_key=os.environ.get(model_judge_api_key_env_var)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Test Data and Few-Shot Data\n",
    "\n",
    "Test texts are Russell paragraphs. Few-shot examples come from 5 different sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment parameters\n",
    "n_sample = 5\n",
    "m_paragraphs_per_sample = 1\n",
    "n_few_shot_per_source = 3  # Number of examples from each source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select deterministic test samples (Russell)\n",
    "quality_texts_deterministic = [\n",
    "    sampler_russell.get_paragraph_chunk(file_index=0, paragraph_range=slice(9, 9+m_paragraphs_per_sample)),\n",
    "    sampler_russell.get_paragraph_chunk(file_index=0, paragraph_range=slice(29, 29+m_paragraphs_per_sample)),\n",
    "    sampler_russell.get_paragraph_chunk(file_index=0, paragraph_range=slice(131, 131+m_paragraphs_per_sample)),\n",
    "    sampler_russell.get_paragraph_chunk(file_index=1, paragraph_range=slice(13, 13+m_paragraphs_per_sample)),\n",
    "    sampler_russell.get_paragraph_chunk(file_index=1, paragraph_range=slice(39, 39+m_paragraphs_per_sample)),\n",
    "    sampler_russell.get_paragraph_chunk(file_index=1, paragraph_range=slice(192, 192+m_paragraphs_per_sample)),\n",
    "    sampler_russell.get_paragraph_chunk(file_index=2, paragraph_range=slice(20, 20+m_paragraphs_per_sample)),\n",
    "    sampler_russell.get_paragraph_chunk(file_index=2, paragraph_range=slice(43, 43+m_paragraphs_per_sample)),\n",
    "    sampler_russell.get_paragraph_chunk(file_index=2, paragraph_range=slice(146, 146+m_paragraphs_per_sample)),\n",
    "    sampler_russell.get_paragraph_chunk(file_index=3, paragraph_range=slice(7, 7+m_paragraphs_per_sample)),\n",
    "    sampler_russell.get_paragraph_chunk(file_index=3, paragraph_range=slice(73, 73+m_paragraphs_per_sample)),\n",
    "    sampler_russell.get_paragraph_chunk(file_index=3, paragraph_range=slice(202, 202+m_paragraphs_per_sample)),\n",
    "    sampler_russell.get_paragraph_chunk(file_index=3, paragraph_range=slice(285, 285+m_paragraphs_per_sample)),\n",
    "    sampler_russell.get_paragraph_chunk(file_index=4, paragraph_range=slice(4, 4+m_paragraphs_per_sample)),\n",
    "    sampler_russell.get_paragraph_chunk(file_index=4, paragraph_range=slice(67, 67+m_paragraphs_per_sample)),\n",
    "    sampler_russell.get_paragraph_chunk(file_index=4, paragraph_range=slice(124, 124+m_paragraphs_per_sample)),\n",
    "    sampler_russell.get_paragraph_chunk(file_index=5, paragraph_range=slice(6, 6+m_paragraphs_per_sample)),\n",
    "    sampler_russell.get_paragraph_chunk(file_index=5, paragraph_range=slice(119, 119+m_paragraphs_per_sample)),\n",
    "    sampler_russell.get_paragraph_chunk(file_index=5, paragraph_range=slice(301, 301+m_paragraphs_per_sample)),\n",
    "    sampler_russell.get_paragraph_chunk(file_index=6, paragraph_range=slice(23, 23+m_paragraphs_per_sample)),\n",
    "    sampler_russell.get_paragraph_chunk(file_index=6, paragraph_range=slice(75, 75+m_paragraphs_per_sample)),\n",
    "    sampler_russell.get_paragraph_chunk(file_index=6, paragraph_range=slice(152, 152+m_paragraphs_per_sample)),\n",
    "    sampler_russell.get_paragraph_chunk(file_index=6, paragraph_range=slice(198, 198+m_paragraphs_per_sample)),\n",
    "    sampler_russell.get_paragraph_chunk(file_index=6, paragraph_range=slice(271, 271+m_paragraphs_per_sample)),\n",
    "]\n",
    "reindex = [0, 5, 10, 15, 20, 1, 6, 11, 16, 21, 2, 7, 12, 17, 22, 3, 8, 13, 18, 23, 4, 9, 14, 19]\n",
    "test_texts = [\n",
    "    quality_texts_deterministic[i] for i in reindex[:n_sample]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load few-shot examples from Russell (baseline)\n",
    "few_shot_russell = []\n",
    "while len(few_shot_russell) < n_few_shot_per_source:\n",
    "    p = sampler_russell.sample_segment(p_length=m_paragraphs_per_sample)\n",
    "    \n",
    "    # Check if p overlaps with any test text\n",
    "    has_overlap = any(\n",
    "        p.file_index == test_seg.file_index and\n",
    "        p.paragraph_start < test_seg.paragraph_end and\n",
    "        test_seg.paragraph_start < p.paragraph_end\n",
    "        for test_seg in test_texts\n",
    "    )\n",
    "    \n",
    "    if not has_overlap:\n",
    "        few_shot_russell.append(p.text)\n",
    "\n",
    "print(f\"✓ Loaded {len(few_shot_russell)} Russell few-shot examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load few-shot examples from other authors\n",
    "def load_fewshot_from_file(filename: str, n_examples: int) -> list[str]:\n",
    "    \"\"\"Load n_examples paragraphs from a text file.\"\"\"\n",
    "    # Find file index for this filename\n",
    "    file_idx = None\n",
    "    for idx, f in enumerate(sampler_other.files):\n",
    "        if f.name == filename:\n",
    "            file_idx = idx\n",
    "            break\n",
    "    \n",
    "    if file_idx is None:\n",
    "        raise ValueError(f\"File not found in sampler: {filename}\")\n",
    "    \n",
    "    # Sample n_examples non-overlapping paragraphs\n",
    "    examples = []\n",
    "    used_ranges = []\n",
    "    \n",
    "    # Get total paragraphs in file\n",
    "    total_paras = len(sampler_other.paragraph_indices[file_idx])\n",
    "    \n",
    "    # Sample evenly across the text\n",
    "    step = max(1, total_paras // (n_examples + 1))\n",
    "    for i in range(n_examples):\n",
    "        start = min(step * (i + 1), total_paras - m_paragraphs_per_sample)\n",
    "        segment = sampler_other.get_paragraph_chunk(\n",
    "            file_index=file_idx,\n",
    "            paragraph_range=slice(start, start + m_paragraphs_per_sample)\n",
    "        )\n",
    "        examples.append(segment.text)\n",
    "    \n",
    "    return examples\n",
    "\n",
    "# Load examples from each source\n",
    "few_shot_chesterton = load_fewshot_from_file(\n",
    "    FEWSHOT_SOURCE_FILES['fewshot_chesterton'],\n",
    "    n_few_shot_per_source\n",
    ")\n",
    "few_shot_clausewitz = load_fewshot_from_file(\n",
    "    FEWSHOT_SOURCE_FILES['fewshot_clausewitz'],\n",
    "    n_few_shot_per_source\n",
    ")\n",
    "few_shot_freud = load_fewshot_from_file(\n",
    "    FEWSHOT_SOURCE_FILES['fewshot_freud'],\n",
    "    n_few_shot_per_source\n",
    ")\n",
    "few_shot_hume = load_fewshot_from_file(\n",
    "    FEWSHOT_SOURCE_FILES['fewshot_hume'],\n",
    "    n_few_shot_per_source\n",
    ")\n",
    "\n",
    "print(f\"✓ Loaded {len(few_shot_chesterton)} Chesterton few-shot examples\")\n",
    "print(f\"✓ Loaded {len(few_shot_clausewitz)} Clausewitz few-shot examples\")\n",
    "print(f\"✓ Loaded {len(few_shot_freud)} Freud few-shot examples\")\n",
    "print(f\"✓ Loaded {len(few_shot_hume)} Hume few-shot examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Content Flattening\n",
    "\n",
    "Extract content from each test sample, removing stylistic elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from belletrist.prompts import StyleFlatteningConfig\n",
    "\n",
    "print(\"=== Step 1: Flattening and Saving Samples ===\\n\")\n",
    "\n",
    "for k_text, test_sample in enumerate(test_texts):\n",
    "    sample_id = f\"sample_{k_text:03d}\"\n",
    "    \n",
    "    # Skip if already saved\n",
    "    if store.get_sample(sample_id):\n",
    "        print(f\"✓ {sample_id} already flattened (skipping)\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"Flattening {sample_id}...\", end=\" \")\n",
    "    \n",
    "    # Flatten content\n",
    "    flatten_prompt = prompt_maker.render(\n",
    "        StyleFlatteningConfig(text=test_sample.text)\n",
    "    )\n",
    "    flattened = reconstruction_llm.complete(flatten_prompt)\n",
    "    \n",
    "    # Save to store with provenance\n",
    "    source_info = f\"File {test_sample.file_index}, para {test_sample.paragraph_start}-{test_sample.paragraph_end}\"\n",
    "    store.save_sample(\n",
    "        sample_id=sample_id,\n",
    "        original_text=test_sample.text,\n",
    "        flattened_content=flattened.content,\n",
    "        flattening_model=flattened.model,\n",
    "        source_info=source_info\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ ({len(flattened.content)} chars)\")\n",
    "\n",
    "print(f\"\\n✓ All samples flattened and saved to store\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Reconstruction with Different Few-Shot Sources\n",
    "\n",
    "Generate reconstructions using the same prompt template but different few-shot examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from belletrist.prompts import StyleReconstructionFewShotConfig\n",
    "\n",
    "# Configuration\n",
    "n_runs = 2  # Stochastic runs per sample\n",
    "\n",
    "# Map methods to their few-shot examples\n",
    "FEWSHOT_EXAMPLES = {\n",
    "    'fewshot_russell': few_shot_russell,\n",
    "    'fewshot_chesterton': few_shot_chesterton,\n",
    "    'fewshot_clausewitz': few_shot_clausewitz,\n",
    "    'fewshot_freud': few_shot_freud,\n",
    "    'fewshot_hume': few_shot_hume\n",
    "}\n",
    "\n",
    "print(\"=== Step 2: Generating Reconstructions ===\\n\")\n",
    "\n",
    "for sample_id in store.list_samples():\n",
    "    sample = store.get_sample(sample_id)\n",
    "    print(f\"\\n{sample_id}:\")\n",
    "    \n",
    "    for run in range(n_runs):\n",
    "        print(f\"  Run {run}:\")\n",
    "        \n",
    "        for method in METHODS:\n",
    "            if store.has_reconstruction(sample_id, run, method):\n",
    "                print(f\"    ✓ {method:25s} (already done)\")\n",
    "                continue\n",
    "            \n",
    "            # Get few-shot examples for this method\n",
    "            few_shot_examples = FEWSHOT_EXAMPLES[method]\n",
    "            \n",
    "            # Build reconstruction prompt\n",
    "            config = StyleReconstructionFewShotConfig(\n",
    "                content_summary=sample['flattened_content'],\n",
    "                few_shot_examples=few_shot_examples\n",
    "            )\n",
    "            prompt = prompt_maker.render(config)\n",
    "            response = reconstruction_llm.complete(prompt)\n",
    "            \n",
    "            # Save immediately (crash resilient!)\n",
    "            store.save_reconstruction(\n",
    "                sample_id=sample_id,\n",
    "                run=run,\n",
    "                method=method,\n",
    "                reconstructed_text=response.content,\n",
    "                model=response.model\n",
    "            )\n",
    "            print(f\"    ✓ {method:25s} ({len(response.content)} chars)\")\n",
    "\n",
    "stats = store.get_stats()\n",
    "print(f\"\\n✓ Generated {stats['n_reconstructions']} total reconstructions\")\n",
    "print(f\"✓ Configured methods: {stats['configured_methods']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Judging\n",
    "\n",
    "Compare each reconstruction against the original using blind 5-way ranking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from belletrist.style_evaluation_models import StyleJudgmentComparative5Way\n",
    "from belletrist.prompts import StyleJudgeComparative5WayConfig\n",
    "\n",
    "n_judge_runs = 1\n",
    "\n",
    "print(\"=== Step 3: Comparative Blind Judging (5-way) ===\\n\")\n",
    "\n",
    "for sample_id in store.list_samples():\n",
    "    sample = store.get_sample(sample_id)\n",
    "    print(f\"\\n{sample_id}:\")\n",
    "    \n",
    "    for reconstruction_run in range(n_runs):\n",
    "        print(f\"  Reconstruction run {reconstruction_run}:\")\n",
    "        \n",
    "        # Get all 5 reconstructions ONCE\n",
    "        reconstructions = store.get_reconstructions(sample_id, reconstruction_run)\n",
    "        if len(reconstructions) != 5:\n",
    "            print(f\"    ✗ Missing reconstructions (found {len(reconstructions)}/5)\")\n",
    "            continue\n",
    "        \n",
    "        # Create mapping ONCE per reconstruction_run\n",
    "        mapping = store.create_random_mapping(seed=hash(f\"{sample_id}_{reconstruction_run}\"))\n",
    "        \n",
    "        # Build prompt ONCE per reconstruction_run\n",
    "        judge_config = StyleJudgeComparative5WayConfig(\n",
    "            original_text=sample['original_text'],\n",
    "            reconstruction_text_a=reconstructions[mapping.text_a],\n",
    "            reconstruction_text_b=reconstructions[mapping.text_b],\n",
    "            reconstruction_text_c=reconstructions[mapping.text_c],\n",
    "            reconstruction_text_d=reconstructions[mapping.text_d],\n",
    "            reconstruction_text_e=reconstructions[mapping.text_e]\n",
    "        )\n",
    "        judge_prompt = prompt_maker.render(judge_config)\n",
    "        \n",
    "        # Judge the SAME reconstructions multiple times\n",
    "        for judge_run in range(n_judge_runs):\n",
    "            if store.has_judgment(sample_id, reconstruction_run, judge_run):\n",
    "                print(f\"    Judge run {judge_run}: ✓ Already judged (skipping)\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"    Judge run {judge_run}: Judging...\", end=\" \")\n",
    "            \n",
    "            # Get structured JSON judgment\n",
    "            try:\n",
    "                response = judge_llm.complete_with_schema(judge_prompt, StyleJudgmentComparative5Way)\n",
    "                judgment = response.content\n",
    "                \n",
    "                # Save judgment\n",
    "                store.save_judgment(\n",
    "                    sample_id=sample_id,\n",
    "                    reconstruction_run=reconstruction_run,\n",
    "                    judgment=judgment,\n",
    "                    mapping=mapping,\n",
    "                    judge_model=response.model,\n",
    "                    judge_run=judge_run\n",
    "                )\n",
    "                print(f\"✓ (confidence: {judgment.confidence})\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"✗ Error: {e}\")\n",
    "\n",
    "stats = store.get_stats()\n",
    "print(f\"\\n✓ Completed {stats['n_judgments']} judgments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results to DataFrame\n",
    "print(\"=== Exporting Results ===\\n\")\n",
    "\n",
    "df = store.to_dataframe()\n",
    "\n",
    "print(f\"Total judgments: {len(df)}\")\n",
    "print(f\"Samples: {df['sample_id'].nunique()}\")\n",
    "print(f\"Reconstruction runs per sample: {df.groupby('sample_id')['reconstruction_run'].nunique().mean():.1f}\")\n",
    "print(f\"Judge runs per reconstruction: {df.groupby(['sample_id', 'reconstruction_run'])['judge_run'].nunique().mean():.1f}\")\n",
    "\n",
    "# Show first few rows\n",
    "print(f\"\\n=== Sample Results ===\\n\")\n",
    "display_cols = ['sample_id', 'reconstruction_run', 'judge_run'] + [f'ranking_{m}' for m in METHODS] + ['confidence']\n",
    "print(df[display_cols].head(25))\n",
    "\n",
    "# Export to CSV\n",
    "output_file = f\"style_eval_fewshot_sources_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"\\n✓ Results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze ranking distributions\n",
    "print(\"=== Ranking Distribution by Few-Shot Source ===\\n\")\n",
    "\n",
    "for method in METHODS:\n",
    "    col = f'ranking_{method}'\n",
    "    print(f\"\\n{method.upper()}:\")\n",
    "    ranking_counts = df[col].value_counts().sort_index()\n",
    "    for rank in [1, 2, 3, 4, 5]:\n",
    "        count = ranking_counts.get(rank, 0)\n",
    "        pct = (count / len(df) * 100) if len(df) > 0 else 0\n",
    "        print(f\"  Rank {rank}: {count:3d} ({pct:5.1f}%)\")\n",
    "\n",
    "print(\"\\n=== Confidence Distribution ===\\n\")\n",
    "print(df['confidence'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate method performance metrics\n",
    "print(\"=== Few-Shot Source Performance Metrics ===\\n\")\n",
    "\n",
    "# Calculate mean ranking for each method (lower is better: 1 = best, 5 = worst)\n",
    "mean_rankings = {}\n",
    "for method in METHODS:\n",
    "    col = f'ranking_{method}'\n",
    "    mean_rankings[method] = df[col].mean()\n",
    "\n",
    "# Sort by mean ranking (best first)\n",
    "sorted_methods = sorted(mean_rankings.items(), key=lambda x: x[1])\n",
    "\n",
    "print(\"Average Ranking (lower is better):\")\n",
    "for i, (method, mean_rank) in enumerate(sorted_methods, 1):\n",
    "    # Count how often this method ranked 1st\n",
    "    first_place = (df[f'ranking_{method}'] == 1).sum()\n",
    "    first_place_pct = (first_place / len(df) * 100) if len(df) > 0 else 0\n",
    "    \n",
    "    print(f\"{i}. {method:25s}: {mean_rank:.2f} (1st place: {first_place}/{len(df)} = {first_place_pct:.1f}%)\")\n",
    "\n",
    "# Top-2 rate\n",
    "print(\"\\nTop-2 Rate (ranked 1st or 2nd):\")\n",
    "for method in METHODS:\n",
    "    col = f'ranking_{method}'\n",
    "    top2 = ((df[col] == 1) | (df[col] == 2)).sum()\n",
    "    top2_pct = (top2 / len(df) * 100) if len(df) > 0 else 0\n",
    "    print(f\"  {method:25s}: {top2}/{len(df)} = {top2_pct:.1f}%\")\n",
    "\n",
    "# Bottom-2 rate (how often ranked 4th or 5th)\n",
    "print(\"\\nBottom-2 Rate (ranked 4th or 5th):\")\n",
    "for method in METHODS:\n",
    "    col = f'ranking_{method}'\n",
    "    bottom2 = ((df[col] == 4) | (df[col] == 5)).sum()\n",
    "    bottom2_pct = (bottom2 / len(df) * 100) if len(df) > 0 else 0\n",
    "    print(f\"  {method:25s}: {bottom2}/{len(df)} = {bottom2_pct:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export final results\n",
    "output_file = f\"style_evaluation_fewshot_sources_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"\\n✓ Results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "\n",
    "### Key Questions:\n",
    "\n",
    "1. **Does same-author few-shot win?** \n",
    "   - Compare Russell vs. all other sources\n",
    "   - Expected: Russell should rank highest on average\n",
    "\n",
    "2. **Does domain matter more than author?**\n",
    "   - Compare Hume (philosophy) vs. Freud/Clausewitz (different domains)\n",
    "   - If Hume ranks higher than others, domain similarity helps\n",
    "\n",
    "3. **How much worse are cross-author examples?**\n",
    "   - Mean rank difference between Russell and next-best\n",
    "   - Practical significance: is cross-author few-shot viable?\n",
    "\n",
    "4. **Are some authors actively harmful?**\n",
    "   - Check if any method consistently ranks 4th or 5th\n",
    "   - Suggests stylistic mismatch is worse than no examples\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Statistical significance testing (Friedman test for rankings)\n",
    "- Pairwise comparisons (Russell vs. each other source)\n",
    "- Qualitative review of reconstructions\n",
    "- Analyze judge reasoning for insights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
