{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Style Instruction Evaluation Framework\n",
    "\n",
    "This notebook evaluates how well derived style instructions enable style replication, compared to alternative approaches.\n",
    "\n",
    "## Methodology\n",
    "\n",
    "For each gold standard text:\n",
    "1. **Flatten**: Extract content while removing style\n",
    "2. **Reconstruct**: Generate text using 4 different methods (M stochastic runs each):\n",
    "   - Generic baseline\n",
    "   - Few-shot learning\n",
    "   - Author name prompting\n",
    "   - Derived style instructions\n",
    "3. **Judge (Blind Comparative)**: Judge ranks all 4 reconstructions from 1-4 based on similarity to original\n",
    "   - **Blind evaluation**: Judge sees only anonymous labels (Text A, B, C, D) - no method names\n",
    "   - **Ranking**: 1 = most similar, 2 = second, 3 = third, 4 = least similar\n",
    "   - **Order randomized**: Position of methods varies across samples to eliminate bias\n",
    "4. **Aggregate**: Analyze rankings to determine which method best captures style\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- **Crash resilient**: All LLM responses saved to SQLite immediately\n",
    "- **Resume support**: Can restart after failures, skips completed work\n",
    "- **Blind evaluation**: Eliminates judge bias by hiding method names\n",
    "- **Comparative ranking**: More informative than binary comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Libraries and Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: litellm>=1.80.0 in /Users/andersohrn/PycharmProjects/russell_writes/venv/lib/python3.11/site-packages (from -r requirements.txt (line 2)) (1.80.9)\n",
      "Requirement already satisfied: pydantic==2.7.4 in /Users/andersohrn/PycharmProjects/russell_writes/venv/lib/python3.11/site-packages (from -r requirements.txt (line 5)) (2.7.4)\n",
      "Requirement already satisfied: jinja2>=3.1.0 in /Users/andersohrn/PycharmProjects/russell_writes/venv/lib/python3.11/site-packages (from -r requirements.txt (line 8)) (3.1.6)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/andersohrn/PycharmProjects/russell_writes/venv/lib/python3.11/site-packages (from pydantic==2.7.4->-r requirements.txt (line 5)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in /Users/andersohrn/PycharmProjects/russell_writes/venv/lib/python3.11/site-packages (from pydantic==2.7.4->-r requirements.txt (line 5)) (2.18.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/andersohrn/PycharmProjects/russell_writes/venv/lib/python3.11/site-packages (from pydantic==2.7.4->-r requirements.txt (line 5)) (4.15.0)\n",
      "Requirement already satisfied: aiohttp>=3.10 in /Users/andersohrn/PycharmProjects/russell_writes/venv/lib/python3.11/site-packages (from litellm>=1.80.0->-r requirements.txt (line 2)) (3.13.2)\n",
      "Requirement already satisfied: click in /Users/andersohrn/PycharmProjects/russell_writes/venv/lib/python3.11/site-packages (from litellm>=1.80.0->-r requirements.txt (line 2)) (8.3.1)\n",
      "Requirement already satisfied: fastuuid>=0.13.0 in /Users/andersohrn/PycharmProjects/russell_writes/venv/lib/python3.11/site-packages (from litellm>=1.80.0->-r requirements.txt (line 2)) (0.14.0)\n",
      "Requirement already satisfied: grpcio<1.68.0,>=1.62.3 in /Users/andersohrn/PycharmProjects/russell_writes/venv/lib/python3.11/site-packages (from litellm>=1.80.0->-r requirements.txt (line 2)) (1.67.1)\n",
      "Requirement already satisfied: httpx>=0.23.0 in /Users/andersohrn/PycharmProjects/russell_writes/venv/lib/python3.11/site-packages (from litellm>=1.80.0->-r requirements.txt (line 2)) (0.28.1)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in /Users/andersohrn/PycharmProjects/russell_writes/venv/lib/python3.11/site-packages (from litellm>=1.80.0->-r requirements.txt (line 2)) (8.7.0)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /Users/andersohrn/PycharmProjects/russell_writes/venv/lib/python3.11/site-packages (from litellm>=1.80.0->-r requirements.txt (line 2)) (4.25.1)\n",
      "Requirement already satisfied: openai>=2.8.0 in /Users/andersohrn/PycharmProjects/russell_writes/venv/lib/python3.11/site-packages (from litellm>=1.80.0->-r requirements.txt (line 2)) (2.8.1)\n",
      "Requirement already satisfied: python-dotenv>=0.2.0 in /Users/andersohrn/PycharmProjects/russell_writes/venv/lib/python3.11/site-packages (from litellm>=1.80.0->-r requirements.txt (line 2)) (1.2.1)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in /Users/andersohrn/PycharmProjects/russell_writes/venv/lib/python3.11/site-packages (from litellm>=1.80.0->-r requirements.txt (line 2)) (0.12.0)\n",
      "Requirement already satisfied: tokenizers in /Users/andersohrn/PycharmProjects/russell_writes/venv/lib/python3.11/site-packages (from litellm>=1.80.0->-r requirements.txt (line 2)) (0.22.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/andersohrn/PycharmProjects/russell_writes/venv/lib/python3.11/site-packages (from jinja2>=3.1.0->-r requirements.txt (line 8)) (3.0.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/andersohrn/PycharmProjects/russell_writes/venv/lib/python3.11/site-packages (from aiohttp>=3.10->litellm>=1.80.0->-r requirements.txt (line 2)) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /Users/andersohrn/PycharmProjects/russell_writes/venv/lib/python3.11/site-packages (from aiohttp>=3.10->litellm>=1.80.0->-r requirements.txt (line 2)) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/andersohrn/PycharmProjects/russell_writes/venv/lib/python3.11/site-packages (from aiohttp>=3.10->litellm>=1.80.0->-r requirements.txt (line 2)) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/andersohrn/PycharmProjects/russell_writes/venv/lib/python3.11/site-packages (from aiohttp>=3.10->litellm>=1.80.0->-r requirements.txt (line 2)) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/andersohrn/PycharmProjects/russell_writes/venv/lib/python3.11/site-packages (from aiohttp>=3.10->litellm>=1.80.0->-r requirements.txt (line 2)) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/andersohrn/PycharmProjects/russell_writes/venv/lib/python3.11/site-packages (from aiohttp>=3.10->litellm>=1.80.0->-r requirements.txt (line 2)) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/andersohrn/PycharmProjects/russell_writes/venv/lib/python3.11/site-packages (from aiohttp>=3.10->litellm>=1.80.0->-r requirements.txt (line 2)) (1.22.0)\n",
      "Requirement already satisfied: anyio in /Users/andersohrn/PycharmProjects/russell_writes/venv/lib/python3.11/site-packages (from httpx>=0.23.0->litellm>=1.80.0->-r requirements.txt (line 2)) (4.11.0)\n",
      "Requirement already satisfied: certifi in /Users/andersohrn/PycharmProjects/russell_writes/venv/lib/python3.11/site-packages (from httpx>=0.23.0->litellm>=1.80.0->-r requirements.txt (line 2)) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/andersohrn/PycharmProjects/russell_writes/venv/lib/python3.11/site-packages (from httpx>=0.23.0->litellm>=1.80.0->-r requirements.txt (line 2)) (1.0.9)\n",
      "Requirement already satisfied: idna in /Users/andersohrn/PycharmProjects/russell_writes/venv/lib/python3.11/site-packages (from httpx>=0.23.0->litellm>=1.80.0->-r requirements.txt (line 2)) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/andersohrn/PycharmProjects/russell_writes/venv/lib/python3.11/site-packages (from httpcore==1.*->httpx>=0.23.0->litellm>=1.80.0->-r requirements.txt (line 2)) (0.16.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /Users/andersohrn/PycharmProjects/russell_writes/venv/lib/python3.11/site-packages (from importlib-metadata>=6.8.0->litellm>=1.80.0->-r requirements.txt (line 2)) (3.23.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/andersohrn/PycharmProjects/russell_writes/venv/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.80.0->-r requirements.txt (line 2)) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/andersohrn/PycharmProjects/russell_writes/venv/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.80.0->-r requirements.txt (line 2)) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/andersohrn/PycharmProjects/russell_writes/venv/lib/python3.11/site-packages (from jsonschema<5.0.0,>=4.22.0->litellm>=1.80.0->-r requirements.txt (line 2)) (0.29.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/andersohrn/PycharmProjects/russell_writes/venv/lib/python3.11/site-packages (from openai>=2.8.0->litellm>=1.80.0->-r requirements.txt (line 2)) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in /Users/andersohrn/PycharmProjects/russell_writes/venv/lib/python3.11/site-packages (from openai>=2.8.0->litellm>=1.80.0->-r requirements.txt (line 2)) (0.12.0)\n",
      "Requirement already satisfied: sniffio in /Users/andersohrn/PycharmProjects/russell_writes/venv/lib/python3.11/site-packages (from openai>=2.8.0->litellm>=1.80.0->-r requirements.txt (line 2)) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/andersohrn/PycharmProjects/russell_writes/venv/lib/python3.11/site-packages (from openai>=2.8.0->litellm>=1.80.0->-r requirements.txt (line 2)) (4.67.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/andersohrn/PycharmProjects/russell_writes/venv/lib/python3.11/site-packages (from tiktoken>=0.7.0->litellm>=1.80.0->-r requirements.txt (line 2)) (2025.11.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/andersohrn/PycharmProjects/russell_writes/venv/lib/python3.11/site-packages (from tiktoken>=0.7.0->litellm>=1.80.0->-r requirements.txt (line 2)) (2.32.5)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /Users/andersohrn/PycharmProjects/russell_writes/venv/lib/python3.11/site-packages (from tokenizers->litellm>=1.80.0->-r requirements.txt (line 2)) (1.1.4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: filelock in /Users/andersohrn/PycharmProjects/russell_writes/venv/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm>=1.80.0->-r requirements.txt (line 2)) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/andersohrn/PycharmProjects/russell_writes/venv/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm>=1.80.0->-r requirements.txt (line 2)) (2025.10.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /Users/andersohrn/PycharmProjects/russell_writes/venv/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm>=1.80.0->-r requirements.txt (line 2)) (1.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/andersohrn/PycharmProjects/russell_writes/venv/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm>=1.80.0->-r requirements.txt (line 2)) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/andersohrn/PycharmProjects/russell_writes/venv/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm>=1.80.0->-r requirements.txt (line 2)) (6.0.3)\n",
      "Requirement already satisfied: shellingham in /Users/andersohrn/PycharmProjects/russell_writes/venv/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm>=1.80.0->-r requirements.txt (line 2)) (1.5.4)\n",
      "Requirement already satisfied: typer-slim in /Users/andersohrn/PycharmProjects/russell_writes/venv/lib/python3.11/site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm>=1.80.0->-r requirements.txt (line 2)) (0.20.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/andersohrn/PycharmProjects/russell_writes/venv/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken>=0.7.0->litellm>=1.80.0->-r requirements.txt (line 2)) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/andersohrn/PycharmProjects/russell_writes/venv/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken>=0.7.0->litellm>=1.80.0->-r requirements.txt (line 2)) (2.5.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Providers\n",
      "=========\n",
      "* openai\n",
      "* openai_like\n",
      "* bytez\n",
      "* xai\n",
      "* custom_openai\n",
      "* text-completion-openai\n",
      "* cohere\n",
      "* cohere_chat\n",
      "* clarifai\n",
      "* anthropic\n",
      "* anthropic_text\n",
      "* replicate\n",
      "* huggingface\n",
      "* together_ai\n",
      "* datarobot\n",
      "* openrouter\n",
      "* cometapi\n",
      "* vertex_ai\n",
      "* vertex_ai_beta\n",
      "* gemini\n",
      "* ai21\n",
      "* baseten\n",
      "* azure\n",
      "* azure_text\n",
      "* azure_ai\n",
      "* sagemaker\n",
      "* sagemaker_chat\n",
      "* bedrock\n",
      "* vllm\n",
      "* nlp_cloud\n",
      "* petals\n",
      "* oobabooga\n",
      "* ollama\n",
      "* ollama_chat\n",
      "* deepinfra\n",
      "* perplexity\n",
      "* mistral\n",
      "* groq\n",
      "* nvidia_nim\n",
      "* cerebras\n",
      "* baseten\n",
      "* ai21_chat\n",
      "* volcengine\n",
      "* codestral\n",
      "* text-completion-codestral\n",
      "* deepseek\n",
      "* sambanova\n",
      "* maritalk\n",
      "* cloudflare\n",
      "* fireworks_ai\n",
      "* friendliai\n",
      "* watsonx\n",
      "* watsonx_text\n",
      "* triton\n",
      "* predibase\n",
      "* databricks\n",
      "* empower\n",
      "* github\n",
      "* custom\n",
      "* litellm_proxy\n",
      "* hosted_vllm\n",
      "* llamafile\n",
      "* lm_studio\n",
      "* galadriel\n",
      "* gradient_ai\n",
      "* github_copilot\n",
      "* novita\n",
      "* meta_llama\n",
      "* featherless_ai\n",
      "* nscale\n",
      "* nebius\n",
      "* dashscope\n",
      "* moonshot\n",
      "* v0\n",
      "* heroku\n",
      "* oci\n",
      "* morph\n",
      "* lambda_ai\n",
      "* vercel_ai_gateway\n",
      "* wandb\n",
      "* ovhcloud\n",
      "* lemonade\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import litellm\n",
    "    print('Providers\\n=========')\n",
    "    print('* ' + '\\n* '.join(litellm.LITELLM_CHAT_PROVIDERS))\n",
    "    litellm.drop_params = True\n",
    "except ImportError as e:\n",
    "    print(f\"✗ Cannot import litellm: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Base Objects\n",
    "The base objects part of the current project library (`belletrist`) are initialized. They are:\n",
    "* `LLM`: the LLM object.\n",
    "* `LLMConfig`: the configuration of the LLM object, such as what model to use.\n",
    "* `PromptMaker`: generates prompts from templates and variables\n",
    "* `DataSampler`: retrieves and samples text at a source directory\n",
    "\n",
    "These will implement text transformations by LLMs part of the evaluation process. They build on the third-party LLMs, which we furthermore split into LLMs for text reconstruction and text judging, the key parameters for which are set below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_reconstruction_string = 'together_ai/Qwen/Qwen3-235B-A22B-Instruct-2507-tput'\n",
    "model_reconstruction_api_key_env_var = 'TOGETHER_AI_API_KEY'\n",
    "#model_reconstruction_string = 'anthropic/claude-sonnet-4-5-20250929'\n",
    "#model_reconstruction_api_key_env_var = 'ANTHROPIC_API_KEY'\n",
    "#model_reconstruction_string = 'openai/gpt-5.1-2025-11-13'\n",
    "#model_reconstruction_api_key_env_var = 'OPENAI_API_KEY'\n",
    "#model_reconstruction_string = 'together_ai/moonshotai/Kimi-K2-Instruct'\n",
    "#model_reconstruction_api_key_env_var = 'TOGETHER_AI_API_KEY'\n",
    "#model_reconstruction_string = 'mistral/mistral-large-2512'\n",
    "#model_reconstruction_api_key_env_var = 'MISTRAL_API_KEY'\n",
    "#model_judge_string = 'anthropic/claude-sonnet-4-5-20250929'\n",
    "#model_judge_api_key_env_var = 'ANTHROPIC_API_KEY'\n",
    "model_judge_string = 'mistral/mistral-large-2512'\n",
    "model_judge_api_key_env_var = 'MISTRAL_API_KEY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Data path: /Users/andersohrn/PycharmProjects/ClaudeCodeCourse/style-retrieval/data/russell\n",
      "✓ Segment database: /Users/andersohrn/PycharmProjects/ClaudeCodeCourse/style-retrieval/segments_qwen.db\n",
      "✓ Evaluation database: /Users/andersohrn/PycharmProjects/ClaudeCodeCourse/style-retrieval/style_eval_agent_fewshot.db\n",
      "✓ Configured methods: ['generic', 'fewshot', 'author', 'agent_fewshot']\n"
     ]
    }
   ],
   "source": [
    "from belletrist import PromptMaker, DataSampler, StyleEvaluationStore, SegmentStore\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION - Modify these parameters before running\n",
    "# ============================================================================\n",
    "\n",
    "# Data paths\n",
    "DATA_PATH = Path(os.getcwd()) / \"data\" / \"russell\"\n",
    "SEGMENT_DB_PATH = Path(os.getcwd()) / \"segments_qwen.db\"\n",
    "EVALUATION_DB_PATH = Path(os.getcwd()) / \"style_eval_agent_fewshot.db\"\n",
    "\n",
    "# Methods for this experiment (must be exactly 4)\n",
    "METHODS = ['generic', 'fewshot', 'author', 'agent_fewshot']\n",
    "\n",
    "# ============================================================================\n",
    "\n",
    "# Validate configuration\n",
    "if not DATA_PATH.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Data directory not found: {DATA_PATH}\\n\"\n",
    "        f\"Please ensure the data directory exists.\"\n",
    "    )\n",
    "\n",
    "if not SEGMENT_DB_PATH.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Segment database not found: {SEGMENT_DB_PATH}\\n\"\n",
    "        f\"The 'agent_fewshot' method requires a segment catalog.\\n\"\n",
    "        f\"Please run 'python runs/style_retrieval.py' first to build the catalog.\"\n",
    "    )\n",
    "\n",
    "# Initialize components\n",
    "prompt_maker = PromptMaker()\n",
    "\n",
    "sampler = DataSampler(data_path=DATA_PATH.resolve())\n",
    "\n",
    "store = StyleEvaluationStore(\n",
    "    EVALUATION_DB_PATH,\n",
    "    methods=METHODS\n",
    ")\n",
    "\n",
    "print(f\"✓ Data path: {DATA_PATH}\")\n",
    "print(f\"✓ Segment database: {SEGMENT_DB_PATH}\")\n",
    "print(f\"✓ Evaluation database: {EVALUATION_DB_PATH}\")\n",
    "print(f\"✓ Configured methods: {METHODS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "store.reset('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from belletrist import LLM, LLMConfig\n",
    "\n",
    "reconstruction_llm = LLM(LLMConfig(\n",
    "    model=model_reconstruction_string,\n",
    "    api_key=os.environ.get(model_reconstruction_api_key_env_var)\n",
    "))\n",
    "judge_llm = LLM(LLMConfig(\n",
    "    model=model_judge_string,\n",
    "    api_key=os.environ.get(model_judge_api_key_env_var)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Test Data and Few-Shot Data\n",
    "The reconstruction method tests build on gold standard texts. The test also includes few-shot prompting with the gold standard texts. In order to not skew the tests, no few-shot examples can overlap with the test texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = 5\n",
    "m_paragraphs_per_sample = 5\n",
    "n_few_shot_sample = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_texts = []\n",
    "#for _ in range(n_sample):\n",
    "#    test_texts.append(sampler.sample_segment(p_length=m_paragraphs_per_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_texts_deterministic = [\n",
    "    sampler.get_paragraph_chunk(file_index=0, paragraph_range=slice(9, 9+m_paragraphs_per_sample)),\n",
    "    sampler.get_paragraph_chunk(file_index=0, paragraph_range=slice(29, 29+m_paragraphs_per_sample)),\n",
    "    sampler.get_paragraph_chunk(file_index=0, paragraph_range=slice(131, 131+m_paragraphs_per_sample)),\n",
    "    sampler.get_paragraph_chunk(file_index=1, paragraph_range=slice(13, 13+m_paragraphs_per_sample)),\n",
    "    sampler.get_paragraph_chunk(file_index=1, paragraph_range=slice(39, 39+m_paragraphs_per_sample)),\n",
    "    sampler.get_paragraph_chunk(file_index=1, paragraph_range=slice(192, 192+m_paragraphs_per_sample)),\n",
    "    sampler.get_paragraph_chunk(file_index=2, paragraph_range=slice(20, 20+m_paragraphs_per_sample)),\n",
    "    sampler.get_paragraph_chunk(file_index=2, paragraph_range=slice(43, 43+m_paragraphs_per_sample)),\n",
    "    sampler.get_paragraph_chunk(file_index=2, paragraph_range=slice(146, 146+m_paragraphs_per_sample)),\n",
    "    sampler.get_paragraph_chunk(file_index=3, paragraph_range=slice(7, 7+m_paragraphs_per_sample)),\n",
    "    sampler.get_paragraph_chunk(file_index=3, paragraph_range=slice(73, 73+m_paragraphs_per_sample)),\n",
    "    sampler.get_paragraph_chunk(file_index=3, paragraph_range=slice(202, 202+m_paragraphs_per_sample)),\n",
    "    sampler.get_paragraph_chunk(file_index=3, paragraph_range=slice(285, 285+m_paragraphs_per_sample)),\n",
    "    sampler.get_paragraph_chunk(file_index=4, paragraph_range=slice(4, 4+m_paragraphs_per_sample)),\n",
    "    sampler.get_paragraph_chunk(file_index=4, paragraph_range=slice(67, 67+m_paragraphs_per_sample)),\n",
    "    sampler.get_paragraph_chunk(file_index=4, paragraph_range=slice(124, 124+m_paragraphs_per_sample)),\n",
    "    sampler.get_paragraph_chunk(file_index=5, paragraph_range=slice(6, 6+m_paragraphs_per_sample)),\n",
    "    sampler.get_paragraph_chunk(file_index=5, paragraph_range=slice(119, 119+m_paragraphs_per_sample)),\n",
    "    sampler.get_paragraph_chunk(file_index=5, paragraph_range=slice(301, 301+m_paragraphs_per_sample)),\n",
    "    sampler.get_paragraph_chunk(file_index=6, paragraph_range=slice(23, 23+m_paragraphs_per_sample)),\n",
    "    sampler.get_paragraph_chunk(file_index=6, paragraph_range=slice(75, 75+m_paragraphs_per_sample)),\n",
    "    sampler.get_paragraph_chunk(file_index=6, paragraph_range=slice(152, 152+m_paragraphs_per_sample)),\n",
    "    sampler.get_paragraph_chunk(file_index=6, paragraph_range=slice(198, 198+m_paragraphs_per_sample)),\n",
    "    sampler.get_paragraph_chunk(file_index=6, paragraph_range=slice(271, 271+m_paragraphs_per_sample)),\n",
    "]\n",
    "reindex = [0, 5, 10, 15, 20, 1, 6, 11, 16, 21, 2, 7, 12, 17, 22, 3, 8, 13, 18, 23, 4, 9, 14, 19]\n",
    "test_texts = [\n",
    "    quality_texts_deterministic[i] for i in reindex[:n_sample]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_texts = []\n",
    "while len(few_shot_texts) < n_few_shot_sample:\n",
    "    p = sampler.sample_segment(p_length=m_paragraphs_per_sample)\n",
    "\n",
    "    # Check if p overlaps with any test text\n",
    "    # Two segments overlap if they're from the same file AND their paragraph ranges overlap\n",
    "    # Ranges [a, b) and [c, d) overlap if: a < d AND c < b\n",
    "    has_overlap = any(\n",
    "        p.file_index == test_seg.file_index and\n",
    "        p.paragraph_start < test_seg.paragraph_end and\n",
    "        test_seg.paragraph_start < p.paragraph_end\n",
    "        for test_seg in test_texts\n",
    "    )\n",
    "\n",
    "    if not has_overlap:\n",
    "        few_shot_texts.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#few_shot_texts = [\n",
    "#    quality_texts_deterministic[i] for i in reversed(reindex[:n_few_shot_sample])\n",
    "#]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sampler_fake = DataSampler(\n",
    "#    data_path=(Path(os.getcwd()) / \"data\" / \"other_author\").resolve()\n",
    "#)\n",
    "#few_shot_texts = [\n",
    "#    sampler_fake.get_paragraph_chunk(file_index=0, paragraph_range=slice(0, 5)),\n",
    "#    sampler_fake.get_paragraph_chunk(file_index=0, paragraph_range=slice(10, 15)),\n",
    "#    sampler_fake.get_paragraph_chunk(file_index=0, paragraph_range=slice(20, 25)),\n",
    "#    sampler_fake.get_paragraph_chunk(file_index=0, paragraph_range=slice(30, 35)),\n",
    "#    sampler_fake.get_paragraph_chunk(file_index=0, paragraph_range=slice(40, 45)),\n",
    "#]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Test Transformation Objects\n",
    "The combination of prompt and LLM leads to the following operators in the test chain:\n",
    "* **Style Flattener**, which given a text compresses it into its content bare bones.\n",
    "* **Reconstructor, LLM House Style**, which given a compressed content expands it into a complete text with the \"house style\" of the LLM employed for the reconstruction.\n",
    "* **Reconstructor, Few Shot**, which given a compressed content expands it into a complete text with a few text excerpts on unrelated topics as style guide.\n",
    "* **Reconstructor, LLM Author Model**, which given a compressed content expands it into a complete text with the named author's style as the LLM conceives it without any other guidance.\n",
    "* **Reconstructor, Style Instruction**, which given a compressed content expands it into a complete text following the detailed style instruction, as derived from previous analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Configuration: 3 runs per sample, 4 reconstruction methods\n",
      "✓ Methods: generic, fewshot, author, agent_fewshot\n"
     ]
    }
   ],
   "source": [
    "# Import from style-retrieval (no models/ subdirectory)\n",
    "from belletrist.style_evaluation_models import (\n",
    "    MethodMapping,\n",
    "    StyleJudgmentComparative\n",
    ")\n",
    "from belletrist.prompts import (\n",
    "    StyleFlatteningConfig,\n",
    "    StyleReconstructionGenericConfig,\n",
    "    StyleReconstructionFewShotConfig,\n",
    "    StyleReconstructionAuthorConfig,\n",
    "    StyleJudgeComparativeConfig\n",
    ")\n",
    "\n",
    "# Configuration\n",
    "n_runs = 3\n",
    "n_judge_runs = 1\n",
    "AUTHOR_NAME = \"Bertrand Russell\"\n",
    "\n",
    "# Reconstructor configs (agent_fewshot handled separately)\n",
    "RECONSTRUCTORS_CFGS = {\n",
    "    'generic': StyleReconstructionGenericConfig,\n",
    "    'fewshot': StyleReconstructionFewShotConfig,\n",
    "    'author': StyleReconstructionAuthorConfig,\n",
    "    'agent_fewshot': None  # Special case: uses agent_rewriter\n",
    "}\n",
    "\n",
    "# Reconstructor kwargs\n",
    "RECONSTRUCTORS_KWARGS = {\n",
    "    'generic': {},\n",
    "    'fewshot': {'few_shot_examples': [seg.text for seg in few_shot_texts]},\n",
    "    'author': {'author_name': AUTHOR_NAME},\n",
    "    'agent_fewshot': {}  # Handled separately\n",
    "}\n",
    "\n",
    "print(f\"✓ Configuration: {n_runs} runs per sample, {len(METHODS)} reconstruction methods\")\n",
    "print(f\"✓ Methods: {', '.join(METHODS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Content Flattening\n",
    "\n",
    "Extract content from each test sample, removing stylistic elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Step 1: Flattening and Saving Samples ===\n",
      "\n",
      "Flattening sample_000... ✓ (6183 chars)\n",
      "Flattening sample_001... ✓ (6119 chars)\n",
      "Flattening sample_002... ✓ (3613 chars)\n",
      "Flattening sample_003... ✓ (3496 chars)\n",
      "Flattening sample_004... ✓ (3610 chars)\n",
      "\n",
      "✓ All samples flattened and saved to store\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Save samples and flatten content\n",
    "print(\"=== Step 1: Flattening and Saving Samples ===\\n\")\n",
    "\n",
    "for k_text, test_sample in enumerate(test_texts):\n",
    "    sample_id = f\"sample_{k_text:03d}\"\n",
    "    \n",
    "    # Skip if already saved\n",
    "    if store.get_sample(sample_id):\n",
    "        print(f\"✓ {sample_id} already flattened (skipping)\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"Flattening {sample_id}...\", end=\" \")\n",
    "    \n",
    "    # Flatten content\n",
    "    flatten_prompt = prompt_maker.render(\n",
    "        StyleFlatteningConfig(text=test_sample.text)\n",
    "    )\n",
    "    flattened = reconstruction_llm.complete(flatten_prompt)\n",
    "    \n",
    "    # Save to store with provenance\n",
    "    source_info = f\"File {test_sample.file_index}, para {test_sample.paragraph_start}-{test_sample.paragraph_end}\"\n",
    "    store.save_sample(\n",
    "        sample_id=sample_id,\n",
    "        original_text=test_sample.text,\n",
    "        flattened_content=flattened.content,\n",
    "        flattening_model=flattened.model,\n",
    "        source_info=source_info\n",
    "    )\n",
    "    \n",
    "    print(f\"✓ ({len(flattened.content)} chars)\")\n",
    "\n",
    "print(f\"\\n✓ All samples flattened and saved to store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====ORIGINAL===\n",
      "In reading even the best treatises on education written in former\n",
      "times, one becomes aware of certain changes that have come over\n",
      "educational theory. The two great reformers of educational theory\n",
      "before the nineteenth century were Locke and Rousseau. Both deserved\n",
      "their reputation, for both repudiated many errors which were\n",
      "wide-spread when they wrote. But neither went as far in his own\n",
      "direction as almost all modern educationists go. Both, for example,\n",
      "belong to the tendency which led to liberalism and democracy; yet\n",
      "both consider only the education of an aristocratic boy, to which one\n",
      "man’s whole time is devoted. However excellent might be the results\n",
      "of such a system, no man with a modern outlook would give it serious\n",
      "consideration, because it is arithmetically impossible for every child\n",
      "to absorb the whole time of an adult tutor. The system is therefore\n",
      "one which can only be employed by a privileged caste; in a just world,\n",
      "its existence would be impossible. The modern man, though he may seek\n",
      "special advantages for his own children in practice, does not consider\n",
      "the theoretical problem solved except by some method of education\n",
      "which could be open to all, or at least to all whose capacities render\n",
      "them capable of profiting by it. I do not mean that the well-to-do\n",
      "should, here and now, forego educational opportunities which, in the\n",
      "existing world, are not open to all. To do that would be to sacrifice\n",
      "civilization to justice. What I do mean is that the educational system\n",
      "we must aim at producing in the future is one which gives to every boy\n",
      "and girl an opportunity for the best that exists. The ideal system of\n",
      "education must be democratic, although that ideal is not immediately\n",
      "attainable. This, I think, would, nowadays, be pretty generally\n",
      "conceded. In this sense, I shall keep democracy in view. Whatever\n",
      "I shall advocate will be capable of being universal, though the\n",
      "individual should not meantime sacrifice his children to the badness\n",
      "of what is common, if he has the intelligence and the opportunity to\n",
      "secure something better. Even this very attenuated form of democratic\n",
      "principle is absent from the treatises of Locke and Rousseau. Although\n",
      "the latter was a disbeliever in aristocracy, he never perceived the\n",
      "implications of his disbelief where education was concerned.\n",
      "\n",
      "This matter of democracy and education is one as to which clarity\n",
      "is important. It would be disastrous to insist upon a dead level of\n",
      "uniformity. Some boys and girls are cleverer than others, and can\n",
      "derive more benefit from higher education. Some teachers have been\n",
      "better trained or have more native aptitude than others, but it is\n",
      "impossible that everybody should be taught by the few best teachers.\n",
      "Even if the highest education were desirable for all, which I doubt,\n",
      "it is impossible that all should have it at present, and therefore a\n",
      "crude application of democratic principles might lead to the conclusion\n",
      "that none should have it. Such a view, if adopted, would be fatal to\n",
      "scientific progress, and would make the general level of education a\n",
      "hundred years hence needlessly low. Progress should not be sacrificed\n",
      "to a mechanical equality at the present moment; we must approach\n",
      "educational democracy carefully, so as to destroy in the process as\n",
      "little as possible of the valuable products that happen to have been\n",
      "associated with social injustice.\n",
      "\n",
      "But we cannot regard a method of education as satisfactory if it is one\n",
      "which could not possibly be universal. The children of rich people\n",
      "often have, in addition to their mother, a nurse, a nurserymaid,\n",
      "and a share in the other domestic servants; this involves an amount\n",
      "of attention which could never, in any social system, be given to\n",
      "all children. It is very doubtful whether carefully tended children\n",
      "really gain by being made unnecessarily parasitic, but in any case\n",
      "no impartial person can recommend special advantages for the few,\n",
      "except for special reasons, such as feeble-mindedness or genius. The\n",
      "wise parent, at the present day, is likely to choose, if he can, some\n",
      "method of education for his children which is not in fact universal,\n",
      "and for the sake of experiment it is desirable that parents should\n",
      "have the opportunity of trying new methods. But they ought to be such\n",
      "as could be made universal, if found to produce good results, not\n",
      "such as must from their very nature be confined to a privileged few.\n",
      "Fortunately, some of the best elements in modern educational theory\n",
      "and practice have had an extremely democratic origin; for example,\n",
      "Madame Montessori’s work began with nursery-schools in slums. In\n",
      "higher education, exceptional opportunity for exceptional ability is\n",
      "indispensable, but otherwise there is no reason why any child should\n",
      "suffer from the adoption of systems which might be adopted by all.\n",
      "\n",
      "There is another modern tendency in education, which is connected\n",
      "with democracy, but perhaps somewhat more open to question--I mean\n",
      "the tendency to make education useful rather than ornamental. The\n",
      "connection of the ornamental with aristocracy has been set forth\n",
      "searchingly in Veblen’s “Theory of the Leisure Class”, but it is\n",
      "only the educational aspect of this connection that concerns us. In\n",
      "male education, the matter is bound up with the controversy between\n",
      "a classical and a “modern” education; in the education of girls,\n",
      "it is part of the conflict between the ideal of the “gentlewoman”\n",
      "and the desire to train girls to be self-supporting. But the whole\n",
      "educational problem, where women are concerned, has been distorted by\n",
      "the desire for sex equality: there has been an attempt to acquire the\n",
      "same education as that given to boys, even where it was by no means\n",
      "good in itself. Consequently women educators have aimed at giving to\n",
      "their girls such “useless” knowledge as is given to boys of the same\n",
      "class, and have been bitter opponents of the notion that some part\n",
      "of female education should be a technical training for motherhood.\n",
      "These cross-currents make the tendency that I am considering in some\n",
      "respects less definite where women are concerned, though the decay of\n",
      "the ideal of the “fine lady” is one of the most noteworthy examples of\n",
      "the tendency. In order to avoid confusing the issue, I shall for the\n",
      "moment confine myself to male education.\n",
      "\n",
      "Many separate controversies, in all of which other questions arise, are\n",
      "in part dependent upon our present question. Should boys learn mainly\n",
      "classics or mainly science? Among other considerations, one is that the\n",
      "classics are ornamental and science is useful. Should education as soon\n",
      "as possible become technical instruction for some trade or profession?\n",
      "Again the controversy between the useful and the ornamental is\n",
      "relevant, though not decisive. Should children be taught to enunciate\n",
      "correctly and to have pleasant manners, or are these mere relics of\n",
      "aristocracy? Is appreciation of art a thing of any value except in\n",
      "the artist? Should spelling be phonetic? All these and many other\n",
      "controversies are argued in part in terms of the controversy between\n",
      "the useful and the ornamental.\n",
      "\n",
      "\n",
      "====FLATTENED====\n",
      "- Educational theory has changed significantly since earlier times, particularly in relation to democracy and accessibility.\n",
      "\n",
      "- Before the nineteenth century, the two major figures in educational reform were Locke and Rousseau.\n",
      "\n",
      "- Both Locke and Rousseau rejected many widespread educational errors of their time and contributed meaningfully to educational thought.\n",
      "\n",
      "- Despite their progressive reputations, neither Locke nor Rousseau extended their ideas to support universal education.\n",
      "\n",
      "- Both thinkers focused on the education of a single aristocratic boy, assuming that one adult could devote full time to one child.\n",
      "\n",
      "- This model, while potentially effective, is not scalable because it is impossible for every child to have a full-time tutor.\n",
      "\n",
      "- Therefore, the model is limited to a privileged minority and cannot exist in a just society.\n",
      "\n",
      "- Modern educational thought requires that any acceptable system must be applicable to all children, at least in theory.\n",
      "\n",
      "- The ideal educational system must be democratic, even if full realization of this ideal is not immediately possible.\n",
      "\n",
      "- This democratic ideal is now widely accepted.\n",
      "\n",
      "- The author supports keeping democracy as a guiding principle in educational reform.\n",
      "\n",
      "- Any educational method advocated should be capable of universal application.\n",
      "\n",
      "- Individuals may currently use non-universal methods if better than available public options, but such choices should not be considered ideal.\n",
      "\n",
      "- The democratic principle, even in a minimal form, is absent in the works of both Locke and Rousseau.\n",
      "\n",
      "- Rousseau, despite opposing aristocracy, failed to apply his anti-aristocratic views to education.\n",
      "\n",
      "- Clarity is essential when discussing democracy and education.\n",
      "\n",
      "- Insisting on uniformity in education would be harmful.\n",
      "\n",
      "- There are natural differences in ability among children: some are more capable of benefiting from advanced education.\n",
      "\n",
      "- Teachers also vary in quality, and it is impossible for all students to be taught by the best teachers.\n",
      "\n",
      "- Even if higher education were suitable for everyone, it is not currently feasible for all to access it.\n",
      "\n",
      "- A rigid application of democratic equality might lead to the conclusion that no one should receive higher education, which would be detrimental.\n",
      "\n",
      "- Such an outcome would hinder scientific progress and lower the overall educational standard in the future.\n",
      "\n",
      "- Progress should not be sacrificed for mechanical equality.\n",
      "\n",
      "- Educational democracy must be pursued carefully to preserve existing valuable educational achievements, even if they originated in unjust social conditions.\n",
      "\n",
      "- An educational method cannot be considered satisfactory if it cannot, in principle, be made universal.\n",
      "\n",
      "- Children of wealthy families often receive disproportionate attention from multiple caregivers, such as a mother, nurse, nurserymaid, and domestic servants.\n",
      "\n",
      "- This level of individual attention cannot be provided to all children in any feasible social system.\n",
      "\n",
      "- It is uncertain whether such careful, dependent upbringing benefits children, and it may foster parasitic behavior.\n",
      "\n",
      "- No impartial observer can justify special educational advantages for the few, except in cases of exceptional need or ability, such as genius or intellectual disability.\n",
      "\n",
      "- At present, wise parents may choose non-universal educational methods for their children, especially for experimental purposes.\n",
      "\n",
      "- Such experimental methods should be ones that could, in principle, be extended to all if proven effective.\n",
      "\n",
      "- Methods that are inherently limited to a privileged few should not be promoted.\n",
      "\n",
      "- Some of the most valuable developments in modern education originated from democratic contexts.\n",
      "\n",
      "- For example, Montessori’s work began in nursery schools in impoverished urban areas.\n",
      "\n",
      "- In higher education, exceptional opportunities should be available for those with exceptional ability.\n",
      "\n",
      "- Outside of cases involving exceptional talent, there is no justification for educational systems that cannot be universally adopted.\n",
      "\n",
      "- A second modern trend in education is the shift from ornamental to useful education, which is related to democratic values but more debatable.\n",
      "\n",
      "- Veblen’s “Theory of the Leisure Class” discusses the link between ornamentation and aristocracy, though only the educational implications are relevant here.\n",
      "\n",
      "- In male education, this trend appears in the debate between classical education (ornamental) and modern, science-based education (useful).\n",
      "\n",
      "- In female education, the conflict manifests as a choice between the ideal of the “gentlewoman” and training for self-sufficiency.\n",
      "\n",
      "- The educational issues concerning women have been complicated by the pursuit of sex equality.\n",
      "\n",
      "- Women’s education has often aimed to replicate the education given to boys, even when that education is not inherently valuable.\n",
      "\n",
      "- Women educators have sometimes emphasized the same “useless” knowledge given to boys and have resisted including training related to motherhood.\n",
      "\n",
      "- These conflicting goals make the shift from ornamental to useful education less clear-cut in the context of female education.\n",
      "\n",
      "- The decline of the “fine lady” ideal is a significant example of the broader trend toward useful education.\n",
      "\n",
      "- To clarify the argument, the author will focus only on male education for the remainder of the discussion.\n",
      "\n",
      "- Several educational controversies are partially shaped by the distinction between useful and ornamental education.\n",
      "\n",
      "- Examples include: whether boys should study classics or science, with classics seen as ornamental and science as useful.\n",
      "\n",
      "- Another issue is whether education should quickly become technical training for a specific trade or profession, where usefulness is a central concern.\n",
      "\n",
      "- The debate over whether children should be taught correct speech and polite manners involves questioning whether these are merely aristocratic relics.\n",
      "\n",
      "- The value of art appreciation is questioned, particularly for those who are not artists.\n",
      "\n",
      "- The proposal for phonetic spelling is another issue influenced by the useful vs. ornamental framework.\n",
      "\n",
      "- Many educational debates are, at least in part, framed by the underlying conflict between usefulness and ornamentation.\n"
     ]
    }
   ],
   "source": [
    "print('====ORIGINAL===')\n",
    "print(store.get_sample('sample_000')['original_text'])\n",
    "print('\\n\\n====FLATTENED====')\n",
    "print(store.get_sample('sample_000')['flattened_content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Reconstruction\n",
    "\n",
    "Generate reconstructions using all 4 methods, with M stochastic runs each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 2: Generate reconstructions (with crash resume and agent_fewshot support)\nprint(\"=== Step 2: Generating Reconstructions ===\\n\")\n\nfor sample_id in store.list_samples():\n    sample = store.get_sample(sample_id)\n    print(f\"\\n{sample_id}:\")\n    \n    for run in range(n_runs):\n        print(f\"  Run {run}:\")\n        \n        # Check which methods need reconstruction\n        for method in METHODS:\n            if store.has_reconstruction(sample_id, run, method):\n                print(f\"    ✓ {method:15s} (already done)\")\n                continue\n            \n            # Special handling for agent_fewshot\n            if method == 'agent_fewshot':\n                from belletrist.agent_rewriter import agent_rewrite\n                \n                # Initialize LLMs with proper temperatures for agent workflow\n                planning_llm = LLM(LLMConfig(\n                    model=model_reconstruction_string,\n                    api_key=os.environ.get(model_reconstruction_api_key_env_var),\n                    temperature=0.5,  # Deterministic planning\n                    max_tokens=8192  # Large JSON plans need generous token budget\n                ))\n                rewriting_llm = LLM(LLMConfig(\n                    model=model_reconstruction_string,\n                    api_key=os.environ.get(model_reconstruction_api_key_env_var),\n                    temperature=0.7,  # Creative rewriting\n                    max_tokens=8192  # Full paragraph rewrites need generous token budget\n                ))\n                \n                with SegmentStore(SEGMENT_DB_PATH) as segment_store:\n                    reconstructed_text = agent_rewrite(\n                        flattened_content=sample['flattened_content'],\n                        segment_store=segment_store,\n                        planning_llm=planning_llm,\n                        rewriting_llm=rewriting_llm,\n                        prompt_maker=prompt_maker\n                    )\n                \n                # Save reconstruction\n                store.save_reconstruction(\n                    sample_id=sample_id,\n                    run=run,\n                    method=method,\n                    reconstructed_text=reconstructed_text,\n                    model=model_reconstruction_string\n                )\n                print(f\"    ✓ {method:15s} ({len(reconstructed_text)} chars)\")\n            else:\n                # Standard reconstruction\n                config = RECONSTRUCTORS_CFGS[method](\n                    content_summary=sample['flattened_content'],\n                    **RECONSTRUCTORS_KWARGS[method]\n                )\n                prompt = prompt_maker.render(config)\n                response = reconstruction_llm.complete(prompt)\n                \n                # Save immediately (crash resilient!)\n                store.save_reconstruction(\n                    sample_id=sample_id,\n                    run=run,\n                    method=method,\n                    reconstructed_text=response.content,\n                    model=response.model\n                )\n                print(f\"    ✓ {method:15s} ({len(response.content)} chars)\")\n\nstats = store.get_stats()\nprint(f\"\\n✓ Generated {stats['n_reconstructions']} total reconstructions\")\nprint(f\"✓ Configured methods: {stats['configured_methods']}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = RECONSTRUCTORS_CFGS['fewshot'](\n",
    "                content_summary=sample['flattened_content'],\n",
    "                **RECONSTRUCTORS_KWARGS['fewshot']\n",
    "            )\n",
    "prompt = prompt_maker.render(config)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructions = store.get_reconstructions('sample_000', 0)\n",
    "for reconstructor in reconstructions.keys():\n",
    "    print(f\"{reconstructor.upper()}\\n===================\")\n",
    "    print(f\"\\n{reconstructions.get(reconstructor)}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Judging\n",
    "\n",
    "Compare each reconstruction against the original using the judge LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import from style-retrieval (no models/ subdirectory)\n",
    "from belletrist.style_evaluation_models import StyleJudgmentComparative\n",
    "from belletrist.prompts import StyleJudgeComparativeConfig\n",
    "\n",
    "# Step 3: Comparative blind judging (with crash resume and judge consistency testing)\n",
    "print(\"=== Step 3: Comparative Blind Judging ===\\n\")\n",
    "\n",
    "for sample_id in store.list_samples():\n",
    "    sample = store.get_sample(sample_id)\n",
    "    print(f\"\\n{sample_id}:\")\n",
    "    \n",
    "    for reconstruction_run in range(n_runs):\n",
    "        print(f\"  Reconstruction run {reconstruction_run}:\")\n",
    "        \n",
    "        # Get all 4 reconstructions ONCE\n",
    "        reconstructions = store.get_reconstructions(sample_id, reconstruction_run)\n",
    "        if len(reconstructions) != 4:\n",
    "            print(f\"    ✗ Missing reconstructions (found {len(reconstructions)}/4)\")\n",
    "            continue\n",
    "        \n",
    "        # Create mapping ONCE per reconstruction_run (deterministic seed for reproducibility)\n",
    "        mapping = store.create_random_mapping(seed=hash(f\"{sample_id}_{reconstruction_run}\"))\n",
    "        \n",
    "        # Build prompt ONCE per reconstruction_run (SAME prompt for all judge runs)\n",
    "        judge_config = StyleJudgeComparativeConfig(\n",
    "            original_text=sample['original_text'],\n",
    "            reconstruction_text_a=reconstructions[mapping.text_a],\n",
    "            reconstruction_text_b=reconstructions[mapping.text_b],\n",
    "            reconstruction_text_c=reconstructions[mapping.text_c],\n",
    "            reconstruction_text_d=reconstructions[mapping.text_d]\n",
    "        )\n",
    "        judge_prompt = prompt_maker.render(judge_config)\n",
    "        \n",
    "        # Judge the SAME reconstructions with the SAME prompt multiple times\n",
    "        for judge_run in range(n_judge_runs):\n",
    "            if store.has_judgment(sample_id, reconstruction_run, judge_run):\n",
    "                print(f\"    Judge run {judge_run}: ✓ Already judged (skipping)\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"    Judge run {judge_run}: Judging...\", end=\" \")\n",
    "            \n",
    "            # Get structured JSON judgment with schema enforcement\n",
    "            try:\n",
    "                response = judge_llm.complete_with_schema(judge_prompt, StyleJudgmentComparative)\n",
    "                judgment = response.content  # Already validated Pydantic instance\n",
    "                \n",
    "                # Save judgment with both reconstruction_run and judge_run\n",
    "                store.save_judgment(\n",
    "                    sample_id=sample_id,\n",
    "                    reconstruction_run=reconstruction_run,\n",
    "                    judgment=judgment,\n",
    "                    mapping=mapping,\n",
    "                    judge_model=response.model,\n",
    "                    judge_run=judge_run\n",
    "                )\n",
    "                print(f\"✓ (confidence: {judgment.confidence})\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"✗ Error: {e}\")\n",
    "\n",
    "stats = store.get_stats()\n",
    "print(f\"\\n✓ Completed {stats['n_judgments']} judgments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results to DataFrame\n",
    "print(\"=== Exporting Results ===\\n\")\n",
    "\n",
    "# Export from store (resolves anonymous rankings to methods)\n",
    "df = store.to_dataframe()\n",
    "\n",
    "print(f\"Total judgments: {len(df)}\")\n",
    "print(f\"Samples: {df['sample_id'].nunique()}\")\n",
    "print(f\"Reconstruction runs per sample: {df.groupby('sample_id')['reconstruction_run'].nunique().mean():.1f}\")\n",
    "print(f\"Judge runs per reconstruction: {df.groupby(['sample_id', 'reconstruction_run'])['judge_run'].nunique().mean():.1f}\")\n",
    "\n",
    "# Show first few rows (with dynamic method columns)\n",
    "print(f\"\\n=== Sample Results ===\\n\")\n",
    "display_cols = ['sample_id', 'reconstruction_run', 'judge_run'] + [f'ranking_{m}' for m in METHODS] + ['confidence']\n",
    "print(df[display_cols].head(25))\n",
    "\n",
    "# Export to CSV\n",
    "output_file = f\"style_eval_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"\\n✓ Results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Judge Consistency Analysis ===\n",
      "\n",
      "⚠ Only one judge_run per reconstruction. Set n_judge_runs > 1 to test consistency.\n"
     ]
    }
   ],
   "source": [
    "# Analyze judge consistency across multiple judgments of same reconstructions\n",
    "print(\"=== Judge Consistency Analysis ===\\n\")\n",
    "\n",
    "if 'judge_run' in df.columns and df['judge_run'].nunique() > 1:\n",
    "    # For each (sample_id, reconstruction_run), check ranking variance across judge_runs\n",
    "    consistency_results = []\n",
    "    \n",
    "    for (sample_id, recon_run), group in df.groupby(['sample_id', 'reconstruction_run']):\n",
    "        if len(group) > 1:  # Only if multiple judge runs exist\n",
    "            for method in ['generic', 'fewshot', 'author', 'instructions']:\n",
    "                col = f'ranking_{method}'\n",
    "                ranks = group[col].values\n",
    "                variance = ranks.std()\n",
    "                mean_rank = ranks.mean()\n",
    "                \n",
    "                consistency_results.append({\n",
    "                    'sample_id': sample_id,\n",
    "                    'reconstruction_run': recon_run,\n",
    "                    'method': method,\n",
    "                    'mean_rank': mean_rank,\n",
    "                    'std_dev': variance,\n",
    "                    'min_rank': ranks.min(),\n",
    "                    'max_rank': ranks.max(),\n",
    "                    'rank_range': ranks.max() - ranks.min(),\n",
    "                    'n_judgments': len(ranks)\n",
    "                })\n",
    "    \n",
    "    if consistency_results:\n",
    "        consistency_df = pd.DataFrame(consistency_results)\n",
    "        \n",
    "        print(\"Method-level consistency (across all samples):\")\n",
    "        method_consistency = consistency_df.groupby('method').agg({\n",
    "            'std_dev': 'mean',\n",
    "            'rank_range': 'mean'\n",
    "        }).round(2)\n",
    "        method_consistency.columns = ['Avg Std Dev', 'Avg Rank Range']\n",
    "        print(method_consistency)\n",
    "        print(\"\\n(Lower values = more consistent)\")\n",
    "        \n",
    "        print(\"\\n\\nMost inconsistent cases (judge disagreed most):\")\n",
    "        inconsistent = consistency_df.nlargest(10, 'rank_range')[\n",
    "            ['sample_id', 'reconstruction_run', 'method', 'min_rank', 'max_rank', 'rank_range']\n",
    "        ]\n",
    "        print(inconsistent.to_string(index=False))\n",
    "        \n",
    "        print(\"\\n\\nMost consistent cases (judge agreed most):\")\n",
    "        consistent = consistency_df.nsmallest(10, 'std_dev')[\n",
    "            ['sample_id', 'reconstruction_run', 'method', 'mean_rank', 'std_dev']\n",
    "        ]\n",
    "        print(consistent.to_string(index=False))\n",
    "        \n",
    "else:\n",
    "    print(\"⚠ Only one judge_run per reconstruction. Set n_judge_runs > 1 to test consistency.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To rank these texts, I focused on how closely each reconstruction captures the original author’s distinctive voice—its rhythm, cadence, word choice, and the relationship it establishes with the reader. The original text is characterized by a measured, philosophical tone that balances intellectual rigor with a conversational accessibility. It often employs long, flowing sentences that build complex arguments, yet it avoids pretension through direct address (e.g., \"I do not mean that...\") and a subtle dry wit. The author’s voice is authoritative but not dogmatic, inviting the reader into a shared exploration of ideas rather than lecturing. There’s also a rhythmic quality to the prose, with clauses that accumulate evidence or counterarguments in a deliberate, almost musical progression. The original also exhibits a particular kind of moral urgency, especially around democratic ideals, without slipping into polemic. It’s this combination of intellectual depth, stylistic elegance, and democratic conviction that defines the author’s voice.\n",
      "\n",
      "**Text C (Rank 1)** is the most stylistically similar to the original. It mirrors the original’s cadence and rhythm almost perfectly, with long, carefully structured sentences that build arguments in a way that feels both organic and deliberate. For example, the opening sentence—\"The history of educational thought is a tale of slow emancipation, like all human progress, from the cloistered chambers of privilege toward the open air of common life\"—echoes the original’s blend of metaphorical richness and philosophical sweep. Text C also captures the original’s conversational yet authoritative tone, using direct address (e.g., \"Let them\") and rhetorical questions to engage the reader. The word choices feel natural and aligned with the original’s vocabulary, avoiding both archaism and modern jargon. The moral urgency around democratic education is present, but it’s delivered with the same balance of idealism and pragmatism as the original. Text C also replicates the original’s dry wit, such as the observation that Rousseau’s educational ideal \"remains aristocratic in structure, if not in sentiment.\" This text would be the hardest to distinguish from the original if encountered in the wild.\n",
      "\n",
      "**Text A (Rank 2)** is very close to the original in content and argument, but it lacks some of the rhythmic and stylistic finesse. The sentences are well-constructed and the ideas are clearly articulated, but they often feel more mechanical and less fluid than the original. For example, the phrase \"This does not mean that affluent families must immediately abandon current advantages...\" is grammatically correct but lacks the original’s effortless flow. Text A also leans slightly more toward modern academic phrasing (e.g., \"scalable,\" \"universal application\"), which distances it from the original’s timeless quality. However, it does a good job of replicating the original’s moral urgency and democratic convictions, and its word choices are generally appropriate. It would likely fool a casual reader but might strike a more discerning one as slightly less polished.\n",
      "\n",
      "**Text B (Rank 3)** is competent and closely aligned with the original’s arguments, but it feels more like a summary or a paraphrase than a reconstruction of the author’s voice. The sentences are shorter and more straightforward, lacking the original’s rhythmic complexity. For example, the opening line—\"Educational theory has undergone significant transformation...\"—is clear but flat compared to the original’s evocative phrasing. Text B also occasionally slips into a more detached, analytical tone, as if it’s reporting on the original rather than embodying its voice. The moral urgency is present, but it’s delivered in a more didactic way, with less of the original’s conversational engagement. While it’s a strong reconstruction in terms of content, it doesn’t fully capture the stylistic fingerprint of the original.\n",
      "\n",
      "**Text D (Rank 4)** is the least similar to the original. While it gets the ideas right, its stylistic choices often feel at odds with the original’s voice. The sentences are overly formal and sometimes convoluted, with a tendency toward abstraction that the original avoids. For example, the phrase \"The evolution of educational theory, like that of most human institutions, has been a gradual and halting progress...\" is accurate but lacks the original’s immediacy and warmth. Text D also leans into a more academic, almost pedantic tone, which clashes with the original’s conversational authority. There are moments where the phrasing feels archaic or stilted (e.g., \"curiously unembarrassed by a glaring contradiction\"), which distances it from the original’s natural flow. While it’s a thoughtful reconstruction, it doesn’t feel like it was written by the same person as the original.\n"
     ]
    }
   ],
   "source": [
    "print(df.loc[0,'reasoning'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze ranking distributions (dynamic for configured methods)\n",
    "print(\"=== Ranking Distribution by Method ===\\n\")\n",
    "\n",
    "for method in METHODS:\n",
    "    col = f'ranking_{method}'\n",
    "    print(f\"\\n{method.upper()}:\")\n",
    "    ranking_counts = df[col].value_counts().sort_index()\n",
    "    for rank in [1, 2, 3, 4]:\n",
    "        count = ranking_counts.get(rank, 0)\n",
    "        pct = (count / len(df) * 100) if len(df) > 0 else 0\n",
    "        print(f\"  Rank {rank}: {count:3d} ({pct:5.1f}%)\")\n",
    "\n",
    "print(\"\\n=== Confidence Distribution ===\\n\")\n",
    "print(df['confidence'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate method performance metrics (dynamic for configured methods)\n",
    "print(\"=== Method Performance Metrics ===\\n\")\n",
    "\n",
    "# Calculate mean ranking for each method (lower is better: 1 = best, 4 = worst)\n",
    "mean_rankings = {}\n",
    "for method in METHODS:\n",
    "    col = f'ranking_{method}'\n",
    "    mean_rankings[method] = df[col].mean()\n",
    "\n",
    "# Sort by mean ranking (best first)\n",
    "sorted_methods = sorted(mean_rankings.items(), key=lambda x: x[1])\n",
    "\n",
    "print(\"Average Ranking (lower is better):\")\n",
    "for i, (method, mean_rank) in enumerate(sorted_methods, 1):\n",
    "    # Count how often this method ranked 1st\n",
    "    first_place = (df[f'ranking_{method}'] == 1).sum()\n",
    "    first_place_pct = (first_place / len(df) * 100) if len(df) > 0 else 0\n",
    "    \n",
    "    print(f\"{i}. {method:15s}: {mean_rank:.2f} (1st place: {first_place}/{len(df)} = {first_place_pct:.1f}%)\")\n",
    "\n",
    "# Win rate (percentage of times ranked 1st or 2nd)\n",
    "print(\"\\nTop-2 Rate (ranked 1st or 2nd):\")\n",
    "for method in METHODS:\n",
    "    col = f'ranking_{method}'\n",
    "    top2 = ((df[col] == 1) | (df[col] == 2)).sum()\n",
    "    top2_pct = (top2 / len(df) * 100) if len(df) > 0 else 0\n",
    "    print(f\"  {method:15s}: {top2}/{len(df)} = {top2_pct:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Results saved to style_evaluation_results_20251202_164845.csv\n"
     ]
    }
   ],
   "source": [
    "# Export results\n",
    "output_file = f\"style_evaluation_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "df.to_csv(output_file, index=False)\n",
    "print(f\"\\n✓ Results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "TODO: Add analysis cells for:\n",
    "- Statistical significance testing\n",
    "- Visualization of results\n",
    "- Sample-by-sample breakdown\n",
    "- Confidence level analysis\n",
    "- Qualitative review of reasoning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (russell_writes)",
   "language": "python",
   "name": "russell_writes"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}